## Lysva field inventory dataset {#sec-lysva-dataset}

The main original dataset used in the study is the Lysva field inventory dataset, named by the closest town to its location.
The dataset is released into open access with an accompanying paper that describes the data in detail and provides a basic baseline for individual tree detection [@dubrovinExplorationPropertiesPoint2024].
The study area is located in Perm Krai, Russia, 86 kilometers to the east of Perm.
The dataset consists of a field inventory of 3600 trees across 10 rectangular ground plots 100 meters in lengths and 50 meters in width fully covered by a UAV LiDAR and RGB orthophoto surveys.
@fig-lysva-roi shows the locations of the ground plots over the full size RGB orthophoto and a visualization of the field inventory for a single plot on top of the LiDAR point cloud.
Colored points represent trees, with different colors mapping to different species.
The point cloud is visualized as a 2D scatter plot with points colored by height (darker points are lower, brighter points are higher).

::: {#fig-lysva-roi}
![](../images/lysva_roi.jpg)

The study region for the Lysva field inventory dataset.
**Left**: The locations of field survey plot boundaries on a full-size RGB orthophoto.
Each plot is a 50 by 100 meter rectangle, with every tree within measured and recorded.
Buffered cutouts of the orthophoto come with the dataset, along with LiDAR point clouds.
**Right**: A close up of plot number 4.
Each colored point represents a single tree of a different species, on top of a point cloud colored by the value of the height of each point (lower points are dark, higher points are bright) and the same orthophoto.
Figure reused from [@dubrovinExplorationPropertiesPoint2024].
:::

The field inventory is a tabular dataset where every row represents a single tree.
@tbl-inventory-example shows a random sample of entries from the field inventory table.
Every tree is represented by a point in UTM 40N coordinate reference system (EPSG:32640).
Every tree has a species label and diameter at breast height (dbh), measured with calipers at 1.3 m from the ground at two perpendicular directions and averaged.
@fig-lysva-species-distribution shows the distribution of species in the data: the dominant species is spruce, but overall the trees are evenly split between deciduous and coniferous, 1793 and 1807 respectively, with seven species in total: spruce, birch, fir, aspen, tilia, alder, and willow.
Approximately 20% of the trees have height data measured during the inventory, and 10% have ages measured on core samples, shown in the table in meters and years respectively.

{{< embed 03a_lysva_dataset.ipynb#tbl-inventory-example >}}

{{< embed 03a_lysva_dataset.ipynb#fig-lysva-species-distribution >}}

The LiDAR sensor used for the survey is AGM-MS3 produced by AGM Systems.
It has 640 kHz acquisition rate, 300-meter range, and spatial accuracy of 3–5 centimeters.
The raw point clouds were processed with the combination of the AGM ScanWorks software from the sensor vendor and the TerraScan software.
The point clouds were preprocessed by removing duplicate points and high and low noise points.
The duplicate removal was run with a threshold distance between points of 1 mm.
Noise was removed by visually inspecting the point cloud and manually selecting height thresholds to cut off points that are lower than the ground or higher than the canopies.
Ground point classification was performed and ground points were used to normalize height by subtracting the ground level from the Z coordinate of every point.
Height normalization allows treating the Z coordinate as height above ground rather than the absolute elevation, which simplifies many subsequent steps.
The camera used for the orthophoto survey is Sony A6000.
The resolution of the orthophoto is 7 centimeters per pixel.
@fig-example-3d-point-cloud is a 3D visualization of the point cloud over plot number 10.
It shows the unmodified point cloud on the left, with points colored by height above ground, and a point cloud enriched with color information by sampling the orthophoto at the planar coordinates of the points.
@fig-example-ortho shows the orthophoto for the same plot.
The carrier UAV was configured to follow the terrain at 150 meter height using the SRTM elevation map as a reference [@farrShuttleRadarTopography2000].

{{< embed 03a_lysva_dataset.ipynb#fig-example-3d-point-cloud >}}

{{< embed 03a_lysva_dataset.ipynb#fig-example-ortho >}}

@tbl-lysva-plot-stats shows some descriptive statistics for each plot in the field inventory: the number of trees in the plot, average LiDAR point density in points per square meter, and dominant species type.
The overall average point density is 37 points per square meters.
Exactly half of the plots are predominantly coniferous and half are predominantly deciduous.
@fig-lysva-canopy-structure shows two of point clouds clipped by plot bounds in 3D, highlighting the differences in canopy structure complexity between predominantly deciduous and coniferous plots.
The figure highlights that the forest is indeed dense and mixed, with non-uniform, complex canopy structure.

{{< embed 03a_lysva_dataset.ipynb#tbl-lysva-plot-stats >}}

{{< embed 03a_lysva_dataset.ipynb#fig-lysva-canopy-structure >}}

### On using intensity-based features {#sec-intensity-based-features}

Even though some sources report intensity-based features as some of the most important ones [@shiImportantLiDARMetrics2018], the features seem to me unreliable in the context of forestry because of the physics of light reflection.
There are simply too many factors that affect the amplitude of the reflected signal, which might be useful when imaging stable targets such as urban environments but become completely unpredictable on highly unstable targets such as trees: they move in the wind in the time span of a single survey, they grow in the time span between repeated surveys, the leaves and branches are angled in every possible way.
Moreover, the quality of the recorded intensities highly depends on the used sensor.
As an example, @fig-intensity-with-example shows the distribution of intensity values for every point in the Lysva dataset, and shows plot number 10 in 3D with points colored by their recorded intensity.
The distribution of intensities seems like an artifact of faulty quantization (the fact that the maximum overall value is 63 makes me suspect it is stored by the hardware using a 6-bit unsigned integer, probably an ad hoc optimization by the sensor vendor).
With this distribution in mind, it is not surprising that coloring points by their intensity values results in images that look like noise, and there is no signal to be exploited for predictive modeling.

{{< embed 03a_lysva_dataset.ipynb#fig-intensity-with-example >}}

Some sensors do provide more consistent values.
However, relying on intensity-based features limits the applicability of developed models and methods, as any such model would surely fail on data like this.

### Comparison to other datasets

TODO: Rephrase this. This is directly from the Scientific Reports paper.

Our dataset is in many ways similar to the NewFor benchmark [@eysnAlpineITDBenchmark2015].
It serves the same purpose and also offers overlapping field survey ground plots and UAV LiDAR point clouds.
There are, however, many notable differences.
The NewFor benchmark covers much more diverse regions, including ground plots from France, Italy, Switzerland, Austria, and Slovenia, while all our data comes from the same area.
The tree species covered by the datasets are also different: both contain spruce and fir, but the NewFor data also has beech, Scots pine, larch, sycamore, and poplar, while ours also has birch, aspen, tilia, alder, and willow.
Our dataset has more than twice as many individual trees as the Alpine benchmark, and the forest is denser and more complex, making it more complicated to detect trees in.
Our data contains very mild terrain variations, while the slopes of the terrain in Alpine data are very steep, which plays a role during height normalization, since subtraction of steep terrain introduces artificial slope to the points in the canopy, changing the overall shape of the tree.
Our dataset has an additional information source – an RGB orthophoto that allows development of algorithms that fuse multimodal data, which, we believe, is a key to success in such complex environments.
Our dataset has species labels for every surveyed tree, but only partial coverage of tree heights and no timber volume information at all.

Another similar dataset is the NeonTreeEvaluation Benchmark [@weinsteinDataNeonTreeEvaluationBenchmark2022], which offers bounding box annotation for tree detection across a wide range of different forest types.
It offers coregistered RGB, LiDAR, and hyperspectral images over 31,000 individual trees.
The main difference in the reference data between the NeonTreeEvaluation Benchmark and our dataset is the source: our data comes from a field inventory and thus has additional tree information that can be used in downstream tasks, such as species classification or timber volume prediction, while the NeonTreeEvaluation Benchmark annotations are created from the RGB photo and thus only offer the positions and sized of trees.

Another dataset is the IDTReeS 2020 Competition Data [@gravesIDTReeS2020Competition2020] aimed to develop algorithms for delineation and species classification of individual tree crowns in RGB, LiDAR, and hyperspectral data.
It offers bounding box annotations for 1200 individual trees covered by RGB, LiDAR, and hyperspectral images in 3 national forests in the USA.
Similarly, the source of the data is annotation of images, not a field inventory.

There are also datasets available that don't have LiDAR point cloud coverage, or use terrestrial LiDAR instead of UAV LiDAR, or use photogrammetric point clouds instead of LiDAR  point clouds.
We do not mention them here because we specifically focus on UAV LiDAR.
