## Training tree segmentation neural networks

The architecture chosen to serve as the tree segmentation network is the PointNet++ [@qiPointNetPlusPlus2017], described in detail in @sec-ml-dl.
It is a relatively simple architecture, and further potential quality improvements might be achieved by using more modern and advanced architectures instead.
However, the main goal of this thesis was to develop and verify an overall framework, and thus the choice was set on a model that is simple to implement and work with to allow easy experimentation with other parts of the proposed system.

The architecture of the used PointNet++ is similar to the segmentation architecture shown in @fig-pointnet2-architecture.
The main differences are the use of one more stacked set abstraction layer to make the network deeper, and the use of a regression head that predicts a continuous value for each point instead of a classification head that predicts per-point class scores, since the model needs to assign a unique ID to every tree in the patch.
The three stacked set abstraction layers have the proportions of points sampled set to 0.75, 0.5, and 0.5, and neighborhood radii for feature aggregation set to 0.1, 0.2, and 0.4.
Note that the scale of the input is normalized (see next section for details), so the radii are not in meters.
The model has 30 million trainable parameters.

### Coordinate and feature normalization

It's a well established practice to scale the inputs to neural networks that is known to improve the speed and accuracy of gradient descent convergence [@bishop2006pattern].
Before going through the network, a set of augmentations and transformations is applied to each synthetic forest patch.
The augmentations are described in detail with visualized examples in @sec-augmentations.
The transformations include scale and feature normalization – the coordinates are centered and normalized to the interval $(-1, 1)$ and the features are normalized to the interval $(-1, 1)$.

### Data augmentation {#sec-augmentations}

The primary objective of data augmentation is to enhance the quantity, quality, and variety of data used for training [@mumuniDataAugmentationComprehensive2022].
This is especially important when the sizes of the available datasets are limited, like in the case of the individual trees data that is the base of the synthetic forest datasets.
Carefully selected augmentations allow increasing the effective dataset size.
However, it is important to pay attention that the chosen augmentations keep the transformed examples semantically equivalent to the original.
A readily understandable example of a bad augmentation in the task of digit classification, which is often used as the "hello world" of deep learning with the MNIST dataset of handwritten digits [@deng2012mnist], is a vertical flip, as most digits lose meaning when upside down.
A flip is also a bad augmentation example for a synthetic forest patch described in @sec-synthetic-forest-dataset, as it changes the order of the trees within a patch, thus breaking the labels that are assumed to increase in a specific pattern.

In the scenario when the data for is created by combining several smaller inputs, there is a possibility of applying augmentations on two different scales.
The most simple approach is to treat a synthetic forest patch as a whole and apply augmentations directly to it.
However, it is also possible to apply per-tree augmentations, effectively increasing the size of the underlying tree set from which synthetic forest patches constructed.
Only the latter kind are used for training the tree segmentation network.

The first transformation that changes the shape but doesn't affect any semantics for an individual tree is a random rotation around the vertical axis.
A tree remains completely the same when rotated, but the coordinates of all points change.
@fig-random-rotate-effect shows the effect of applying random rotation transformation of different magnitudes to a single aspen tree.
For visualization purposes, the rotation is forced to apply with full magnitude for every parameter.
During training, the angle is uniformly sampled from the specified range.
For the final tree segmentation model, the range is set to $[-180, 180]$ degrees, as no amount of rotation breaks the semantics.

{{< embed 03d_augmentations.ipynb#fig-random-rotate-effect >}}

Another transformation that keeps the tree the same but changes the coordinates of the points is random scaling.
It simply multiplies the coordinates by a scaling factor, making the tree larger or smaller in all directions.
Unlike random rotation around the vertical axis, however, the range needs to be chosen much more carefully, as there is a possibility of making unrealistically large or small trees that would confuse the model during training.
@fig-random-scale-effect shows the effect of applying random scale transformation to a single aspen tree.
Again, for purposes of visualization, the scale is forced to apply with full magnitude.
During training, the scale is uniformly sampled from the specified range.
For the final tree segmentation model, the range is set to $[0.8, 1.2]$.

{{< embed 03d_augmentations.ipynb#fig-random-scale-effect >}}

Another way to change the positions is to slightly translate each point for a random distance in a random direction.
That transformation is commonly referred to as random jitter.
Since LiDAR sensors have limited spatial accuracy, introducing random translations within the accuracy range should not have any effect on the result of tree segmentation.
Going further, small translations even outside the accuracy range make sense, since they have very limited effect on the overall shape of the tree.
@fig-random-jitter-effect shows this effect on a single aspen tree.
The amount of translation is uniformly sampled independently for each point from a set range.
Note that the random jitter augmentation is applied before the scale normalization.
It matters because the augmentation's only parameter is the translation range, which depends on the scale.
The parameters on the figure are thus in the original coordinate units – meters.
Note how the shape of the tree almost doesn't change when the maximum range is set to 20 centimeters, and starts to become fuzzy and loose shape at 1 meter and higher.
For the final tree segmentation model, the maximum translation is set to 30 centimeters.

{{< embed 03d_augmentations.ipynb#fig-random-jitter-effect >}}

Another useful effect augmentations can provide is to make synthetic data look more like real data.
As was mentioned in @sec-individual-trees-dataset, there is a selection bias in the dataset of individual trees: the trees that are easiest to manually separate are exponentially more likely to end up in the data.
As such, the tree clouds in the individual tree dataset are significantly different from trees of the same species with similar sizes and shapes, but standing in dense clusters.
One of the most important differences is that almost all the tree clouds have trunks, while in actual forest, dense canopy cover blocks most pulses and the trunk representation is very poor.
One way to mitigate that issue is to apply a height-dependent dropout function to each tree cloud.
For that purpose a probability threshold function that is around zero for the highest points of the tree and quickly ramps up to almost one for the lowest points is needed.
An example function that satisfies these criteria is a modified sigmoid:

$$
\text{threshold}(z) = \big[1 + e^{z \times \text{scale} + \text{shift}}\big]^{-1},
$$

where $z$ is the height normalized to $[0, 1]$ and reversed by subtraction from 1, $\text{scale}$ and $\text{shift}$ are hyperparameters that control the shape of the curve.
Changing $\text{scale}$ controls how steep is the climb from 0 to 1: the larger, the steeper.
Changing $\text{shift}$ controls the position of the climb: the larger, the lower.
@fig-height-dropout shows an example of applying such dropout function to a single aspen tree, and @fig-height-dropout-aggressive show the same tree with more aggressive parameters, resulting in much more points being dropped from the tree cloud.

{{< embed 03d_augmentations.ipynb#fig-height-dropout >}}

{{< embed 03d_augmentations.ipynb#fig-height-dropout-aggressive >}}

All the augmentation are applied on the fly right before the examples are loaded into GPU memory and passed through the network.

### Other training parameters {#sec-other-training-params}

Mean absolute error is used as a loss function.
Some experiments have shown improvements when using a custom loss function that modifies the mean absolute error loss by weighting it reversely proportional to the distance of the tree centroid.
The idea of the modification is to make the network focus more on points closer to the center of each tree, as these points are much less likely to be overlapping with the crowns of adjacent trees.

The batch size is limited by the available GPU device memory, and in the described setup competes for memory space with the synthetic forest patch dimensions used in the training dataset.
The preference is given to the size of the patch, so the batch size is set to 1, but is compensated for by using gradient accumulation steps – updates to the model parameters are made after the gradient is accumulated for a set number of iterations.
This slows down the training, but enables usage of larger batches when there is not enough memory to fit them, making the training procedure overall more stable.

Early stopping [@precheltAutomaticEarlyStopping1998] is set up to terminate training early if there is no improvement in average validation loss in a set number of epochs.
This makes sure that valuable GPU time is not wasted on continuing training models that are likely to have started overfitting.

Model checkpointing is set up as well.
After each training epoch, when the average validation loss and accuracy are calculated, the model state is saved to disk if it's current accuracy is better than the last saved one, where accuracy is the proportion of points for which the rounded integer label is correct.
This makes sure that the best performing model can always be recovered, even if the training process was run for too long and the latest model is not the best one.

The learning rate schedule is set to follow a fast linear warm up and slow linear decay.
Learning rate is set to ramp up from 0.001 to 0.01 in a span of 2 epochs, and then decay back to 0.001 in a span of 30 epochs.
Adam optimizer is used [@kingmaAdamMethodStochastic2014].

Training was performed on an NVIDIA A100 GPU with 80G of memory.
Inference, depending on the point density of the input, might work even on a much smaller GPU like Tesla T4 with 16G of memory.
GPUs of this size are available with limits on usage for free on services like Google Colab and Kaggle.

### On implementation of PointNet++

As mentioned in the introduction, the deep learning code, including the code for PointNet++, is implemented using the PyTorch Geometric library designed for writing and training graph neural networks.
This makes the implementation not exactly the same as described in the PointNet papers.
The main ideas, namely local feature learning in a k-nearest neighbors neighborhood and max pulling for permutation-invariant aggregation, are there.
Input transform and feature transform networks are not defined explicitly, but networks that process features learn to perform a similar function.
