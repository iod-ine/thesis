## Training tree segmentation neural networks

PointNet++ tree segmentation networks.

- Height threshold
- Transformations (augmentations + normalization [@bishop2006pattern]

Early stopping [@precheltAutomaticEarlyStopping1998]

### Augmentations

The primary objective of data augmentation is to enhance the quantity, quality, and variety of data used for training [@mumuniDataAugmentationComprehensive2022].
This is especially important when the sizes of the available datasets are limited, like in the case of the individual trees data that is the base of the synthetic forest datasets.

Carefully selected augmentations allow increasing the effective dataset size.
However, it is important to pay attention that the chosen augmentations keep the transformed examples semantically equivalent to the original.
A readily understandable example of a bad augmentation in the task of digit classification, which is often used as the "hello world" of deep learning with the MNIST dataset of handwritten digits [@deng2012mnist], is a vertical flip, as most digits lose meaning when upside down.
A flip is also a bad augmentation example for a synthetic forest patch described in @sec-synthetic-forest-dataset, as it changes the order of the trees within a patch, thus breaking the labels.

In the scenario when the data for is created by combining several smaller inputs, there is a possibility of applying augmentations on two different scales.
The most simple approach is to treat a synthetic forest patch as a whole and apply augmentations directly to it.
However, it is also possible to apply per-tree augmentations, effectively increasing the size of the underlying tree set from which synthetic forest patches constructed.
Both types of augmentations are used in training tree segmentation networks, described in detail in the following sections.

#### Per-tree augmentations

The first transformation that changes the shape but doesn't affect any semantics for an individual tree is a random rotation around the vertical axis.
A tree remains completely the same when rotated, but the coordinates of all points change.
@fig-random-rotate-effect shows the effect of applying random rotation transformation of different magnitudes to a single aspen tree.
For visualization purposes, the rotation is forced to apply with full magnitude for every parameter.
During training, the angle is uniformly sampled from the specified range.
For the final tree segmentation model, the range is set to $[-180, 180]$ degrees, as no amount of rotation breaks the semantics.

{{< embed 03d_augmentations.ipynb#fig-random-rotate-effect >}}

Another transformation that keeps the tree the same but changes the coordinates of the points is random scaling.
It simply multiplies the coordinates by a scaling factor, making the tree larger or smaller in all directions.
Unlike random rotation around the vertical axis, however, the range needs to be chosen much more carefully, as there is a possibility of making unrealistically large or small trees that would confuse the model during training.
@fig-random-scale-effect shows the effect of applying random scale transformation to a single aspen tree.
Again, for purposes of visualization, the scale is forced to apply with full magnitude.
During training, the scale is uniformly sampled from the specified range.
For the final tree segmentation model, the range is set to $[0.8, 1.2]$.

{{< embed 03d_augmentations.ipynb#fig-random-scale-effect >}}

{{< embed 03d_augmentations.ipynb#fig-random-jitter-effect >}}

Another useful effect augmentations can provide is to make synthetic data look more like real data.
As was mentioned in @sec-individual-trees-dataset, there is a selection bias in the dataset of individual trees: the trees that are easiest to manually separate are exponentially more likely to end up in the data.
As such, almost all the trees have trunks, making them different from the trees in dense forest stands.
One way to mitigate that issue 
A probability threshold that is around zero for the highest points
An example function that satisfies these criteria is a modified sigmoid:

$$
\text{threshold}(z) = \big[1 + e^{z \times \text{scale} + \text{shift}}\big]^{-1},
$$

where $z$ is the height normalized to $[0, 1]$ and reversed by subtraction from 1, range and $\text{scale}$ and $\text{shift}$ are hyperparameters that control the shape of the curve.
@fig-height-dropout shows an example of applying such dropout function to a single aspen tree.

{{< embed 03d_augmentations.ipynb#fig-height-dropout >}}

#### Per-patch augmentations

Random rotation within $[0; 45]$ degrees range.

### On implementation of PointNet++

As mentioned in the introduction, the PointNet++ is implemented using the PyTorch Geometric library designed for writing and training graph neural networks.
This makes the implementation not exactly the same as described in the PointNet papers.
Input transform and feature transfrom networks are not define explicitly, but local networks that process features learn to perform a similar function.
The main ideas, namely local feature learning and max pulling for permutation-invariant aggregation, are there.
Sequential application of PointNetConv layers allows the network to learn hierarchical features.
