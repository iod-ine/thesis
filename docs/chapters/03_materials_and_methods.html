<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.39">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>3&nbsp; Materials and methods â€“ A deep learning framework for mixed dense forests parameter estimation at individual tree scale</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/04_results.html" rel="next">
<link href="../chapters/02_literature_review.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-e26003cea8cd680ca0c55a263523d882.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-626149efe8f5d16e1d391ba177679bf0.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="../site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="../site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="../site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/03_materials_and_methods.html"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Materials and methods</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Tree-scale parameter estimation with deep learning</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/iod-ine/thesis" title="Source Code" class="quarto-navigation-tool px-1" aria-label="Source Code"><i class="bi bi-github"></i></a>
    <a href="../ivan-dubrovin-phd-thesis.pdf" title="Download PDF" class="quarto-navigation-tool px-1" aria-label="Download PDF"><i class="bi bi-file-pdf"></i></a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Acknowledgments <span style="visibility: hidden">ðŸŒ¹</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01_introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Introduction</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02_literature_review.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Literature review</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03_materials_and_methods.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Materials and methods</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04_results.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Results</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05_conclusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Conclusion</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/99_references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#sec-lysva-dataset" id="toc-sec-lysva-dataset" class="nav-link active" data-scroll-target="#sec-lysva-dataset"><span class="header-section-number">3.1</span> Lysva dataset</a>
  <ul class="collapse">
  <li><a href="#sec-intensity-based-features" id="toc-sec-intensity-based-features" class="nav-link" data-scroll-target="#sec-intensity-based-features"><span class="header-section-number">3.1.1</span> On using intensity-based features</a></li>
  <li><a href="#comparison-to-other-datasets" id="toc-comparison-to-other-datasets" class="nav-link" data-scroll-target="#comparison-to-other-datasets"><span class="header-section-number">3.1.2</span> Comparison to other datasets</a></li>
  </ul></li>
  <li><a href="#sec-individual-trees-dataset" id="toc-sec-individual-trees-dataset" class="nav-link" data-scroll-target="#sec-individual-trees-dataset"><span class="header-section-number">3.2</span> Individual tree point clouds dataset</a></li>
  <li><a href="#sec-synthetic-forest-dataset" id="toc-sec-synthetic-forest-dataset" class="nav-link" data-scroll-target="#sec-synthetic-forest-dataset"><span class="header-section-number">3.3</span> Synthetic forests generated from individual trees</a></li>
  <li><a href="#training-tree-segmentation-neural-networks" id="toc-training-tree-segmentation-neural-networks" class="nav-link" data-scroll-target="#training-tree-segmentation-neural-networks"><span class="header-section-number">3.4</span> Training tree segmentation neural networks</a>
  <ul class="collapse">
  <li><a href="#coordinate-and-feature-normalization" id="toc-coordinate-and-feature-normalization" class="nav-link" data-scroll-target="#coordinate-and-feature-normalization"><span class="header-section-number">3.4.1</span> Coordinate and feature normalization</a></li>
  <li><a href="#sec-augmentations" id="toc-sec-augmentations" class="nav-link" data-scroll-target="#sec-augmentations"><span class="header-section-number">3.4.2</span> Data augmentation</a></li>
  <li><a href="#sec-other-training-params" id="toc-sec-other-training-params" class="nav-link" data-scroll-target="#sec-other-training-params"><span class="header-section-number">3.4.3</span> Other training parameters</a></li>
  <li><a href="#on-implementation-of-pointnet" id="toc-on-implementation-of-pointnet" class="nav-link" data-scroll-target="#on-implementation-of-pointnet"><span class="header-section-number">3.4.4</span> On implementation of PointNet++</a></li>
  </ul></li>
  <li><a href="#sec-training-tree-processors" id="toc-sec-training-tree-processors" class="nav-link" data-scroll-target="#sec-training-tree-processors"><span class="header-section-number">3.5</span> Training segmented trees processing models</a></li>
  <li><a href="#sec-matching-algorithm" id="toc-sec-matching-algorithm" class="nav-link" data-scroll-target="#sec-matching-algorithm"><span class="header-section-number">3.6</span> Matching detections to field inventory ground truth</a></li>
  <li><a href="#application-of-the-framework" id="toc-application-of-the-framework" class="nav-link" data-scroll-target="#application-of-the-framework"><span class="header-section-number">3.7</span> Application of the framework</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/iod-ine/thesis/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/iod-ine/thesis/blob/main/chapters/03_materials_and_methods.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span id="sec-materials-and-methods" class="quarto-section-identifier"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Materials and methods</span></span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>This chapter describes in detail used datasets, processing techniques and the methods, and comments on the reasoning behind the choices. The code of the project on my GitHub might be a helpful addition for that.</p>
<section id="sec-lysva-dataset" class="level2" data-number="3.1">
<h2 data-number="3.1" class="anchored" data-anchor-id="sec-lysva-dataset"><span class="header-section-number">3.1</span> Lysva dataset</h2>
<p>The main original dataset used in the study is the Lysva field inventory dataset, named by the closest town to its location. The dataset is released into open access with an accompanying paper that describes the data in detail and provides a basic baseline for individual tree detection <span class="citation" data-cites="dubrovinExplorationPropertiesPoint2024">(<a href="99_references.html#ref-dubrovinExplorationPropertiesPoint2024" role="doc-biblioref">Dubrovin and Fortin 2024</a>)</span>. The study area is located in Perm Krai, Russia, 86 kilometers to the east of Perm. The forest in the region is natural, with the average age of coniferous trees over 85 years and deciduous over 60 years. The dataset consists of a field inventory of 3600 trees across 10 rectangular ground plots 100 meters in lengths and 50 meters in width fully covered by a UAV LiDAR and RGB orthophoto surveys. <a href="#fig-lysva-roi" class="quarto-xref">Figure&nbsp;<span>3.1</span></a> shows the locations of the ground plots over the full size RGB orthophoto and a visualization of the field inventory for a single plot on top of the LiDAR point cloud. The low vegetation areas visible on the orthophoto are naturally regenerating old logging and agricultural sites. No artificial afforestation has been done in the area. Colored points represent trees, with different colors mapping to different species. The point cloud is visualized as a 2D scatter plot with points colored by height (darker points are lower, brighter points are higher, and points are unsorted â€“ some lower points end up over higher ones). The baseline is a simple one-pass local maxima filter applied directly on the point cloud with a fixed window size.</p>
<div id="fig-lysva-roi" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lysva-roi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="../images/lysva_roi.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;3.1: The study region for the Lysva field inventory dataset. Left: The locations of field survey plot boundaries on a full-size RGB orthophoto. Each plot is a 50 by 100 meter rectangle, with every tree with dbh of 6.1 cm or higher measured and recorded. Buffered cutouts of the orthophoto come with the dataset, along with LiDAR point clouds. Right: A close up of plot number 4. Each colored point represents a single tree of a different species, on top of a point cloud colored by the value of the height of each point (lower points are dark, higher points are bright) and the same orthophoto. Note that the points of the point cloud are unsorted, and some lower points overlap higher points. Figure reused from @dubrovinOpenDatasetIndividual2024."><img src="../images/lysva_roi.jpg" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lysva-roi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.1: The study region for the Lysva field inventory dataset. <strong>Left</strong>: The locations of field survey plot boundaries on a full-size RGB orthophoto. Each plot is a 50 by 100 meter rectangle, with every tree with dbh of 6.1 cm or higher measured and recorded. Buffered cutouts of the orthophoto come with the dataset, along with LiDAR point clouds. <strong>Right</strong>: A close up of plot number 4. Each colored point represents a single tree of a different species, on top of a point cloud colored by the value of the height of each point (lower points are dark, higher points are bright) and the same orthophoto. Note that the points of the point cloud are unsorted, and some lower points overlap higher points. Figure reused from <span class="citation" data-cites="dubrovinOpenDatasetIndividual2024">Dubrovin, Fortin, and Kedrov (<a href="99_references.html#ref-dubrovinOpenDatasetIndividual2024" role="doc-biblioref">2024</a>)</span>.
</figcaption>
</figure>
</div>
<p>The field inventory is a tabular dataset where every row represents a single tree. According to the state-mandated requirements that were in place during the collection of the data, all trees with dbh starting from 6.1 centimeters were included. <a href="#tbl-inventory-example" class="quarto-xref">Table&nbsp;<span>3.1</span></a> shows a random sample of entries from the field inventory table. Every tree is represented by a point in UTM 40N coordinate reference system (EPSG:32640). The coordinates of the trees were determined using the South NTS-360R total station with a reflector prism using the closed traverse method. The total station measurements used two points basis determined using a dual-frequency South G1 Plus IMU GNSS receiver in RTK mode. The basis points were selected to have reliable satellite signal. The average error of coordinate measurements is 10 centimeters, with a maximum of 30 centimeters. Every tree has a species label and diameter at breast height (dbh), measured with calipers at 1.3 m from the ground at two perpendicular directions and averaged. <a href="#fig-lysva-species-distribution" class="quarto-xref">Figure&nbsp;<span>3.2</span></a> shows the distribution of species in the data: the dominant species is spruce, but overall the trees are evenly split between deciduous and coniferous, 1793 and 1807 respectively, with seven species in total: spruce, birch, fir, aspen, tilia, alder, and willow. Approximately 20% of the trees have height data measured during the inventory, and 10% have ages measured on core samples, shown in the table in meters and years respectively.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03a_lysva_dataset.ipynb" data-notebook-cellid="cell-tbl-inventory-example">
<div class="cell" data-execution_count="6">
<div id="tbl-inventory-example" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="6">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-inventory-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.1: Example of data in the field inventory table. Each row is a recorded tree.
</figcaption>
<div aria-describedby="tbl-inventory-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">plot</th>
<th data-quarto-table-cell-role="th">tree_no</th>
<th data-quarto-table-cell-role="th">species</th>
<th data-quarto-table-cell-role="th">d1</th>
<th data-quarto-table-cell-role="th">d2</th>
<th data-quarto-table-cell-role="th">dbh</th>
<th data-quarto-table-cell-role="th">age</th>
<th data-quarto-table-cell-role="th">height</th>
<th data-quarto-table-cell-role="th">angle</th>
<th data-quarto-table-cell-role="th">comment</th>
<th data-quarto-table-cell-role="th">geometry</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>7.0</td>
<td>111.0</td>
<td>Birch</td>
<td>23.0</td>
<td>23.0</td>
<td>23.00</td>
<td>NaN</td>
<td>NaN</td>
<td>0.0</td>
<td>None</td>
<td>POINT (545784.419 6449359.132)</td>
</tr>
<tr class="even">
<td>5.0</td>
<td>136.0</td>
<td>Fir</td>
<td>23.5</td>
<td>23.0</td>
<td>23.25</td>
<td>90.0</td>
<td>17.5</td>
<td>0.0</td>
<td>Rotten</td>
<td>POINT (545721.242 6449901.051)</td>
</tr>
<tr class="odd">
<td>3.0</td>
<td>119.0</td>
<td>Aspen</td>
<td>36.1</td>
<td>42.1</td>
<td>39.10</td>
<td>89.0</td>
<td>25.5</td>
<td>0.0</td>
<td>None</td>
<td>POINT (546630.326 6450158.241)</td>
</tr>
<tr class="even">
<td>9.0</td>
<td>345.0</td>
<td>Spruce</td>
<td>19.7</td>
<td>22.0</td>
<td>20.85</td>
<td>NaN</td>
<td>15.9</td>
<td>0.0</td>
<td>None</td>
<td>POINT (546201.645 6449118.199)</td>
</tr>
<tr class="odd">
<td>6.0</td>
<td>267.0</td>
<td>Spruce</td>
<td>12.9</td>
<td>12.9</td>
<td>12.90</td>
<td>NaN</td>
<td>NaN</td>
<td>0.0</td>
<td>None</td>
<td>POINT (545568.842 6449407.13)</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</div>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03a_lysva_dataset.ipynb" data-notebook-cellid="cell-fig-lysva-species-distribution">
<div id="cell-fig-lysva-species-distribution" class="cell" data-execution_count="14">
<div class="cell-output cell-output-display">
<div id="fig-lysva-species-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lysva-species-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-lysva-species-distribution-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;3.2: Distribution of species in the Lysva field inventory dataset. The dominant species is spruce, but overall the split between coniferous and decidious species is even: there are 1807 coniferous and 1793 deciduous trees. Figure reused from @dubrovinOpenDatasetIndividual2024."><img src="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-lysva-species-distribution-output-1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lysva-species-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.2: Distribution of species in the Lysva field inventory dataset. The dominant species is spruce, but overall the split between coniferous and decidious species is even: there are 1807 coniferous and 1793 deciduous trees. Figure reused from <span class="citation" data-cites="dubrovinOpenDatasetIndividual2024">Dubrovin, Fortin, and Kedrov (<a href="99_references.html#ref-dubrovinOpenDatasetIndividual2024" role="doc-biblioref">2024</a>)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>The LiDAR sensor used for the survey is AGM-MS3 produced by AGM Systems. It has 640 kHz acquisition rate, 300-meter range, and spatial accuracy of 3â€“5 centimeters. The raw point clouds were processed with the combination of the AGM ScanWorks software from the sensor vendor and the TerraScan software. The point clouds were preprocessed by removing duplicate points and high and low noise points. The duplicate removal was run with a threshold distance between points of 1 mm. Noise was removed by visually inspecting the point cloud and manually selecting height thresholds to cut off points that are lower than the ground or higher than the canopies. Ground point classification was performed and ground points were used to normalize height by subtracting the ground level from the Z coordinate of every point. Height normalization allows treating the Z coordinate as height above ground rather than the absolute elevation, which simplifies many subsequent steps. The camera used to collect raw digital images for creating the orthophoto is Sony A6000. Agisoft Metashape software was used to generate the orthophoto mosaic. A canopy-height map was created from the LiDAR point cloud and used as a reference for adjusting the planar coordinates of the orthophoto to align them. The resolution of the orthophoto is 7 centimeters per pixel. <a href="#fig-example-3d-point-cloud" class="quarto-xref">Figure&nbsp;<span>3.3</span></a> is a 3D visualization of the point cloud over plot number 10. It shows the unmodified point cloud on the left, with points colored by height above ground, and a point cloud enriched with color information by sampling the orthophoto at the planar coordinates of the points. <a href="#fig-example-ortho" class="quarto-xref">Figure&nbsp;<span>3.4</span></a> shows the orthophoto for the same plot. The carrier UAV used for the LiDAR survey is DJI Matrice 600 Pro hexacopter. The speed during the survey was 10 meters per second. The UAV was configured to follow the terrain at 150 meter height using the SRTM elevation map as a reference <span class="citation" data-cites="farrShuttleRadarTopography2000">(<a href="99_references.html#ref-farrShuttleRadarTopography2000" role="doc-biblioref">Farr and Kobrick 2000</a>)</span>. The carrier UAV used for the orthophoto survey is the fixed-wing Geoscan 201. The speed during the survey was 16.6 meters per second.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03a_lysva_dataset.ipynb" data-notebook-cellid="cell-fig-example-3d-point-cloud">
<div id="fig-example-3d-point-cloud" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-example-3d-point-cloud-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-example-3d-point-cloud" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-example-3d-point-cloud-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-example-3d-point-cloud-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-example-3d-point-cloud-output-1.png" class="lightbox" data-gallery="fig-example-3d-point-cloud" title="Figure&nbsp;3.3&nbsp;(a): Points colored by height."><img src="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-example-3d-point-cloud-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-example-3d-point-cloud"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-example-3d-point-cloud-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Points colored by height.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-example-3d-point-cloud" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-example-3d-point-cloud-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-example-3d-point-cloud-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-example-3d-point-cloud-output-2.png" class="lightbox" data-gallery="fig-example-3d-point-cloud" title="Figure&nbsp;3.3&nbsp;(b): Points assigned color by sampling the orthophoto."><img src="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-example-3d-point-cloud-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-example-3d-point-cloud"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-example-3d-point-cloud-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) Points assigned color by sampling the orthophoto.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-example-3d-point-cloud-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.3: A 3D visualization of the UAV LiDAR point cloud of plot 10.
</figcaption>
</figure>
</div>
</div>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03a_lysva_dataset.ipynb" data-notebook-cellid="cell-fig-example-ortho">
<div id="cell-fig-example-ortho" class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<div id="fig-example-ortho" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-example-ortho-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-example-ortho-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;3.4: A visualization of the orthophoto of plot 10."><img src="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-example-ortho-output-1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-example-ortho-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.4: A visualization of the orthophoto of plot 10.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p><a href="#tbl-lysva-plot-stats" class="quarto-xref">Table&nbsp;<span>3.2</span></a> shows some descriptive statistics for each plot in the field inventory: the number of trees in the plot, average LiDAR point density in points per square meter, and dominant species type. The overall average point density is 37 points per square meter. Exactly half of the plots are predominantly coniferous and half are predominantly deciduous. <a href="#fig-lysva-canopy-structure" class="quarto-xref">Figure&nbsp;<span>3.5</span></a> shows two point clouds clipped by plot bounds in 3D, highlighting the differences in canopy structure complexity between predominantly deciduous and coniferous plots. The figure highlights that the forest is indeed dense and mixed, with non-uniform, complex canopy structure.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03a_lysva_dataset.ipynb" data-notebook-cellid="cell-tbl-lysva-plot-stats">
<div class="cell" data-execution_count="8">
<div id="tbl-lysva-plot-stats" class="cell quarto-float quarto-figure quarto-figure-center anchored" data-execution_count="8">
<figure class="quarto-float quarto-float-tbl figure">
<figcaption class="quarto-float-caption-top quarto-float-caption quarto-float-tbl" id="tbl-lysva-plot-stats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Table&nbsp;3.2: Statistics for the plots in the Lysva dataset.
</figcaption>
<div aria-describedby="tbl-lysva-plot-stats-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="cell-output cell-output-display">
<table class="dataframe do-not-create-environment cell caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th">Plot</th>
<th data-quarto-table-cell-role="th">Tree count</th>
<th data-quarto-table-cell-role="th">Point density</th>
<th data-quarto-table-cell-role="th">Dominant type</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1.0</td>
<td>420</td>
<td>31.7</td>
<td>Deciduous</td>
</tr>
<tr class="even">
<td>2.0</td>
<td>365</td>
<td>47.9</td>
<td>Deciduous</td>
</tr>
<tr class="odd">
<td>3.0</td>
<td>332</td>
<td>40.3</td>
<td>Deciduous</td>
</tr>
<tr class="even">
<td>4.0</td>
<td>261</td>
<td>33.5</td>
<td>Coniferous</td>
</tr>
<tr class="odd">
<td>5.0</td>
<td>208</td>
<td>14.2</td>
<td>Coniferous</td>
</tr>
<tr class="even">
<td>6.0</td>
<td>290</td>
<td>39.1</td>
<td>Coniferous</td>
</tr>
<tr class="odd">
<td>7.0</td>
<td>408</td>
<td>41.9</td>
<td>Deciduous</td>
</tr>
<tr class="even">
<td>8.0</td>
<td>341</td>
<td>35.5</td>
<td>Coniferous</td>
</tr>
<tr class="odd">
<td>9.0</td>
<td>459</td>
<td>42.1</td>
<td>Coniferous</td>
</tr>
<tr class="even">
<td>10.0</td>
<td>518</td>
<td>42.9</td>
<td>Deciduous</td>
</tr>
</tbody>
</table>
</div>
</div>
</figure>
</div>
</div>
</div>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03a_lysva_dataset.ipynb" data-notebook-cellid="cell-fig-lysva-canopy-structure">
<div id="cell-fig-lysva-canopy-structure" class="cell" data-execution_count="11">
<div class="cell-output cell-output-display">
<div id="fig-lysva-canopy-structure" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lysva-canopy-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-lysva-canopy-structure-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;3.5: 3D visualizations of point clouds from plot 1 (predominantly deciduous) and plot 6 (predominantly coniferous). Note the difference in the canopy structure: it is relatively easy to tell confifiers apart visually, while deciduous species do not have pronounced shapes and are hard to discriminate. Figure reused from @dubrovinOpenDatasetIndividual2024."><img src="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-lysva-canopy-structure-output-1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lysva-canopy-structure-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.5: 3D visualizations of point clouds from plot 1 (predominantly deciduous) and plot 6 (predominantly coniferous). Note the difference in the canopy structure: it is relatively easy to tell confifiers apart visually, while deciduous species do not have pronounced shapes and are hard to discriminate. Figure reused from <span class="citation" data-cites="dubrovinOpenDatasetIndividual2024">Dubrovin, Fortin, and Kedrov (<a href="99_references.html#ref-dubrovinOpenDatasetIndividual2024" role="doc-biblioref">2024</a>)</span>.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<section id="sec-intensity-based-features" class="level3" data-number="3.1.1">
<h3 data-number="3.1.1" class="anchored" data-anchor-id="sec-intensity-based-features"><span class="header-section-number">3.1.1</span> On using intensity-based features</h3>
<p>Even though some sources report intensity-based features as some of the most important ones <span class="citation" data-cites="shiImportantLiDARMetrics2018">(<a href="99_references.html#ref-shiImportantLiDARMetrics2018" role="doc-biblioref">Shi et al. 2018</a>)</span>, the features seem to me unreliable in the context of forestry because of the physics of light reflection. There are simply too many factors that affect the amplitude of the reflected signal, which might be useful when imaging stable targets such as urban environments but become completely unpredictable on highly unstable targets such as trees: they move in the wind in the time span of a single survey, they grow in the time span between repeated surveys, the leaves and branches are angled in every possible way. Moreover, the quality of the recorded intensities highly depends on the used sensor. As an example, <a href="#fig-intensity-with-example" class="quarto-xref">Figure&nbsp;<span>3.6</span></a> shows the distribution of intensity values for every point in the Lysva dataset, and shows plot number 10 in 3D with points colored by their recorded intensity. The distribution of intensities seems like an artifact of faulty quantization (the fact that the maximum overall value is 63 makes me suspect it is stored by the hardware using a 6-bit unsigned integer, probably an ad hoc optimization by the sensor vendor). With this distribution in mind, it is not surprising that coloring points by their intensity values results in images that look like noise, and there is no signal to be exploited for predictive modeling.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03a_lysva_dataset.ipynb" data-notebook-cellid="cell-fig-intensity-with-example">
<div id="fig-intensity-with-example" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-intensity-with-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-intensity-with-example" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-intensity-with-example-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-intensity-with-example-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-intensity-with-example-output-1.png" class="lightbox" data-gallery="fig-intensity-with-example" title="Figure&nbsp;3.6&nbsp;(a): Distribution of intensity over all points in the Lysva dataset."><img src="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-intensity-with-example-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-intensity-with-example"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-intensity-with-example-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Distribution of intensity over all points in the Lysva dataset.
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-intensity-with-example" style="flex-basis: 50.0%;justify-content: flex-start;">
<div id="fig-intensity-with-example-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-intensity-with-example-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-intensity-with-example-output-2.png" class="lightbox" data-gallery="fig-intensity-with-example" title="Figure&nbsp;3.6&nbsp;(b): A point cloud of plot 10 with points colored by intensity."><img src="03_materials_and_methods_files/figure-html/03a_lysva_dataset-fig-intensity-with-example-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-intensity-with-example"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-intensity-with-example-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) A point cloud of plot 10 with points colored by intensity.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-intensity-with-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.6: An example of the unreliability of the intensity attribute.
</figcaption>
</figure>
</div>
</div>
<p>Some sensors do provide more consistent values. However, relying on intensity-based features limits the applicability of developed models and methods, as any such model would surely fail on data like this.</p>
</section>
<section id="comparison-to-other-datasets" class="level3" data-number="3.1.2">
<h3 data-number="3.1.2" class="anchored" data-anchor-id="comparison-to-other-datasets"><span class="header-section-number">3.1.2</span> Comparison to other datasets</h3>
<p>Our dataset is in many ways similar to the NewFor benchmark <span class="citation" data-cites="eysnAlpineITDBenchmark2015">(<a href="99_references.html#ref-eysnAlpineITDBenchmark2015" role="doc-biblioref">Eysn et al. 2015</a>)</span>. It serves the same purpose and also offers overlapping field survey ground plots and UAV LiDAR point clouds. There are, however, many notable differences. The NewFor benchmark covers much more diverse regions, including ground plots from France, Italy, Switzerland, Austria, and Slovenia, while all our data comes from the same area. The tree species covered by the datasets are also different: both contain spruce and fir, but the NewFor data also has beech, Scots pine, larch, sycamore, and poplar, while ours also has birch, aspen, tilia, alder, and willow. Our dataset has more than twice as many individual trees as the Alpine benchmark, and the forest is denser and more complex, making it more complicated to detect trees in. Our data contains very mild terrain variations, while the slopes of the terrain in Alpine data are very steep, which plays a role during height normalization, since subtraction of steep terrain introduces artificial slope to the points in the canopy, changing the overall shape of the tree. Our dataset has an additional information source â€“ an RGB orthophoto that allows development of algorithms that fuse multimodal data, which, we believe, is a key to success in such complex environments. Our dataset has species labels for every surveyed tree, but only partial coverage of tree heights and no timber volume information at all.</p>
<p>Another similar dataset is the NeonTreeEvaluation Benchmark <span class="citation" data-cites="weinsteinDataNeonTreeEvaluationBenchmark2022">(<a href="99_references.html#ref-weinsteinDataNeonTreeEvaluationBenchmark2022" role="doc-biblioref">Weinstein, Marconi, and White 2022</a>)</span>, which offers bounding box annotation for tree detection across a wide range of different forest types. It offers coregistered RGB, LiDAR, and hyperspectral images over 31,000 individual trees. The main difference in the reference data between the NeonTreeEvaluation Benchmark and our dataset is the source: our data comes from a field inventory and thus has additional tree information that can be used in downstream tasks, such as species classification or timber volume prediction, while the NeonTreeEvaluation Benchmark annotations are created from the RGB photo and thus only offer the positions and sized of trees.</p>
<p>Another dataset is the IDTReeS 2020 Competition Data <span class="citation" data-cites="gravesIDTReeS2020Competition2020">(<a href="99_references.html#ref-gravesIDTReeS2020Competition2020" role="doc-biblioref">Graves and Marconi 2020</a>)</span> aimed to develop algorithms for delineation and species classification of individual tree crowns in RGB, LiDAR, and hyperspectral data. It offers bounding box annotations for 1200 individual trees covered by RGB, LiDAR, and hyperspectral images in 3 national forests in the USA. Similarly, the source of the data is annotation of images, not a field inventory.</p>
<p>There are also datasets available that do not have LiDAR point cloud coverage, or use terrestrial LiDAR instead of UAV LiDAR, or use photogrammetric point clouds instead of LiDAR point clouds. We do not mention them here because we specifically focus on UAV LiDAR.</p>
</section>
</section>
<section id="sec-individual-trees-dataset" class="level2" data-number="3.2">
<h2 data-number="3.2" class="anchored" data-anchor-id="sec-individual-trees-dataset"><span class="header-section-number">3.2</span> Individual tree point clouds dataset</h2>
<p>The main dataset used for training the models is a collection of point clouds of individual trees, sometimes referred to in this text as tree clouds, extracted manually from larger UAV LiDAR point clouds. The dataset is released into open access, and was originally presented in <span class="citation" data-cites="dubrovinExplorationPropertiesPoint2024">(<a href="99_references.html#ref-dubrovinExplorationPropertiesPoint2024" role="doc-biblioref">Dubrovin and Fortin 2024</a>)</span>, although it has been expanded since and now contains twice as many individual trees. It consists of 394 trees, 192 of which are extracted from the previously described Lysva survey, and 202 from other surveys in Perm Krai. The distinction between the parts is important because the Lysva dataset has RGB orthophoto coverage, making it possible to infuse the tree clouds with orthophoto-based features. Thus, for training the tree segmentation networks that rely on these features, the effective size of the dataset is 192 tree clouds, as only the former part is used. However, the whole dataset is used for training regression and classification models that process segmented trees.</p>
<p><a href="#fig-individual-trees-species-distribution" class="quarto-xref">Figure&nbsp;<span>3.7</span></a> shows the distribution of species in the individual tree point clouds dataset. The split between coniferous and deciduous trees is almost even: there are 202 coniferous and 193 deciduous trees. There are seven species in total: spruce, birch, aspen, pine, fir, alder, and tilia, with most focus on 4 most important species listed first. Note the presence of pine trees, which are not present in the Lysva field inventory data, and the absence of willow trees. All the pine trees are from other field surveys. Willows are skipped intentionally, since they are of little interest in terms of timber harvesting: they are considered low quality, and their ripening cycle is mismatched with main timber species â€“ when the overall plot is ready to be harvested, the willows are already rotten.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03b_individual_trees.ipynb" data-notebook-cellid="cell-fig-individual-trees-species-distribution">
<div id="cell-fig-individual-trees-species-distribution" class="cell" data-execution_count="3">
<div class="cell-output cell-output-display">
<div id="fig-individual-trees-species-distribution" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-individual-trees-species-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03b_individual_trees-fig-individual-trees-species-distribution-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;3.7: Distribution of tree species in the individual trees dataset. It contains 394 point clouds of individual trees: 201 coniferous and 193 deciduous. Note the presence of pine trees, as there are no pine trees in the Lysva field inventory â€“ all of the pines come from other field surveys."><img src="03_materials_and_methods_files/figure-html/03b_individual_trees-fig-individual-trees-species-distribution-output-1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-individual-trees-species-distribution-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.7: Distribution of tree species in the individual trees dataset. It contains 394 point clouds of individual trees: 201 coniferous and 193 deciduous. Note the presence of pine trees, as there are no pine trees in the Lysva field inventory â€“ all of the pines come from other field surveys.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p><a href="#fig-individual-trees-visualization" class="quarto-xref">Figure&nbsp;<span>3.8</span></a> is a visualization of the data. It shows a random tree of every species as a 2D scatter plot and a single spruce as a 3D scatter plot with points colored by height. Because the observations are made from above, many trees have the highest concentrations of points at the top of their canopy and a very limited number of points along the trunk. Additionally, slight slopes of the terrain manifest as artificial tilt in some of the trees because of the height normalization of the original point cloud.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03b_individual_trees.ipynb" data-notebook-cellid="cell-fig-individual-trees-visualization">
<div class="cell" data-layout="[[2.5,1]]" data-execution_count="6">
<div id="fig-individual-trees-visualization" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-individual-trees-visualization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-individual-trees-visualization" style="flex-basis: 71.4%;justify-content: flex-start;">
<div id="fig-individual-trees-visualization-1" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-individual-trees-visualization-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03b_individual_trees-fig-individual-trees-visualization-output-1.png" class="lightbox" data-gallery="fig-individual-trees-visualization" title="Figure&nbsp;3.8&nbsp;(a): Cross-sections of random trees of every species (ignoring the Y dimension)."><img src="03_materials_and_methods_files/figure-html/03b_individual_trees-fig-individual-trees-visualization-output-1.png" class="img-fluid figure-img" data-ref-parent="fig-individual-trees-visualization"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-individual-trees-visualization-1-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(a) Cross-sections of random trees of every species (ignoring the Y dimension).
</figcaption>
</figure>
</div>
</div>
<div class="cell-output cell-output-display quarto-layout-cell-subref quarto-layout-cell" data-ref-parent="fig-individual-trees-visualization" style="flex-basis: 28.6%;justify-content: flex-start;">
<div id="fig-individual-trees-visualization-2" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-subfloat-fig figure">
<div aria-describedby="fig-individual-trees-visualization-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03b_individual_trees-fig-individual-trees-visualization-output-2.png" class="lightbox" data-gallery="fig-individual-trees-visualization" title="Figure&nbsp;3.8&nbsp;(b): A spruce in 3D."><img src="03_materials_and_methods_files/figure-html/03b_individual_trees-fig-individual-trees-visualization-output-2.png" class="img-fluid figure-img" data-ref-parent="fig-individual-trees-visualization"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-subfloat-caption quarto-subfloat-fig" id="fig-individual-trees-visualization-2-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
(b) A spruce in 3D.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-individual-trees-visualization-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.8: Visualizations of the individual tree point clouds in the dataset. Most of the tree clouds are top-heavy because of the observation from above, and some are artificially tilted because of slight terrain slopes and height normalization. The ground points are present.
</figcaption>
</figure>
</div>
</div>
</div>
<p>An important note is that the extracted trees were not chosen for extraction randomly but were selected by humans based on whether it was possible and relatively easy to separate from the surrounding trees. So there is a selection bias in there favoring the trees that are easily separable, standing outside of large dense clusters. Because of that the trees do not exactly represent what a tree closely surrounded by other trees is like in a point cloud. It is especially apparent in very pronounced trunks of all the visualized trees. In a dense forest, such as the one visualized in <a href="#fig-lysva-canopy-structure" class="quarto-xref">Figure&nbsp;<span>3.5</span></a>, there is hardly ever enough penetration for such detailed trunk coverage. In fact, as mentioned in the literature review, good coverage of trunks is an immensely useful feature of terrestrial LiDAR surveys, which consistently show very good results using algorithm that segment trees from the trunk up.</p>
</section>
<section id="sec-synthetic-forest-dataset" class="level2" data-number="3.3">
<h2 data-number="3.3" class="anchored" data-anchor-id="sec-synthetic-forest-dataset"><span class="header-section-number">3.3</span> Synthetic forests generated from individual trees</h2>
<p>To train tree segmentation models, training data where every point is assigned to an individual tree is required. Data like that are very labor-intensive to label. An alternative way to create such data is by generating it from a set of tree clouds, as the per point labels arise naturally in this case. I used the dataset of individual tree point clouds described in the previous section to generate a synthetic forest to train a tree segmentation PointNet++.</p>
<p>Synthetic forest is generated in patches of set height and width by sampling individual trees from the full set and placing them from left to right, until the set width is extended, then from bottom to top, until the set height is extended. A height threshold is applied to each tree before placing it to remove ground points and non-tree reflections. This results in patches that are slightly bigger than the set size, but that can be cropped into the exact size. This has an added benefit of mimicking cutoff trees at the edges of the patch that are inevitable when the model is applied in a sliding window. The trees can be sampled with or without replacement, but since a set of augmentations described in <a href="#sec-augmentations" class="quarto-xref"><span>Section 3.4.2</span></a> is applied to each tree individually, exact same tree never occurs in the patch fed to the model even when sampling with replacement. When trees are placed, their planar bounding boxes are tracked to avoid overlap. However, since some overlap might, in fact, be desired to better represent actual forests, a parameter that controls the amount of overlap is added to the dataset generation process. Each tree is also assigned a label, which simply tracks its ordinal number in the patch. An example of a 20 by 20 meter synthetic forest patch with 0.75 meter overlap and 2 meter height threshold is shown in <a href="#fig-synthetic-forest-patch-example" class="quarto-xref">Figure&nbsp;<span>3.9</span></a> from two different perspectives and using two color schemes: the label, i.e.&nbsp;the ordinal ID of the tree within a patch, and RGB color sampled from the orthophoto.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03c_synthetic_forest.ipynb" data-notebook-cellid="cell-fig-synthetic-forest-patch-example">
<div id="cell-fig-synthetic-forest-patch-example" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div id="fig-synthetic-forest-patch-example" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-scap="Visualisation of a synthetic forest patch used for training">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-synthetic-forest-patch-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03c_synthetic_forest-fig-synthetic-forest-patch-example-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;3.9: A visualization of a synthetic forest patch used for training the tree segmentation network. Patch width and height are set to 20 meters, overlap is set to 0.75. Top: Top-down view of the patch. Bottom: 3D view of the same patch. Left: Points colored by label: unique tree ID within the patch. Right: Points colored by RGB color sampled from the orthophoto."><img src="03_materials_and_methods_files/figure-html/03c_synthetic_forest-fig-synthetic-forest-patch-example-output-1.png" class="img-fluid figure-img" data-fig-scap="Visualisation of a synthetic forest patch used for training"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-synthetic-forest-patch-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.9: A visualization of a synthetic forest patch used for training the tree segmentation network. Patch width and height are set to 20 meters, overlap is set to 0.75. <strong>Top:</strong> Top-down view of the patch. <strong>Bottom:</strong> 3D view of the same patch. <strong>Left:</strong> Points colored by label: unique tree ID within the patch. <strong>Right:</strong> Points colored by RGB color sampled from the orthophoto.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>For comparison, a patch of the same size from a real point cloud over one of the plots of the Lysva survey is shown in <a href="#fig-lysva-plot-window-example" class="quarto-xref">Figure&nbsp;<span>3.10</span></a>.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03c_synthetic_forest.ipynb" data-notebook-cellid="cell-fig-lysva-plot-window-example">
<div id="cell-fig-lysva-plot-window-example" class="cell" data-execution_count="13">
<div class="cell-output cell-output-display">
<div id="fig-lysva-plot-window-example" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-scap="Visualisation of a synthetic forest patch used for training">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-lysva-plot-window-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03c_synthetic_forest-fig-lysva-plot-window-example-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;3.10: A piece of the Lysva LiDAR point cloud over plot 10 in within a 20 by 20 meter window. Top: Top-down view. Bottom: The same patch in 3D view. Left: Points colored by height. Right: Points colored by RGB color sampled from the orthophoto."><img src="03_materials_and_methods_files/figure-html/03c_synthetic_forest-fig-lysva-plot-window-example-output-1.png" class="img-fluid figure-img" data-fig-scap="Visualisation of a synthetic forest patch used for training"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-lysva-plot-window-example-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.10: A piece of the Lysva LiDAR point cloud over plot 10 in within a 20 by 20 meter window. <strong>Top:</strong> Top-down view. <strong>Bottom:</strong> The same patch in 3D view. <strong>Left:</strong> Points colored by height. <strong>Right:</strong> Points colored by RGB color sampled from the orthophoto.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>An alternative approach to creating patches of synthetic forest can be to use a fixed number of trees instead of fixed patch size. However, it poorly correlates with how the final model will be applied: in windows of fixed size, not windows with varying size but fixed number of trees. And size of the patch the model sees is important because the scale is normalized, and using a different patch sizes changes the scaled coordinates and confuses the model that learned to rely on them.</p>
<p>Enriching point clouds using only color information from RGB orthophotos provides limited utility because it adds only local information to the points. As was mentioned in the introduction, one of the aspects of the complementary nature of the two data sources is that images provide continuous representations and capture textures. To enrich the image features with context, they can be preprocessed with feature extractors that encode context into pixel values and thus provide more information for the segmentation network. The feature extractors can be simple algorithms or specialized convolutional network feature extractors. For the same reason a simple PointNet++ is used as the architecture for the tree segmentation network, a simple collection of multi-scale features are used as orthophoto features, including intensity, edge, and texture features, extracted from the orthophotos using the <code>scikit-image</code> Python package. The features are calculated on different scales by applying Gaussian smoothing with varying parameters before calculation. <a href="#fig-basic-features-small" class="quarto-xref">Figure&nbsp;<span>3.11</span></a> shows example features from every mentioned group on a very fine scale. The top left image is the original orthophoto of plot 10 from the Lysva survey, the top right image is the intensity calculated from it, and bottom images are examples of an edge feature and a texture feature. The same features but on a coarser scale are shown in <a href="#fig-basic-features-large" class="quarto-xref">Figure&nbsp;<span>3.12</span></a>.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03c_synthetic_forest.ipynb" data-notebook-cellid="cell-fig-basic-features-small">
<div id="cell-fig-basic-features-small" class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div id="fig-basic-features-small" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-scap="Visualisation of a basic orthophoto-based features (small scale)">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-basic-features-small-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03c_synthetic_forest-fig-basic-features-small-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;3.11: Basic orthophoto-based features used (on a single scale with sigma=1, actual features are across multiple increasing scales). Top left: The original orthophoto for plot 10. Top right: Intensity feature. Bottom left: Edges feature. Bottom right: Texture feature."><img src="03_materials_and_methods_files/figure-html/03c_synthetic_forest-fig-basic-features-small-output-1.png" class="img-fluid figure-img" data-fig-scap="Visualisation of a basic orthophoto-based features (small scale)"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-basic-features-small-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.11: Basic orthophoto-based features used (on a single scale with sigma=1, actual features are across multiple increasing scales). <strong>Top left</strong>: The original orthophoto for plot 10. <strong>Top right</strong>: Intensity feature. <strong>Bottom left</strong>: Edges feature. <strong>Bottom right</strong>: Texture feature.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03c_synthetic_forest.ipynb" data-notebook-cellid="cell-fig-basic-features-large">
<div id="cell-fig-basic-features-large" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div id="fig-basic-features-large" class="quarto-float quarto-figure quarto-figure-center anchored" data-fig-scap="Visualisation of a basic orthophoto-based features (large scale)">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-basic-features-large-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03c_synthetic_forest-fig-basic-features-large-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;3.12: Basic orthophoto-based features used (on a single scale with sigma=16, actual features are across multiple increasing scales). Top left: The original orthophoto for plot 10. Top right: Intensity feature. Bottom left: Edges feature. Bottom right: Texture feature."><img src="03_materials_and_methods_files/figure-html/03c_synthetic_forest-fig-basic-features-large-output-1.png" class="img-fluid figure-img" data-fig-scap="Visualisation of a basic orthophoto-based features (large scale)"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-basic-features-large-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.12: Basic orthophoto-based features used (on a single scale with sigma=16, actual features are across multiple increasing scales). <strong>Top left</strong>: The original orthophoto for plot 10. <strong>Top right</strong>: Intensity feature. <strong>Bottom left</strong>: Edges feature. <strong>Bottom right</strong>: Texture feature.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>It is worth mentioning that using synthetic data for training comes with potential biases that are important to keep in mind. The biases present in the source dataset can easily become exaggerated. In this case, the selection bias mentioned in <a href="#sec-individual-trees-dataset" class="quarto-xref"><span>Section 3.2</span></a> is the main potential culprit. The limited size of the source dataset also limits the variability of the examples, which can easily lead to complex overfitting. In this case, the range of shapes of the trees within each species is small, making it hard to generalize well. Moreover, different species will most likely have different accuracies associated with them. Models trained entirely on synthetic datasets are also a lot harder to properly evaluate. These drawbacks are partially addressed by carefully choosing the augmentations for the training procedure, but they cannot be completely circumvented.</p>
</section>
<section id="training-tree-segmentation-neural-networks" class="level2" data-number="3.4">
<h2 data-number="3.4" class="anchored" data-anchor-id="training-tree-segmentation-neural-networks"><span class="header-section-number">3.4</span> Training tree segmentation neural networks</h2>
<p>The architecture chosen to serve as the tree segmentation network is the PointNet++ <span class="citation" data-cites="qiPointNetPlusPlus2017">(<a href="99_references.html#ref-qiPointNetPlusPlus2017" role="doc-biblioref">Qi et al. 2017</a>)</span>, described in detail in <a href="02_literature_review.html#sec-ml-dl" class="quarto-xref"><span>Section 2.2</span></a>. It is a relatively simple architecture, and further potential quality improvements might be achieved by using more modern and advanced architectures instead. However, the main goal of this thesis was to develop and verify an overall framework, and thus the choice was set on a model that is simple to implement and work with to allow easy experimentation with other parts of the proposed system.</p>
<p>The architecture of the used PointNet++ is similar to the segmentation architecture shown in <a href="02_literature_review.html#fig-pointnet2-architecture" class="quarto-xref">Figure&nbsp;<span>2.3</span></a>. The main differences from the shown architecture are the use of one more stacked set abstraction layer to make the network deeper, and the use of a regression head that predicts a continuous value for each point instead of a classification head that predicts per-point class scores, since the model needs to assign a unique ID to every tree in the patch. The three stacked set abstraction layers have the proportions of points sampled set to 0.75, 0.5, and 0.5, and neighborhood radii for feature aggregation set to 0.1, 0.2, and 0.4. Note that the scale of the input is normalized (see next section for details), so the radii are not in meters. The final model has 30 million trainable parameters.</p>
<section id="coordinate-and-feature-normalization" class="level3" data-number="3.4.1">
<h3 data-number="3.4.1" class="anchored" data-anchor-id="coordinate-and-feature-normalization"><span class="header-section-number">3.4.1</span> Coordinate and feature normalization</h3>
<p>Itâ€™s a well established practice to scale the inputs to neural networks that is known to improve the speed and accuracy of gradient descent convergence <span class="citation" data-cites="bishop2006pattern">(<a href="99_references.html#ref-bishop2006pattern" role="doc-biblioref">Bishop 2006</a>)</span>. Before going through the network, a set of augmentations and transformations is applied to each synthetic forest patch. The augmentations are described in detail with visualized examples in <a href="#sec-augmentations" class="quarto-xref"><span>Section 3.4.2</span></a>. The transformations include scale and feature normalization â€“ the coordinates are centered and normalized to the interval <span class="math inline">\((-1, 1)\)</span> and the features are normalized to the interval <span class="math inline">\((-1, 1)\)</span> without centering.</p>
</section>
<section id="sec-augmentations" class="level3" data-number="3.4.2">
<h3 data-number="3.4.2" class="anchored" data-anchor-id="sec-augmentations"><span class="header-section-number">3.4.2</span> Data augmentation</h3>
<p>The primary objective of data augmentation is to enhance the quantity, quality, and variety of data used for training <span class="citation" data-cites="mumuniDataAugmentationComprehensive2022">(<a href="99_references.html#ref-mumuniDataAugmentationComprehensive2022" role="doc-biblioref">Mumuni and Mumuni 2022</a>)</span>. This is especially important when the sizes of the available datasets are limited, like in the case of the individual trees data that is the base of the synthetic forest datasets. Carefully selected augmentations allow increasing the effective dataset size. However, it is important to pay attention that the chosen augmentations keep the transformed examples semantically equivalent to the original. A readily understandable example of a bad augmentation in the task of digit classification, which is often used as the â€œhello worldâ€ of deep learning with the MNIST dataset of handwritten digits <span class="citation" data-cites="deng2012mnist">(<a href="99_references.html#ref-deng2012mnist" role="doc-biblioref">Deng 2012</a>)</span>, is a vertical flip, as most digits lose meaning when upside down. A flip is also a bad augmentation example for a synthetic forest patch described in <a href="#sec-synthetic-forest-dataset" class="quarto-xref"><span>Section 3.3</span></a>, as it changes the order of the trees within a patch, thus breaking the labels that are assumed to increase in a specific pattern.</p>
<p>In the scenario when the data for is created by combining several smaller inputs, there is a possibility of applying augmentations on two different scales. The most simple approach is to treat a synthetic forest patch as a whole and apply augmentations directly to it. However, it is also possible to apply per-tree augmentations, effectively increasing the size of the underlying tree set from which synthetic forest patches constructed. Only the latter kind are used for training the tree segmentation network.</p>
<p>The first transformation that changes the shape but does not affect any semantics for an individual tree is a random rotation around the vertical axis. A tree remains completely the same when rotated, but the coordinates of all points change. <a href="#fig-random-rotate-effect" class="quarto-xref">Figure&nbsp;<span>3.13</span></a> shows the effect of applying random rotation transformation of different magnitudes to a single aspen tree. For visualization purposes, the rotation is forced to apply with full magnitude for every parameter. During training, the angle is uniformly sampled from the specified range. For the final tree segmentation model, the range is set to <span class="math inline">\([-180, 180]\)</span> degrees, as no amount of rotation breaks the semantics.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03d_augmentations.ipynb" data-notebook-cellid="cell-fig-random-rotate-effect">
<div id="cell-fig-random-rotate-effect" class="cell" data-execution_count="6">
<div class="cell-output cell-output-display">
<div id="fig-random-rotate-effect" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-random-rotate-effect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03d_augmentations-fig-random-rotate-effect-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;3.13: Visualization of the random rotation around Z axis augmentation on a single aspen tree. The effect is forced to happen with full amplitude for visualization purposes, during training a rotation angle is uniformly sampled from a set range."><img src="03_materials_and_methods_files/figure-html/03d_augmentations-fig-random-rotate-effect-output-1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-random-rotate-effect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.13: Visualization of the random rotation around Z axis augmentation on a single aspen tree. The effect is forced to happen with full amplitude for visualization purposes, during training a rotation angle is uniformly sampled from a set range.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Another transformation that keeps the tree the same but changes the coordinates of the points is random scaling. It simply multiplies the coordinates by a scaling factor, making the tree larger or smaller in all directions. Unlike random rotation around the vertical axis, however, the range needs to be chosen much more carefully, as there is a possibility of making unrealistically large or small trees that would confuse the model during training. <a href="#fig-random-scale-effect" class="quarto-xref">Figure&nbsp;<span>3.14</span></a> shows the effect of applying random scale transformation to a single aspen tree. Again, for purposes of visualization, the scale is forced to apply with full magnitude. During training, the scale is uniformly sampled from the specified range. For the final tree segmentation model, the range is set to <span class="math inline">\([0.8, 1.2]\)</span>.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03d_augmentations.ipynb" data-notebook-cellid="cell-fig-random-scale-effect">
<div id="cell-fig-random-scale-effect" class="cell" data-execution_count="7">
<div class="cell-output cell-output-display">
<div id="fig-random-scale-effect" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-random-scale-effect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03d_augmentations-fig-random-scale-effect-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Figure&nbsp;3.14: Visualization of the random scale augmentation on a single aspen tree. The effect is forced to happen with full amplitude for visualization purposes, during training a scale factor is uniformly sampled from a set range."><img src="03_materials_and_methods_files/figure-html/03d_augmentations-fig-random-scale-effect-output-1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-random-scale-effect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.14: Visualization of the random scale augmentation on a single aspen tree. The effect is forced to happen with full amplitude for visualization purposes, during training a scale factor is uniformly sampled from a set range.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Another way to change the positions is to slightly translate each point for a random distance in a random direction. That transformation is commonly referred to as random jitter. Since LiDAR sensors have limited spatial accuracy, introducing random translations within the accuracy range should not have any effect on the result of tree segmentation. Going further, small translations even outside the accuracy range make sense, since they have very limited effect on the overall shape of the tree. <a href="#fig-random-jitter-effect" class="quarto-xref">Figure&nbsp;<span>3.15</span></a> shows this effect on a single aspen tree. The amount of translation is uniformly sampled independently for each point from a set range. Note that the random jitter augmentation is applied before the scale normalization. It matters because the augmentationâ€™s only parameter is the translation range, which depends on the scale. The parameters on the figure are thus in the original coordinate units â€“ meters. Note how the shape of the tree almost does not change when the maximum range is set to 20 centimeters, and starts to become fuzzy and loose shape at 1 meter and higher. For the final tree segmentation model, the maximum translation is set to 30 centimeters.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03d_augmentations.ipynb" data-notebook-cellid="cell-fig-random-jitter-effect">
<div id="cell-fig-random-jitter-effect" class="cell" data-execution_count="5">
<div class="cell-output cell-output-display">
<div id="fig-random-jitter-effect" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-random-jitter-effect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03d_augmentations-fig-random-jitter-effect-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="Figure&nbsp;3.15: Visualization of the random jitter augmentation on a single aspen tree. The translation magnitude is uniformly sampled from a set range for every point."><img src="03_materials_and_methods_files/figure-html/03d_augmentations-fig-random-jitter-effect-output-1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-random-jitter-effect-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.15: Visualization of the random jitter augmentation on a single aspen tree. The translation magnitude is uniformly sampled from a set range for every point.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>Another useful effect augmentations can provide is to make synthetic data look more like real data. As was mentioned in <a href="#sec-individual-trees-dataset" class="quarto-xref"><span>Section 3.2</span></a>, there is a selection bias in the dataset of individual trees: the trees that are easiest to manually separate are exponentially more likely to end up in the data. As such, the tree clouds in the individual tree dataset are significantly different from trees of the same species with similar sizes and shapes, but standing in dense clusters. One of the most important differences is that almost all the tree clouds have trunks, while in actual forest, dense canopy cover blocks most pulses and the trunk representation is very poor. One way to mitigate that issue is to apply a height-dependent dropout function to each tree cloud. For that purpose a probability threshold function that is around zero for the highest points of the tree and quickly ramps up to almost one for the lowest points is needed. An example function that satisfies these criteria is a modified sigmoid:</p>
<p><span class="math display">\[
\text{threshold}(z) = \big[1 + e^{z \times \text{scale} + \text{shift}}\big]^{-1},
\]</span></p>
<p>where <span class="math inline">\(z\)</span> is the height normalized to <span class="math inline">\([0, 1]\)</span> and reversed by subtraction from 1, <span class="math inline">\(\text{scale}\)</span> and <span class="math inline">\(\text{shift}\)</span> are hyperparameters that control the shape of the curve. Changing <span class="math inline">\(\text{scale}\)</span> controls how steep is the climb from 0 to 1: the larger, the steeper. Changing <span class="math inline">\(\text{shift}\)</span> controls the position of the climb: the larger, the lower. <a href="#fig-height-dropout" class="quarto-xref">Figure&nbsp;<span>3.16</span></a> shows an example of applying such dropout function to a single aspen tree, and <a href="#fig-height-dropout-aggressive" class="quarto-xref">Figure&nbsp;<span>3.17</span></a> show the same tree with more aggressive parameters, resulting in much more points being dropped from the tree cloud.</p>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03d_augmentations.ipynb" data-notebook-cellid="cell-fig-height-dropout">
<div id="cell-fig-height-dropout" class="cell" data-execution_count="8">
<div class="cell-output cell-output-display">
<div id="fig-height-dropout" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-height-dropout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03d_augmentations-fig-height-dropout-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="Figure&nbsp;3.16: Effect of the height-dependent modified sigmoid dropout on a single aspen tree. Scale is set to 8, shift is set to 3. a) A single aspen tree with points colored by height. b) Height-dependent probability of dropout (a modified sigmoid). c) The same aspen with points colored by probability of dropout. d) The same aspen with points that will be dropped marked red. e) The same aspen after the dropout is appleid with point colored by height."><img src="03_materials_and_methods_files/figure-html/03d_augmentations-fig-height-dropout-output-1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-height-dropout-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.16: Effect of the height-dependent modified sigmoid dropout on a single aspen tree. Scale is set to 8, shift is set to 3. <strong>a)</strong> A single aspen tree with points colored by height. <strong>b)</strong> Height-dependent probability of dropout (a modified sigmoid). <strong>c)</strong> The same aspen with points colored by probability of dropout. <strong>d)</strong> The same aspen with points that will be dropped marked red. <strong>e)</strong> The same aspen after the dropout is appleid with point colored by height.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<div class="quarto-embed-nb-cell" data-notebook="/Users/ivandubrovin/src/phd/rendered/chapters/03d_augmentations.ipynb" data-notebook-cellid="cell-fig-height-dropout-aggressive">
<div id="cell-fig-height-dropout-aggressive" class="cell" data-execution_count="9">
<div class="cell-output cell-output-display">
<div id="fig-height-dropout-aggressive" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-height-dropout-aggressive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="03_materials_and_methods_files/figure-html/03d_augmentations-fig-height-dropout-aggressive-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20" title="Figure&nbsp;3.17: Same as Figure&nbsp;fig-height-dropout, but with a more aggressive dropout threshold function. Scale is set to 18, shift is set to 2.5."><img src="03_materials_and_methods_files/figure-html/03d_augmentations-fig-height-dropout-aggressive-output-1.png" class="img-fluid figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-height-dropout-aggressive-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.17: Same as <a href="#fig-height-dropout" class="quarto-xref">Figure&nbsp;<span>3.16</span></a>, but with a more aggressive dropout threshold function. Scale is set to 18, shift is set to 2.5.
</figcaption>
</figure>
</div>
</div>
</div>
</div>
<p>All the augmentation are applied on the fly right before the examples are loaded into GPU memory and passed through the network.</p>
</section>
<section id="sec-other-training-params" class="level3" data-number="3.4.3">
<h3 data-number="3.4.3" class="anchored" data-anchor-id="sec-other-training-params"><span class="header-section-number">3.4.3</span> Other training parameters</h3>
<p>Mean absolute error is used as a loss function. Some experiments have shown improvements when using a custom loss function that modifies the mean absolute error loss by weighting it reversely proportional to the distance of the tree centroid. The idea of the modification is to make the network focus more on points closer to the center of each tree, as these points are much less likely to be overlapping with the crowns of adjacent trees.</p>
<p>The batch size is limited by the available GPU device memory, and in the described setup competes for memory space with the synthetic forest patch dimensions used in the training dataset. The preference is given to the size of the patch, so the batch size is set to 1, but is compensated for by using gradient accumulation steps â€“ updates to the model parameters are made after the gradient is accumulated for a set number of iterations. This slows down the training, but enables usage of larger batches when there is not enough memory to fit them, making the training procedure overall more stable.</p>
<p>Early stopping <span class="citation" data-cites="precheltAutomaticEarlyStopping1998">(<a href="99_references.html#ref-precheltAutomaticEarlyStopping1998" role="doc-biblioref">Prechelt 1998</a>)</span> is set up to terminate training early if there is no improvement in average validation loss in a set number of epochs. This makes sure that valuable GPU time is not wasted on continuing training models that are likely to have started overfitting.</p>
<p>Model checkpointing is set up as well. After each training epoch, when the average validation loss and accuracy are calculated, the model state is saved to disk if itâ€™s current accuracy is better than the last saved one, where accuracy is the proportion of points for which the rounded integer label is correct. This makes sure that the best performing model can always be recovered, even if the training process was run for too long and the latest model is not the best one.</p>
<p>The learning rate schedule is set to follow a fast linear warm up and slow linear decay. Learning rate is set to ramp up from 0.001 to 0.01 in a span of 2 epochs, and then decay back to 0.001 in a span of 30 epochs. Adam optimizer is used <span class="citation" data-cites="kingmaAdamMethodStochastic2014">(<a href="99_references.html#ref-kingmaAdamMethodStochastic2014" role="doc-biblioref">Kingma and Ba 2014</a>)</span>.</p>
<p>Training was performed on an NVIDIA A100 GPU with 80 Gb of memory. Inference, depending on the point density of the input, might work even on a much smaller GPU like Tesla T4 with 16 Gb of memory. GPUs of this size are available with limits on usage for free on services like Google Colab and Kaggle.</p>
</section>
<section id="on-implementation-of-pointnet" class="level3" data-number="3.4.4">
<h3 data-number="3.4.4" class="anchored" data-anchor-id="on-implementation-of-pointnet"><span class="header-section-number">3.4.4</span> On implementation of PointNet++</h3>
<p>As mentioned in the introduction, the deep learning code, including the code for PointNet++, is implemented using the PyTorch Geometric library designed for writing and training graph neural networks. This makes the implementation not exactly the same as described in the PointNet papers. The main ideas, namely local feature learning in a k-nearest neighbors neighborhood and max pooling for permutation-invariant aggregation, are there. Input transform and feature transform networks are not defined explicitly, but networks that process features learn to perform a similar function.</p>
</section>
</section>
<section id="sec-training-tree-processors" class="level2" data-number="3.5">
<h2 data-number="3.5" class="anchored" data-anchor-id="sec-training-tree-processors"><span class="header-section-number">3.5</span> Training segmented trees processing models</h2>
<p>To predict per-tree forest attributes from a segmented point cloud, a set of specialized regression and classification models is trained on the tree clouds. These specialized models are classic machine learning models that operate on most common metrics described in the literature overview chapter. As mentioned in the literature overview, common features used for machine learning on point clouds are based on the eigenvalues of the covariance matrix of the point coordinates, calculated either for an entire cloud, or per-point in a neighborhood around it. These features include linearity, planarity, scatter, that aim to indicate the presence of linear, planar, or volumetric structures, and also omnivariance, anisotropy, eigentropy, the sum of eigenvalues, and curvature. The features are defined as follows, with eigenvalues sorted in descending order such that <span class="math inline">\(\lambda_1 \ge \lambda_2 \ge \lambda_3\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
\text{linearity} &amp;= \frac{\lambda_1 - \lambda_2}{\lambda_1} \\
\text{planarity} &amp;= \frac{\lambda_2 - \lambda_3}{\lambda_1} \\
\text{scatter} &amp;= \frac{\lambda_3}{\lambda_1}  \\
\text{omnivariance} &amp;= \sqrt[3]{\lambda_1\lambda_2\lambda_3} \\
\text{anisotropy} &amp;= \frac{\lambda_1 - \lambda_3}{\lambda_1} \\
\text{eigentropy} &amp;= -\sum_{i=1}^{3} \lambda_i \ln(\lambda_i) \\
\text{sum of eigenvalues} &amp;= \lambda_1 + \lambda_2 + \lambda_3 \\
\text{curvature} &amp;= \frac{\lambda_3}{\lambda_1 + \lambda_2 + \lambda_3} \\
\end{aligned}
\]</span></p>
<p>Another common set of features, especially popular in forestry applications, are various statistics that describe the height distribution of points within the neighborhood or the cloud. They include maximum and average height, standard deviation, kurtosis, skew, and entropy of the height distribution, percentage of points above the mean and each of the deciles of height and the deciles of height themselves. The features are calculated for the entire tree cloud, effectively reducing each tree to a collection of metrics, resulting in a tabular dataset. In this form, the dataset is used to train the models.</p>
<p>In <span class="citation" data-cites="dubrovinExplorationPropertiesPoint2024">Dubrovin and Fortin (<a href="99_references.html#ref-dubrovinExplorationPropertiesPoint2024" role="doc-biblioref">2024</a>)</span>, we propose a way to help build intuition into the meaning of some of the less obvious features by visualizing individual trees on a different end of the range of the featureâ€™s values. <a href="#fig-feature-ranges" class="quarto-xref">Figure&nbsp;<span>3.18</span></a> shows is an example of such visualization, showing the effect of the shape of spruce on omnivariance and the effect of the shape of aspen on percent of points above mean height. Note the presence of ground points, which are filtered out before calculating features for the models.</p>
<div id="fig-feature-ranges" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-feature-ranges-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><a href="../images/fig-feature-ranges-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21" title="Figure&nbsp;3.18: Visualizations of how values of commonly used features calculated for the whole tree cloud map to the shapes of individual trees. Top: Effect of the shape of spruce on omnivariance. Bottom: Effect of on the shape of aspen on percent of points higher than mean height. Figure reused from @dubrovinExplorationPropertiesPoint2024. Note the presence of ground points, which are filtered out before calculating features for the models."><img src="../images/fig-feature-ranges-output-1.png" class="img-fluid figure-img"></a></p>
<p><a href="../images/fig-feature-ranges-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-22" title="Figure&nbsp;3.18: Visualizations of how values of commonly used features calculated for the whole tree cloud map to the shapes of individual trees. Top: Effect of the shape of spruce on omnivariance. Bottom: Effect of on the shape of aspen on percent of points higher than mean height. Figure reused from @dubrovinExplorationPropertiesPoint2024. Note the presence of ground points, which are filtered out before calculating features for the models."><img src="../images/fig-feature-ranges-output-2.png" class="img-fluid figure-img"></a></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-feature-ranges-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3.18: Visualizations of how values of commonly used features calculated for the whole tree cloud map to the shapes of individual trees. <strong>Top</strong>: Effect of the shape of spruce on omnivariance. <strong>Bottom</strong>: Effect of on the shape of aspen on percent of points higher than mean height. Figure reused from <span class="citation" data-cites="dubrovinExplorationPropertiesPoint2024">Dubrovin and Fortin (<a href="99_references.html#ref-dubrovinExplorationPropertiesPoint2024" role="doc-biblioref">2024</a>)</span>. Note the presence of ground points, which are filtered out before calculating features for the models.
</figcaption>
</figure>
</div>
<p>The feature set is thinned by using sequential feature selection: a greedy approach that selects the best feature according to 5-fold cross-validation accuracy on every iteration until a set number of features is selected. The models are trained on 20 best features selected this way.</p>
<p>To make the trees more closely resemble the trees standing in dense clusters, the same height-dependent dropout function that is used for augmenting synthetic forests is used. This dropout, together with a small amount of random jitter, is also used to create additional samples for training the models, as the original dataset size is very small. Great care is taken to only create additional samples within the training set, to avoid any data leakage. Every training set sample is repeated 3 times with random jitter with maximum amplitude of 20 cm and a dropout applied.</p>
<p>Two models are used as proof of concept for the proposed framework: a classifier that predicts the tree species, and a regressor that estimates a treeâ€™s diameter at breast height. Random Forest models are used in both cases. The number of trees is set to 200. For the classification model, class weights are assigned to every example that are inversely proportional to the class frequency in the data during model fitting.</p>
</section>
<section id="sec-matching-algorithm" class="level2" data-number="3.6">
<h2 data-number="3.6" class="anchored" data-anchor-id="sec-matching-algorithm"><span class="header-section-number">3.6</span> Matching detections to field inventory ground truth</h2>
<p>To evaluate the results of any tree detection system against field inventory data, an automated and deterministic method for matching detected tree candidates to the ground truth trees is needed. It should determine which ground truth trees, if any, the detected candidates correspond to. It should also classify both the trees and the candidates as either a true positive, meaning that the ground truth tree was successfully detected, a false negative, meaning that a ground truth tree was not detected, or a false positive, meaning a tree was detected when there is none.</p>
<p>I use the same matching procedure that is described and implemented in <span class="citation" data-cites="dubrovinOpenDatasetIndividual2024">Dubrovin, Fortin, and Kedrov (<a href="99_references.html#ref-dubrovinOpenDatasetIndividual2024" role="doc-biblioref">2024</a>)</span>. It considers the locations and heights of the trees, and falls back to using only locations when the height is not available for the ground truth tree. It is parametrized by the maximum allowed 2D distance and the maximum height difference between a detected and a ground truth tree to consider them a match. First, it constructs a distance matrix between the ground truth tree locations and the detected candidates and filters out the pairs for which the distance is larger than the allowed maximum. It then iterates over the remaining pairs in the order of increasing distance, and marks the pairs with suitable heights in which both trees are not yet assigned a class as a true positive. The pairs in which one of the trees is already assigned a class are marked as either a false positive or a false negative, and the pairs in which both trees are assigned a class are skipped. Finally, it marks all unmatched ground truth trees as false negatives, and all unmatched candidate trees as false positives.</p>
<p>Based on the results of the matching algorithm, a set of metrics that are often used to evaluate the results of tree detection can be calculated. The same metrics are usually used for evaluation in classification problems and thus are well-known. The first commonly used metric is recall, which is the proportion of the ground truth trees that were detected. The second metric is precision, which is the proportion of candidates that are correct. Both are important, and describe a detection system from different perspectives. To combine them into a single metric, <span class="math inline">\(F_1\)</span>-score is often used, which is the harmonic mean of precision and recall that provide a balanced measure of the system performance. The formulas for the metrics are as follows:</p>
<p><span class="math display">\[
\text{recall} =
\frac{N_{\text{true positives}}}{N_\text{trees}} =
\frac{N_{\text{true positives}}}{N_{\text{true positives}} + N_{\text{false negatives}}}
\]</span></p>
<p><span class="math display">\[
\text{precision} =
\frac{N_{\text{true positives}}}{N_\text{candidates}} =
\frac{N_{\text{true positives}}}{N_{\text{true positives}} + N_{\text{false positive}}}
\]</span></p>
<p><span class="math display">\[
F_1\text{-score} = 2 \times \frac{\text{precision} \times \text{recall}}{\text{precision} + \text{recall}}
\]</span></p>
<p>Another commonly used metric to evaluate the results of tree detection approaches is the average distance between a detected and a ground truth tree. This metric, however, has limitations that are important to keep in mind when analyzing the results. The first important limitation is that its value is limited in how low it can get by the difference in nature of the compared detections and ground truth field inventory. During the inventory, the treesâ€™ coordinates are recorded at their trunks, approximately at breast height, which is almost at the ground level compared to the height of the trees. The detections, on the other hand, happen with the perspective from above, and correspond to tree tops. Planar distance between the bottom of the trunk and the top of the canopy can be significant, especially if trees are even slightly tilted, or for broadleaf species with wide and irregular canopies, or both. So the average distance between predicted trees and their ground truth counterparts will rarely be zero, even for a perfect detection system. The other limitation it that that distance is also limited from above by the parameters used for the matching algorithm. As mentioned, one of the parameters is the maximum distance allowed between a candidate and an actual tree to consider them a match. Thus, the average distance metric is limited from both directions, but in ways that are not immediately obvious. I do calculate and report it in the results section, but the reader is advised to keep the limitations in mind.</p>
</section>
<section id="application-of-the-framework" class="level2" data-number="3.7">
<h2 data-number="3.7" class="anchored" data-anchor-id="application-of-the-framework"><span class="header-section-number">3.7</span> Application of the framework</h2>
<p>The framework is applied in two steps. First, the tree segmentation network is applied in a sliding window of the same size that was used for patch generation during its training, with overlap to be able to combine the results into a single prediction. The continuous predictions of the regression model are rounded to get the integer labels for every point. The results are merged to create a single point cloud, and a postprocessing routine aimed to transform per-window tree IDs to global tree IDs is run. It traces the overlapping points to find the labels that need to be updated in the later window, and adds a large number to predictions with remaining labels in the later window. Second, the segments identified by predicted integer labels are extracted, selected feature sets are calculated for each one, and each one is passed through the collection of attribute prediction models to get per-tree predictions.</p>


<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list" style="display: none">
<div id="ref-bishop2006pattern" class="csl-entry" role="listitem">
Bishop, Christopher. 2006. <em>Pattern <span>Recognition</span> and <span>Machine Learning</span></em>. Springer. <a href="https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/">https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/</a>.
</div>
<div id="ref-deng2012mnist" class="csl-entry" role="listitem">
Deng, Li. 2012. <span>â€œThe Mnist Database of Handwritten Digit Images for Machine Learning Research.â€</span> <em>IEEE Signal Processing Magazine</em> 29 (6): 141â€“42.
</div>
<div id="ref-dubrovinExplorationPropertiesPoint2024" class="csl-entry" role="listitem">
Dubrovin, Ivan, and Clement Fortin. 2024. <span>â€œAn <span>Exploration</span> of <span>Properties</span> of <span>Point Clouds</span> of <span>Individual Trees Extracted From</span> a <span>Larger UAV Lidar Survey</span>.â€</span> In <em><span>IGARSS</span> 2024 - 2024 <span>IEEE International Geoscience</span> and <span>Remote Sensing Symposium</span></em>, 4503â€“6. Athens, Greece: IEEE. <a href="https://doi.org/10.1109/IGARSS53475.2024.10641061">https://doi.org/10.1109/IGARSS53475.2024.10641061</a>.
</div>
<div id="ref-dubrovinOpenDatasetIndividual2024" class="csl-entry" role="listitem">
Dubrovin, Ivan, Clement Fortin, and Alexander Kedrov. 2024. <span>â€œAn Open Dataset for Individual Tree Detection in <span>UAV LiDAR</span> Point Clouds and <span>RGB</span> Orthophotos in Dense Mixed Forests.â€</span> <em>Scientific Reports</em> 14 (1): 21938. <a href="https://doi.org/10.1038/s41598-024-72669-5">https://doi.org/10.1038/s41598-024-72669-5</a>.
</div>
<div id="ref-eysnAlpineITDBenchmark2015" class="csl-entry" role="listitem">
Eysn, Lothar, Markus Hollaus, Eva Lindberg, FrÃ©dÃ©ric Berger, Jean-Matthieu Monnet, Michele Dalponte, Milan Kobal, et al. 2015. <span>â€œA <span>Benchmark</span> of <span>Lidar-Based Single Tree Detection Methods Using Heterogeneous Forest Data</span> from the <span>Alpine Space</span>.â€</span> <em>Forests</em> 6 (5): 1721â€“47. <a href="https://doi.org/10.3390/f6051721">https://doi.org/10.3390/f6051721</a>.
</div>
<div id="ref-farrShuttleRadarTopography2000" class="csl-entry" role="listitem">
Farr, Tom G., and Mike Kobrick. 2000. <span>â€œShuttle Radar Topography Mission Produces a Wealth of Data.â€</span> <em>Eos, Transactions American Geophysical Union</em> 81 (48): 583â€“85. <a href="https://doi.org/10.1029/EO081i048p00583">https://doi.org/10.1029/EO081i048p00583</a>.
</div>
<div id="ref-gravesIDTReeS2020Competition2020" class="csl-entry" role="listitem">
Graves, Sarah, and Sergio Marconi. 2020. <span>â€œ<span>IDTReeS</span> 2020 <span>Competition Data</span>.â€</span> Zenodo. <a href="https://doi.org/10.5281/ZENODO.3934932">https://doi.org/10.5281/ZENODO.3934932</a>.
</div>
<div id="ref-kingmaAdamMethodStochastic2014" class="csl-entry" role="listitem">
Kingma, Diederik P., and Jimmy Ba. 2014. <span>â€œAdam: <span>A Method</span> for <span>Stochastic Optimization</span>.â€</span> arXiv. <a href="https://doi.org/10.48550/ARXIV.1412.6980">https://doi.org/10.48550/ARXIV.1412.6980</a>.
</div>
<div id="ref-mumuniDataAugmentationComprehensive2022" class="csl-entry" role="listitem">
Mumuni, Alhassan, and Fuseini Mumuni. 2022. <span>â€œData Augmentation: <span>A</span> Comprehensive Survey of Modern Approaches.â€</span> <em>Array</em> 16 (December): 100258. <a href="https://doi.org/10.1016/j.array.2022.100258">https://doi.org/10.1016/j.array.2022.100258</a>.
</div>
<div id="ref-precheltAutomaticEarlyStopping1998" class="csl-entry" role="listitem">
Prechelt, Lutz. 1998. <span>â€œAutomatic Early Stopping Using Cross Validation: Quantifying the Criteria.â€</span> <em>Neural Networks</em> 11 (4): 761â€“67. <a href="https://doi.org/10.1016/S0893-6080(98)00010-0">https://doi.org/10.1016/S0893-6080(98)00010-0</a>.
</div>
<div id="ref-qiPointNetPlusPlus2017" class="csl-entry" role="listitem">
Qi, Charles R., Li Yi, Hao Su, and Leonidas J. Guibas. 2017. <span>â€œ<span>PointNet</span>++: <span>Deep Hierarchical Feature Learning</span> on <span>Point Sets</span> in a <span>Metric Space</span>.â€</span> In <em>Advances in <span>Neural Information Processing Systems</span></em>. Vol. 30. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/hash/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/d8bf84be3800d12f74d8b05e9b89836f-Abstract.html</a>.
</div>
<div id="ref-shiImportantLiDARMetrics2018" class="csl-entry" role="listitem">
Shi, Yifang, Tiejun Wang, Andrew K. Skidmore, and Marco Heurich. 2018. <span>â€œImportant <span>LiDAR</span> Metrics for Discriminating Forest Tree Species in <span>Central Europe</span>.â€</span> <em>ISPRS Journal of Photogrammetry and Remote Sensing</em> 137 (March): 163â€“74. <a href="https://doi.org/10.1016/j.isprsjprs.2018.02.002">https://doi.org/10.1016/j.isprsjprs.2018.02.002</a>.
</div>
<div id="ref-weinsteinDataNeonTreeEvaluationBenchmark2022" class="csl-entry" role="listitem">
Weinstein, Ben, Sergio Marconi, and Ethan White. 2022. <span>â€œData for the <span>NeonTreeEvaluation Benchmark</span>.â€</span> Zenodo. <a href="https://doi.org/10.5281/ZENODO.5914554">https://doi.org/10.5281/ZENODO.5914554</a>.
</div>
</div>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "î§‹";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/02_literature_review.html" class="pagination-link" aria-label="Literature review">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Literature review</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/04_results.html" class="pagination-link" aria-label="Results">
        <span class="nav-page-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Results</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/iod-ine/thesis/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li><li><a href="https://github.com/iod-ine/thesis/blob/main/chapters/03_materials_and_methods.qmd" class="toc-action"><i class="bi empty"></i>View source</a></li></ul></div></div></div></footer><script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>