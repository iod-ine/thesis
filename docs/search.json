[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A deep learning framework for mixed dense forests parameter estimation at individual tree scale",
    "section": "",
    "text": "Acknowledgments 🌹\nAs any large research project, finishing this thesis wouldn’t be possible without the support of many people. I would like to thank some of them, whose help played the biggest roles in my successfully finishing this. First, my supervisor, professor Clement Fortin, who picked up the lead when the project was in a disarray and guided me through to the end, helping to prioritize and focus on what was significant. Second, my previous supervisor, professor Anton Ivanov, under whose supervision the project started and gradually evolved into what it finally became. Third, Alexander Kedrov and Albert Vasiliev from Space Technologies and Services Center, Ltd, who allowed me to work with their data and taught me about the inventories in the field and current approaches of extending them with remote sensing data used in industry. And last, but also the most important, is my beautiful patient wife, who was supportive and forgiving from the very first moments of my PhD studies, and without whose help I would have given up many times over. Thank you all, sincerely, for the guidance, the wisdom, the knowledge, the help, the support.",
    "crumbs": [
      "Acknowledgments <span style=\"visibility: hidden\">🌹</span>"
    ]
  },
  {
    "objectID": "chapters/01_introduction.html",
    "href": "chapters/01_introduction.html",
    "title": "1  Introduction",
    "section": "",
    "text": "1.1 Context\nForests are a crucial part of the global ecosystem, both environmentally and economically. They cover a third of the land area, contain over 80% of terrestrial biodiversity, and somewhere around one-third of humanity depends on forests and forest products for their livelihoods (Aerts and Honnay 2011; The State of the World’s Forests 2020). Forests are an essential renewable natural resource and a huge, dynamic part of the global carbon cycle. Figure 1.1 offers a map of the global tree coverage based on data from Hansen et al. (2013) to highlight the extent of forests on the planet. Responsible management of forests allows using the resources efficiently and sustainably, preserving the biodiversity, and regulating atmospheric \\(CO_2\\), which is becoming especially important as the anthropogenic climate change is ongoing and accelerating (Fahey et al. 2010; Forster et al. 2024). This drives the need for accurate, detailed, up-to-date information about various forest attributes such as distributions of tree species, average heights and ages of trees, estimates of trunk diameter, timber volume, and above ground biomass, and others.\nThe traditional manual forest inventories that rely on people going out into the forest to count and measure trees are extremely labor-intensive and time-consuming, which makes them infeasible to cover extensive areas with sufficient detail, speed, and frequency (Burley, Youngquist, and Evans 2004). This is especially relevant in countries where massive areas are covered by forests, such as Russia, Brazil, Canada, USA, and China, which are the top five countries for forest area according to Global Forest Resources Assessment (2020). For that reason, various remote sensing techniques are widely used to extend and extrapolate the traditional forest inventories. All sorts of data, from satellite and aerial imagery to very detailed terrestrial LiDAR surveys, are used in all sorts of applications that require mapping forest attributes. Some such applications are mentioned in the next chapter dedicated to reviewing the literature.\nMany national space agencies provide open access to satellite data that can be used for mapping forests. European Space Agency offers free and open access1 to the Sentinel missions, including C-band SAR data from Sentinel-1 and medium-resolution multispectral data from Sentinel-2, both with global coverage. They also have an upcoming P-band SAR mission called BIOMASS, designed specifically to study global forest biomass and carbon cycles (Quegan et al. 2019), which further indicates the growing importance and interest in the topic of forest mapping. NASA provides free access to an abundance of satellite data through its Earthdata platform, Landsat mission with the Operation Land Imager (OLI) instrument and Terra mission with Moderate Resolution Imaging Spectroradiometer (MODIS) instrument being the most relevant for forest mapping applications. The Indian Space Research Organisation (ISRO), Brazilian National Institute for Space Research, National Space Research and Development Agency of Nigeria all offer free satellite data that can be used for this purpose. NASA and the ISRO also have a joint upcoming mission called NISAR with two fully-polarimetric SAR sensors at L-band and S-band (Kellogg et al. 2020), which will benefit forestry applications greatly.\nThe open satellite data is an essential tool for wide-area studies, but it usually has coarse resolution, which limits the achievable accuracy and level of detail. It also offers no ability to control the observation parameters, as they are fixed by the instrument configuration and orbit parameters, both of which are outside the data consumer control. Higher resolution data with a limited ability to control the acquisition parameters is available commercially, but the prices are steep. Aerial observations with sensors mounted on planes or helicopters are both more controllable and cheaper alternatives, although still expensive and sill limited in the flexibility of the control of acquisition parameters. A much more affordable and controllable alternative is UAV-based observation. It allows for fine-grained control and on the fly adjustment of many important parameters such as flight height, flight path overlap, combination of used sensors, and so on.\nThe most common way to use UAV remote sensing, be it LiDAR, multispectral, hyperspectral, or other data modalities, for mapping forest attributes in industry is what is known in the LiDAR community as the area-based approach, described in detail in Section 2.3. It is based on extrapolating measurements from ground plots made in traditional inventories by aggregating remote sensing data to the grid with cell area of a ground plot, which results in coarse resolution maps. It is easy to use, but its results are often not detailed enough when working on finer scales. However, modern sensors and processing techniques allow not aggregating at all and instead working on the level of individual trees, which is as detailed as it can possibly get, allowing for any level of aggregation for downstream tasks. This requires robust algorithms that allow detecting individual trees in dense multimodal data. This is relatively2 easy in urban environments, manually planted and managed forest stands, or forests that are either sparse or predominantly coniferous, where the structure of the canopy is easy to interpret. In some such environments state-of-the-art results can be achieved by simple local maxima detection algorithms, that rely on the assumption that a tree can be detected by finding peaks of canopies because they correspond to tree tops. Forest that are mixed and dense, which are a huge part of forests in countries mentioned earlier, are much harder to work with and are a very active area of research for developing methods of detection of individual trees. The canopies in such forests are very complex, especially because the top of the crowns of deciduous tree species often don’t have a single pronounced height maximum, and crowns of nearby trees often overlap.\nThe framework described in this thesis focuses on fusion of two remote sensing data sources, UAV LiDAR point clouds and UAV RGB orthophotos, to detect individual trees in dense mixed forests and predict required attributes for each tree individually, producing the most detailed maps possible. The choice of data sources is driven by their complementary nature, which I believe is key for semantically parsing such complex environments. LiDAR is an active sensor, which means it does not depend on external conditions such as lightning. Cloud and terrain shadows, incidence angles, and weather conditions do not affect LiDAR surveys. Moreover, LiDAR provides 3D vertical structural information about the forest, as laser pulses penetrate the canopy and reach both the undergrowth and the ground. This structural information is essential for understanding complex environments like dense forests. High-resolution RGB imagery does depend on the lighting, but is still an invaluable tool, as it offers detailed data with fixed resolution, continuous coverage of surfaces, unlike the discrete representation of laser scanning, and captures many fine details and textures. It can also benefit from a huge variety of well-established tools and processing techniques from the field of computer vision. Neither of these data sources on their own is enough to reliably and with sufficient robustness separate individual trees in dense mixed forests, and the key to success lies in their fusion.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01_introduction.html#context",
    "href": "chapters/01_introduction.html#context",
    "title": "1  Introduction",
    "section": "",
    "text": "Figure 1.1: Global tree cover for 2010. Data from Hansen et al. (2013)",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01_introduction.html#sec-research-question",
    "href": "chapters/01_introduction.html#sec-research-question",
    "title": "1  Introduction",
    "section": "1.2 Research question and hypothesis",
    "text": "1.2 Research question and hypothesis\nThe main research question could be formulated as follows: “How to reduce effort and cost required for detailed inventories of dense mixed forests without losing accuracy?” The main hypothesis is then: “An accurate and detailed inventory with reduced effort and cost can be achieved through fusion of UAV LiDAR and RGB data using machine learning”. The benchmarks to measure against are both the traditional manual forest inventory and the widely used area-based approach.\nCost and effort are closely connected, as any effort is in the end converted to cost. Still, it often makes sense to separate the two to highlight the nature of the effects the proposed system should have. The cost reduction was partly addressed in the previous section during the discussion of the platform of choice: UAV-based remote sensing offers a great balance of upfront cost, effort to operate, and versatility in observation parameters. The effort reduction comes from greatly decreasing the amount of field inventory data required when using the proposed system, from the relative ease of collection of remote sensing data compared to alternatives, and from relative ease of tuning the system to operate in new areas.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01_introduction.html#overview-of-the-framework",
    "href": "chapters/01_introduction.html#overview-of-the-framework",
    "title": "1  Introduction",
    "section": "1.3 Overview of the framework",
    "text": "1.3 Overview of the framework\nThe proposed framework described in this thesis consist of a neural-network based tree segmentation in UAV LiDAR point clouds enhanced with RGB orthophoto-based features and processing the segments with a collection of specialized classic machine learning models that predict the parameters of interest for each detected tree. The tree segmentation model is trained on synthetic forest patches constructed from a dataset of point clouds of individual trees extracted manually from a large UAV LiDAR survey, heavily relying on augmentations to make the synthetic forest closer to real forest. The parameter prediction classification and regression models are trained on the same dataset of individual trees using a collection of widely used manual point cloud features. Figure 1.2 is a schematic representation of the framework, showing the required inputs in red, processing steps in yellow, and artifacts in cyan. The field inventory is phased out, but it is still required, as application of any framework requires quality assessment and validation.\n\n\n\n\n\n\nFigure 1.2: The schematic representation of the framework in the application stage.\n\n\n\nFigure 1.3 is a schematic of the preparation step for the framework. Each individual node is described in detail in Chapter 3.\n\n\n\n\n\n\nFigure 1.3: The schematic representation of the framework in the preparation stage.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01_introduction.html#thesis-structure",
    "href": "chapters/01_introduction.html#thesis-structure",
    "title": "1  Introduction",
    "section": "1.4 Thesis structure",
    "text": "1.4 Thesis structure\nThe thesis consists of 5 chapters. This section briefly describes the contents and aims of each one.\nChapter 1 aims to give a general introduction to the research project and put it into a wide scientific and societal context. It defines the main research question and the hypothesis, and gives a high-level overview of the proposed framework. It also provides the links to the original datasets and the code.\nChapter 2 aims to give an overview of the scientific literature on topics most relevant to the project. Its main goals are to provide the reader with context for the research described in the thesis, provide references for in-depth materials on topics that are out of scope of this work, and to highlight the research gap that the work tries to address.\nChapter 3 describes in detail the datasets, methods and methodological choices used in the proposed framework. Its aim it to make the work reproducible and to explain the methodological choices made.\nChapter 4 describes the results of each stage of the framework preparation and the validation approach used to verify its applicability and effectiveness on realistic data.\nChapter 5 offers a brief summary of the thesis as a whole, potential approaches for further improvement of the proposed framework, and some concluding thoughts.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01_introduction.html#data-and-code-availability",
    "href": "chapters/01_introduction.html#data-and-code-availability",
    "title": "1  Introduction",
    "section": "1.5 Data and code availability",
    "text": "1.5 Data and code availability\nOriginal datasets described in the thesis are openly available on Kaggle (Lysva field survey here and individual tree point clouds here). All the code used for the project is available on GitHub at iod-ine/phd. An HTML version of this thesis is hosted through GitHub Pages and is available here. The thesis document was developed using Quarto (Allaire et al. 2024) using the literate programming approach (Knuth 1984). The deep learning part is implemented using PyTorch (Ansel et al. 2024), PyTorch Geometric (Fey and Lenssen 2019), and PyTorch Lightning (Falcon and The PyTorch Lightning team 2019), with experiment tracking using MLflow. Classic machine learning models implementations are from scikit-learn (Pedregosa et al. 2011). NumPy (Harris et al. 2020), SciPy (Virtanen et al. 2020), pandas (The pandas development team, n.d.), scikit-image (van der Walt et al. 2014) libraries are used for processing the data. ratserio (Gillies et al. 2013/), geopandas, laspy, lazrs libraries are used for working with geospatial data formats. matplotlib (Hunter 2007) and seaborn (Waskom 2021) libraries are used for visualization.\n\n\n\n\nAerts, Raf, and Olivier Honnay. 2011. “Forest Restoration, Biodiversity and Ecosystem Functioning.” BMC Ecology 11 (1): 29. https://doi.org/10.1186/1472-6785-11-29.\n\n\nAllaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and Christophe Dervieux. 2024. “Quarto.” https://doi.org/10.5281/zenodo.5960048.\n\n\nAnsel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, Bin Bao, et al. 2024. “PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation.” In 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS ’24). ACM. https://doi.org/10.1145/3620665.3640366.\n\n\nBurley, Jeffery, John Youngquist, and Julian Evans, eds. 2004. Encyclopedia of Forest Sciences. 1st ed. Oxford: Elsevier.\n\n\nFahey, Timothy J, Peter B Woodbury, John J Battles, Christine L Goodale, Steven P Hamburg, Scott V Ollinger, and Christopher W Woodall. 2010. “Forest Carbon Storage: Ecology, Management, and Policy.” Frontiers in Ecology and the Environment 8 (5): 245–52. https://doi.org/10.1890/080169.\n\n\nFalcon, William, and The PyTorch Lightning team. 2019. “PyTorch Lightning.” https://doi.org/10.5281/zenodo.3828935.\n\n\nFey, Matthias, and Jan Eric Lenssen. 2019. “Fast Graph Representation Learning with PyTorch Geometric.” https://github.com/pyg-team/pytorch_geometric.\n\n\nForster, Piers M., Chris Smith, Tristram Walsh, William F. Lamb, Robin Lamboll, Bradley Hall, Mathias Hauser, et al. 2024. “Indicators of Global Climate Change 2023: Annual Update of Key Indicators of the State of the Climate System and Human Influence.” Earth System Science Data 16 (6): 2625–58. https://doi.org/10.5194/essd-16-2625-2024.\n\n\nGillies, Sean et al. 2013/. “Rasterio: Geospatial Raster I/O for Python Programmers.” Mapbox. https://github.com/rasterio/rasterio.\n\n\nGlobal Forest Resources Assessment. 2020. FAO. https://doi.org/10.4060/ca9825en.\n\n\nHansen, M. C., P. V. Potapov, R. Moore, M. Hancher, S. A. Turubanova, A. Tyukavina, D. Thau, et al. 2013. “High-Resolution Global Maps of 21st-Century Forest Cover Change.” Science 342 (6160): 850–53. https://doi.org/10.1126/science.1244693.\n\n\nHarris, Charles R., K. Jarrod Millman, Stéfan J van der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020. “Array Programming with NumPy.” Nature 585: 357–62. https://doi.org/10.1038/s41586-020-2649-2.\n\n\nHunter, John D. 2007. “Matplotlib: A 2D Graphics Environment.” Computing in Science & Engineering 9 (3): 90–95. https://doi.org/10.1109/MCSE.2007.55.\n\n\nKellogg, Kent, Pamela Hoffman, Shaun Standley, Scott Shaffer, Paul Rosen, Wendy Edelstein, Charles Dunn, et al. 2020. “NASA-ISRO Synthetic Aperture Radar (NISAR) Mission.” In 2020 IEEE Aerospace Conference, 1–21. Big Sky, MT, USA: IEEE. https://doi.org/10.1109/AERO47225.2020.9172638.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” Journal of Machine Learning Research 12: 2825–30.\n\n\nQuegan, Shaun, Thuy Le Toan, Jerome Chave, Jorgen Dall, Jean-François Exbrayat, Dinh Ho Tong Minh, Mark Lomas, et al. 2019. “The European Space Agency BIOMASS Mission: Measuring Forest Above-Ground Biomass from Space.” Remote Sensing of Environment 227 (June): 44–60. https://doi.org/10.1016/j.rse.2019.03.032.\n\n\nThe pandas development team. n.d. “Pandas-Dev/Pandas: Pandas.” https://doi.org/10.5281/zenodo.3509134.\n\n\nThe State of the World’s Forests. 2020. FAO and UNEP. https://doi.org/10.4060/ca8642en.\n\n\nvan der Walt, Stéfan J., Johannes L. Schönberger, Juan Nunez-Iglesias, François Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart, Tony Yu, and the scikit-image contributors. 2014. “Scikit-Image: Image Processing in Python.” PeerJ 2 (June): e453. https://doi.org/10.7717/peerj.453.\n\n\nVirtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, et al. 2020. “SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python.” Nature Methods 17: 261–72. https://doi.org/10.1038/s41592-019-0686-2.\n\n\nWaskom, Michael. 2021. “Seaborn: Statistical Data Visualization.” Journal of Open Source Software 6 (60): 3021. https://doi.org/10.21105/joss.03021.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/01_introduction.html#footnotes",
    "href": "chapters/01_introduction.html#footnotes",
    "title": "1  Introduction",
    "section": "",
    "text": "Although the access is not open for everyone equally, as I have observed silent bans of the accounts connecting from Russian IP-addresses without any response to support inquires.↩︎\nThe word relatively does a lot of heavy lifting here, as the problem is by no means easy on its own and takes a lot of effort from many researchers to continue to make progress on.↩︎",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "chapters/02_literature_review.html",
    "href": "chapters/02_literature_review.html",
    "title": "2  Literature review",
    "section": "",
    "text": "2.1 Remote sensing for forestry applications\nAs was mentioned in the introduction, remote sensing is widely used for extending labor-intensive and time-consuming manual forest inventories. This section provides examples of various remote sensing techniques used in various forestry applications, before going in more detail into specifically UAV LiDAR and RGB, which are the focus of the described framework.\nHansen, Mitchard, and King (2020) explore the usage of C-band SAR data from the Sentinel-1 mission for binary land use classification into forest/non-forest using a collection of classic machine learning classifiers. SAR is an active sensor, making it independent on external conditions and observation time, and it penetrates cloud cover, making it very reliable during almost any weather. They report accuracies from 80% in the worst case to the 93% in the best case, depending on the area of application.\nFerrari et al. (2023) use Fully Convolutional Networks (Long, Shelhamer, and Darrell 2015) for fusion of multispectral Sentinel-2 and C-band SAR Sentinel-1 data for clear-cut logging detection in the presence of clouds, which limit the use of optical-only approaches and are very common in tropical areas. They show that fusion performs better than single-modality variant for pixels obscured by clouds. This combination of active SAR and passive multispectral data is very common, as the sensors compliment each other nicely, similarly to laser scanning and orthophotos.\nSinica-Sinavskis and Grube (2022) combine LiDAR point clouds with Sentinel-2 images to predict timber volume on a stand level. They use an unusual approach, using Sentinel-2 images for species detection by clustering, LiDAR point clouds for estimating tree counts and average tree heights, and combining them into two variables used to fit the final regression models. The reported relative RMSE values are 14-22%, with errors larger for deciduous tree species.\nLiDAR has been used for forestry applications for a long time, with publications on the topic dating back 40 years. Nelson, Krabill, and MacLean (1984) is one of the earliest studies that explores usage of airborne LiDAR for measuring forest canopy profiles and estimating tree heights and canopy closure (a measure of forest canopy coverage that indicates what proportion of the sky is obscured by the tree crowns when viewed from the ground). Nilsson (1996) is a study looking into tree height and timber volume estimation using airborne LiDAR across a range of point densities and seasons on a coniferous forest stand. Næsset (1997a) and Næsset (1997b) are studies exploring the use of airborne LiDAR for estimating mean tree height and timber volume, suggesting the ways to correct the systematic underestimation of height and showing how regression on LiDAR-derived metrics can predict volume. Carson et al. (2004) offers an overview of some of the applications and approaches and a summary of contemporary state-of-the-art.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature review</span>"
    ]
  },
  {
    "objectID": "chapters/02_literature_review.html#sec-ml-dl",
    "href": "chapters/02_literature_review.html#sec-ml-dl",
    "title": "2  Literature review",
    "section": "2.2 Machine learning and deep learning on point clouds",
    "text": "2.2 Machine learning and deep learning on point clouds\nThe reader is assumed to be familiar with general concepts of machine learning and deep learning. For an introduction or a refresher, one of the best resources is Goodfellow, Bengio, and Courville (2016). For a more detailed exploration, X. Wang, Zhao, and Pourpanah (2020) offer a selection of papers on topics relevant to modern deep learning techniques. As point clouds are a less well-known modality in machine learning and deep learning, the reader is also referred to Bello et al. (2020) and Y. Guo et al. (2021), offering detailed reviews of the deep learning approaches used in various problems related to processing point clouds. Figure 2.1 is a high-level taxonomy of these approaches. Two main groups are structured grid-based, which rely on transformation of point clouds into regular structures that are then processed by 2D or 3D convolutional neural networks, and row point cloud based, which consume point clouds directly. The section is focused on providing a very short overview of these topics as it relates to the framework that is the focus of the thesis.\n\n\n\n\n\n\nFigure 2.1: A high-level taxonomy of deep learning approaches for point clouds. Figure from Bello et al. (2020)\n\n\n\nClassic machine learning approaches rely on feature engineering: manual preparation of features used as inputs for models based on domain expertise and various feature selection techniques. Two main groups of tasks are per-point predictions, which is in many ways similar to the task of semantic segmentation of images, that requires per-point features, and per-cloud predictions that either process entire point clouds or individual segments, separated be some preprocessing routine. Weinmann, Jutzi, and Mallet (2013) show that careful selection of features is crucial for accurate and efficient semantic interpretation of point cloud data: a shotgun approach of using many simple features without much consideration results in worse performance than a surgical approach of using a few carefully selected ones. They also provide definitions of some of the most used manual features that aim to describe the 3D structure of point sets. The features are based on combinations of eigenvalues of local covariance matrices of coordinate vectors of a set of points. They can be calculated on a per-point basis by using fixed-size or nearest-neighbor neighborhoods, or for whole segments of point clouds. The used features were originally introduced by West et al. (2004), Pauly, Gross, and Kobbelt (2002), and Mallet et al. (2011), and include linearity, planarity, and scatter, aiming to indicate the presence of a linear, planar, or volumetric structures, and also omnivariance, anisotropy, eigentropy, the sum of eigenvalues, and curvature. The formulas for the features are provided in Section 3.5, where they are used for training parameter prediction models for segmented trees. Simpler features, that are especially often used in area-based approach, include various statistics describing height and reflection intensity distributions of points, like percentages of points above mean height, deciles of height, cumulative percentages of points below height deciles, and others. Since many LiDAR sensors can record multiple reflections from a single pulse, features that use the reflection numbers are also often used, i.e., relative return number and total number of returns.\nPoint clouds are by their nature irregular, and non-uniform point densities across the cloud often become a problem for both manual feature creation and representation learning as part of a deep learning model. Many researches explore ways to handle this non-uniformity. For example, Özdemi̇r (2021) proposes a framework for semantic segmentation of photogrammetric point clouds in urban environments, the key components of which are voxel-grid filtering-based downsampling for equalizing the point density across the cloud, manual addition of geometric features in a nearest-neighbor neighborhood, and processing the result with a convolutional network for assigning labels to each point, followed by a post-processing step to upscale the labels back to original point cloud size.\nThe first deep learning model to work directly on point clouds without constructing any intermediate representation that can be processed by convolutional models is the PointNet introduced by Qi, Su, et al. (2017). Figure 2.2 shows its architecture. A critical aspect of any model that aims to operate directly on point clouds is permutation invariance, as point clouds are unordered sets, and shuffling the points does not change the point cloud semantically. PointNet achieves that invariance by using a combination of shared multilayer perceptrons (MLP) to process point coordinates and features and max pooling for construction of global feature vector. That feature vector can then be used directly for point cloud classification tasks, or concatenated with point features and processed further by shared MLPs for per-point predictions.\n\n\n\n\n\n\nFigure 2.2: PointNet architecture. Figure from Qi, Su, et al. (2017)\n\n\n\nQi, Yi, et al. (2017) introduces PointNet++, aimed to address the main drawbacks of the original PointNet model, namely the use of only two scales for context encoding – per-point and global, which limits the ability of the model to respond to local structures and fine patterns and thus work in complex scenes. To address this, PointNet++ uses a hierarchical architecture that applies PointNet recursively on nested subsets of the original point set, allowing to learn features on multiple increasing scales. To achieve that, network uses stacked set abstraction layers, that first sample a subset of the point cloud using farthest point sampling – an algorithm aimed to create representative subsets even for point clouds with uneven point density that selects the next point by maximizing its distance to already selected set – then constructs a neighborhood around each selected point using either fixed distance or fixed number of neighbors, and finally applies PointNet to each neighborhood to reduce into a feature vector for the sampled point. Then, similar to the original PointNet, the subset can be further reduced to a final global feature vector used for point clouds classification, or passed to an upscaling branch with skip-connections and k-nearest neighbors interpolation to upscale the features to the original point cloud coordinates. The architecture is visualized in Figure 2.3.\n\n\n\n\n\n\nFigure 2.3: PointNet++ architecture. Figure from Qi, Yi, et al. (2017)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature review</span>"
    ]
  },
  {
    "objectID": "chapters/02_literature_review.html#sec-area-based-approach",
    "href": "chapters/02_literature_review.html#sec-area-based-approach",
    "title": "2  Literature review",
    "section": "2.3 Area-based approach",
    "text": "2.3 Area-based approach\nThe most common way to use LiDAR for mapping forest attributes is the area-based approach (White et al. 2013). Figure 2.4 shows its schematic representation. It consists of a LiDAR survey covering the whole area of interest and a manual forest inventory providing ground truth data for fitting statistical models and validating the results. The inventory usually consists of many circular ground plots with every tree within counted and attributes of interest either directly measured or calculated and averaged. The point cloud is clipped by the extents of the ground plots, and for each plot it is reduced to a collection of manually selected metrics. The metrics usually include descriptions of the height distribution of the points, but often reflection intensities and other sensor-provided information is used as well, such as the return number, the number of returns, etc. (a brief discussion on the use of intensity-based features can be found in Section 3.1.1). In general, any summary statistic that can be derived from a collection of points can be used, including features mentioned in Section 2.2. These metrics are then used as input features for fitting regression and classification models to predict the forest attributes measured on the corresponding plots. The same metrics are calculated for the entire area of interest, using a grid with a cell size similar in area to the area of a single ground plot. The models are then applied to the grid, generating an extrapolation of the required attributes.\n\n\n\n\n\n\nFigure 2.4: Area-based approach schematic. Figure from White et al. (2013)\n\n\n\nArea-based approach is extensively used both in research and in industry because it provides many advantages. It is relatively easy to implement. In fact, basic familiarity with the R programming language is enough to create your own area-based approach pipelines since a full, scalable implementation exists in the lidR package (Roussel et al. 2020). It is also straightforward to extend with other data sources such as satellite or aerial images, and it works even with sparse data: for successful plot and stand level modeling point densities as low as 0.5 points per square meter have been reported to be enough (Treitz et al. 2012; Jakubowski, Guo, and Kelly 2013). Still, it requires a lot of field inventory data to work, since every ground plot becomes a single example for the models. The models that can be used are also relatively simple, because of how expensive the data collection is. Data-hungry approaches like neural networks usually don’t have enough data to train. The results are also very coarse – predicted on a grid with the size defined by the area of a plot (a common plot shape is a circle with 9-meter radius, which is approximately equivalent to a square grid cell with 16-meter side). This is why they are usually further aggregated to stand level.\n\n2.3.1 Examples of studies using ABA\nAs mentioned, area-based approach is used widely both throughout industry and research. I have personally taken part in a couple of projects where it was used for mapping forest attributes on large scales. This subsection offers some examples of its continuing use in research.\nBouvier et al. (2015) suggest a set of 4 metrics they use to fit models for predicting timber volume, above ground biomass, and basal-area on stand level, instead of most commonly used metrics based on the distribution of height. Their metrics are aimed to capture different aspects of the canopy geometry. The authors argue that usage of a very limited set of carefully engineered diverse metrics results in improvement of model generalization ability without loss of accuracy.\nZhang et al. (2023) use a modification of the area-based approach to predict plot-level diameter at breast height (dbh) by utilizing known allometric dependencies between tree height and dbh to limit the size of the hypothesis set for fitting regression models. They use airborne LiDAR measurements with an average density of 9.6 points per square meter and report a relative error of 11%.\nVermeer et al. (2023) use a U-Net (Ronneberger, Fischer, and Brox 2015) image semantic segmentation model on LiDAR-derived 1-meter resolution digital terrain models and canopy height maps to predict the distribution of three main tree species in a Norwegian forest. They achieve a macro \\(F_1\\) of 0.70 when including the background class, and 0.63 when including only the target species classes, as evaluated on independent field inventory plots.\nKc et al. (2024) is a classic example of an area-based approach study utilizing airborne LiDAR survey to predict forest above ground biomass. They use a set of 32 metrics derived from height distribution of points, an automatic feature selection procedure, and two regression models: linear regression and Random Forest to compare the results. They report coefficients of determination of 0.85 and average errors around 83 tons per hectare.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature review</span>"
    ]
  },
  {
    "objectID": "chapters/02_literature_review.html#sec-individual-tree-approach",
    "href": "chapters/02_literature_review.html#sec-individual-tree-approach",
    "title": "2  Literature review",
    "section": "2.4 Individual tree-based approach",
    "text": "2.4 Individual tree-based approach\nWith the constant improvement of accessibility and quality of high-resolution remote sensing data, there is a growing interest in the development of methods that operate on the scale of individual trees. This subsection gives an overview of some of the research in this area, split by the main data source.\nW. Li et al. (2012) developed a well performing algorithmic method for segmentation of individual tree crowns in LiDAR point clouds for coniferous forests that relies on the pointy shape characteristic of many coniferous species and segments the trees from top to bottom.\nLucas et al. (2019) use the set of eigenvalue-based features calculated for each point in a fixed-radius neighborhood, with other geometrical features including maximum local height difference and height standard deviation, local radius and local point density, to identify linear vegetation elements in segmented point clouds. They also use two point-based features that don’t rely on a neighborhood: the number of returns and the normalized return number, proposed in L. Guo et al. (2011), and use Random Forest classifier to separate the points that belong to vegetation after first removing the planar features corresponding to grass, soil, and water surfaces. The approach is somewhere in the middle between the area-based and individual tree-based, but still is a useful example of application of classic machine learning to segment out vegetation from larger point clouds.\n\n2.4.1 Image-only\nMany approaches rely only on images, mostly high-resolution RGB and multispectral ones, but lately also hyperspectral. The main advantage of such approaches is a well-established and well-known toolbox in terms of both algorithmic processing and deep learning, as many of the most important deep learning milestones were achieved in the field of computer vision. They also can rely on consistent resolution, capture of fine details and textures, and continuous representation of sensed environments. The main disadvantages were mentioned in the introduction: passive sensors rely on the sun as the source, and thus greatly depend on lighting such as cloud and terrain shadows, time of day, season. They also offer no information on vertical structure. Still, they are very popular and achieve outstanding results in many environments.\nWeinstein et al. (2020) introduces a Python package for training and inference of ecological object detection neural networks in airborne imagery. It uses a convolutional object detection network described in Weinstein et al. (2019) to predict bounding boxes for individual trees. The data the model is trained on is not as dense as the forests that are the target of this thesis. Moreover, the model requires fine-tuning to be applicable to new data, which greatly limits its applicability, since developing bounding box annotations for individual trees within dense forests is an extremely tedious and labor-intensive task.\nLassalle et al. (2022) use high-resolution satellite imagery to delineate individual tree crowns in mangrove forests by using DeepLabv3+-based Multi-Task Encoder-Decoder network (MT-EDv3), originally proposed by La Rosa et al. (2021), to predict for each pixel the distance to the tree crown border. This distance map is then enhanced by applying the Laplacian over Gaussian filter, and finally the watershed segmentation algorithm is applied to delineate individual crowns. The approach seems to rely on there being semi-clear separation between the tree crowns, which is rarely the case when the forests are dense and mixed.\nThe same network was also successfully used to map tree species in tropical urban environment in Rio de Janeiro, Brazil in Martins et al. (2021). They develop a post-processing approach to combine the semantic segmentation map and the distance map to classify tree species with an average \\(F_1\\)-score of 79.3% and resulting in a realistic tree species map.\nOsco et al. (2020) use a convolutional neural network on UAV multispectral images to count citrus trees in an orchard by predicting a confidence map that shows the likelihood of each pixel containing a tree. They report \\(F_1\\)-scores of up to 0.95, which is impressive even for managed stands.\nVentura et al. (2024) describe a network they call HR-SFAnet, consisting of a VGG-16 (Simonyan and Zisserman 2014) backbone feature extractor, a confidence head, and a parallel attention head, for detecting individual trees in urban environments using high-resolution multispectral images. The network is a deeper modification of the SFANet proposed in Zhu et al. (2019) for counting the number of people in photos. They report \\(F_1\\)-scores of 0.75, with average positioning error of 2.2 meters.\n\n\n2.4.2 Terrestrial LiDAR\nTerrestrial LiDAR surveys usually provide very dense and detailed point clouds. Most importantly for the task of localizing individual trees, terrestrial measurements always capture the trunks of the trees clearly. Tree trunks are very useful for tree detection and segmentation, and there are many algorithms that use bottom-to-top approaches that trace the trunks into the canopies.\nBurt, Disney, and Calders (2018) introduce treeseg, a software package written in C++ for extracting individual trees from LiDAR point clouds. Even though it is designed and presented as a platform-agnostic method made to operate on both terrestrial and UAV LiDAR point clouds, it relies on trunk detection, which is common for many methods based on terrestrial LiDAR and often is not applicable to UAV LiDAR data.\nLópez Serrano et al. (2022) introduce another software package called AID-FOREST for fully automatic processing of terrestrial LiDAR point clouds based on trunk detection. The software takes in a raw point cloud, performs all required preprocessing steps, and runs an object detection neural network on a series of horizontal slices to find cross-sections of trunks, and then processes the detection results from each level to track individual trees.\nAllen et al. (2022) describe an approach for classifying terrestrial LiDAR point clouds of individual trees extracted by treeseg using multi-view representation based on Goyal et al. (2021) and a convolutional ResNet network (He et al. 2016). They report high classification accuracies both overall and per-species on a dataset of almost 2500 individual trees.\nWilkes et al. (2023) introduce TLS2trees, another automated framework for segmentation of individual trees in terrestrial LiDAR point clouds, consisting of a set of Python command line tools. The framework consists of three steps: preprocessing, semantic segmentation, and instance segmentation into a set of individual trees. After that, a set of attributes are computed for each tree. The software is designed to be horizontally scalable (meaning it’s easy to speed up calculations by using more machines doing them in parallel, as opposed to vertically, meaning by increasing the computational resources available to a single machine) to address huge datasets produced during terrestrial laser scanning surveys.\nAll these software packages are impressive, but non-applicable to the data described in this work.\nViana et al. (2022) use a small dataset to compare tree-level inventory metrics extracted from a terrestrial LiDAR survey and from a manual inventory. They report very good results, showing that terrestrial LiDAR can serve as a replacement for manual inventories in diverse secondary forests (secondary forests are forests that regenerate naturally after significant disturbances like logging, storms, or fires).\nNurunnabi et al. (2024) offer a compelling example of how detailed terrestrial LiDAR point clouds can be used to provide very detailed analyses on tree level. The authors report high accuracies on the task of segmenting the point clouds into wood and leaf points by utilizing geometric features mentioned earlier to locate linear and non-linear areas. They then apply an octree-based segmentation algorithm to develop a precise 3D structure of a tree.\n\n\n2.4.3 UAV LiDAR\nA common approach to utilize LiDAR point cloud data for tree detection is by calculating canopy height maps – images where each pixel represents the height of vegetation – and applying the same techniques as for image-only approaches.\nA substantial number of different tree detection algorithms are variations of the local maxima filter, differing in what the filter is applied to, how many times, with what windows, and how the results are combined or preprocessed. For example, Douss and Farah (2022) offers an exploration of how different window size functions for the local maxima filtering applied to LiDAR-derived canopy height maps affect the quality of individual tree detection. They show that for a moderately dense forest in France, it is possible to find the parameters for the local maxima filter window that produce satisfactory results. It is, however, clear that it requires great deal of manual adjustment, and visual inspection of the results of detection makes it clear that they are only locally consistent. Any change in the canopy structure patterns noticeably affects the quality of the detection even for sophisticated window functions.\nLisiewicz et al. (2022) propose a way to improve the results of canopy height map-based individual tree segmentation algorithms. Their approach consists of three steps. The first step is the classification of segments into correct, under-segmented, or over-segmented using a Random Forest classifier, the second step is the refinement of under-segmentation errors by re-segmenting them with adjusted parameters, and the last step is the refinement of over-segmentation errors by merging with the correct segments using an algorithmic approach based on a collection of intensity-derived features. The authors suggest that this approach is universal in terms of forest composition and complexity, unlike many other ad-hoc correction methods based on the study area and thus non-generalizable.\nZ. Wang et al. (2023) propose a two-stage network they call Tree Region-Based Convolutional Neural Network (RCNN) to detect trees in UAV LiDAR point clouds. The first step is generation of dense anchors across the point cloud that are then processed by the RCNN to generate proposals for individual tree locations. The second step involves multi-position feature extraction to refine the proposals. The approach is evaluated on the NewFor benchmark (Eysn et al. 2015) and outperforms all benchmark methods that come with it.\nFu et al. (2024) propose a method for segmenting individual trees in UAV LiDAR point clouds based on a multiscale adaptive local maximum filter applied to the smoothed canopy height map to detect tree tops, a region-growing method for crown delineation that are then refined by a voxel-based clustering algorithm. The method is developed and tested on 21 synthetic plots and one actual survey, and the authors report good accuracy for estimation of both tree locations and heights. Worth noting that both the synthetic and real forest have relatively well-defined an unambiguous canopy structure.\n\n\n2.4.4 Fusion of data\nMany works don’t just use one data source and instead combine multiple complimentary ones for better representation. This subsection gives an overview of data fusion approaches for forestry applications on individual tree scale.\nLian et al. (2022) describe an approach for calculating individual tree biomass by combining UAV multispectral, UAV LiDAR and terrestrial LiDAR data. Terrestrial and UAV LiDAR are merged to get a more representative point cloud, showing the trees equally well both from above and from the ground. The multispectral image is used to generate a species classification map, which is then used to segment the point cloud. The segmented cloud is then used to estimate diameter at breast height and height and calculate above ground biomass.\nQ. Li et al. (2023) explore the contributions of multispectral images, very high resolution panchromatic images, and LiDAR data during fusion on feature and decision level for tree species classification using Support Vector Machine and Random Forest classifiers on manually delineated tree crowns from an urban environment. They report that fusion consistently improves the results over using each of the data sources on its own. They also report that fusion on decision level resulted in the highest overall accuracy metrics.\nBalestra et al. (2024) offers a review of 151 publications concerning fusion of LiDAR data with other remote sensing data. The authors report that in most cases fusion improves the results. Most relevant to this thesis, they report that for individual tree segmentation the results of fusion-based approaches compared to LiDAR-only ones are consistently better. La et al. (2015) show an increase from 63% to 92% for low-density forests, and from 62% to 70% for high-density forests, Aubry-Kientz et al. (2021) show and improvement by 5%, and Zhen, Quackenbush, and Zhang (2014) and Arenas-Corraliza, Nieto, and Moreno (2020) improved their results by 2-4%.\nFerreira et al. (2024) use a U-Net semantic segmentation model for fusion of VNIR images and UAV LiDAR-derived feature maps including surface normals of the canopies, reflection intensities, canopy heights, and leaf-area index for mapping tree species in urban tropical areas. Their results show that adding LiDAR-based features improves \\(F_1\\)-scores across all considered species, with average \\(F_1\\)-score 84.1. They also use the segment anything model (Kirillov et al. 2023) to automatically segment tree crowns with outstanding 98% boundary \\(F_1\\)-score.\nWu et al. (2024) use a combination of high-resolution RGB images and LiDAR to classify urban tree species by extracting seven different feature types and using a Random Forest for pixel-level classification. They report on seven experiments using different combinations of extracted features. The results show an improvement of 18.5% in accuracy when using fusion of LiDAR and RGB compared to RGB-only.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature review</span>"
    ]
  },
  {
    "objectID": "chapters/02_literature_review.html#summary",
    "href": "chapters/02_literature_review.html#summary",
    "title": "2  Literature review",
    "section": "2.5 Summary",
    "text": "2.5 Summary\nThere is a lot of research going on currently on individual tree-level inventories using UAV remote sensing data. Most results that can be considered successful are either in more mild environments, such as urban or managed forests, in predominantly coniferous forests, or use a much more detailed, but at the same time much more labor-intensive data source – terrestrial LiDAR. Mild forest types usually have canopy structures that are a lot easier to interpret, since the trees often stand far enough apart to be separable. At the same time, they allow for much more signal penetration, resulting in a good representation of tree trunks in the data, which is tremendously useful for the task of detecting trees. Predominantly coniferous forests, even dense ones, also have a canopy structure that is easy to interpret because conifer trees have a characteristic triangle shape that results in spikes in canopy height even when trees are standing close to each other. Terrestrial LiDAR data is different from UAV LiDAR in the amount of detail and, most importantly, the perspective. It surveys the trees from below, virtually guaranteeing the presence of trunks in the point clouds. Several fully automated approaches to detailed forest inventories using terrestrial LiDAR data exist that rely on the detection of trunks. A gap persists in solutions that would consistently work on UAV remote sensing data over dense mixed forests, with complex overlapping canopies. The active nature and the vertical structure information provided by LiDAR is essential in such complex environments. At the same time, many results indicate that fusion of LiDAR point clouds with other data sources consistently improves the results in forestry applications, which is why the proposed framework relies on fusion of LiDAR with RGB imagery.\n\n\n\n\nAllen, Matthew J., Stuart W. D. Grieve, Harry J. F. Owen, and Emily R. Lines. 2022. “Tree Species Classification from Complex Laser Scanning Data in Mediterranean Forests Using Deep Learning.” Methods in Ecology and Evolution, September. https://doi.org/10.1111/2041-210X.13981.\n\n\nArenas-Corraliza, Isabel, Ana Nieto, and Gerardo Moreno. 2020. “Automatic Mapping of Tree Crowns in Scattered-Tree Woodlands Using Low-Density LiDAR Data and Infrared Imagery.” Agroforestry Systems 94 (5): 1989–2002. https://doi.org/10.1007/s10457-020-00517-2.\n\n\nAubry-Kientz, Mélaine, Anthony Laybros, Ben Weinstein, James G. C. Ball, Toby Jackson, David Coomes, and Grégoire Vincent. 2021. “Multisensor Data Fusion for Improved Segmentation of Individual Tree Crowns in Dense Tropical Forests.” IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 14: 3927–36. https://doi.org/10.1109/JSTARS.2021.3069159.\n\n\nBalestra, Mattia, Suzanne Marselis, Temuulen Tsagaan Sankey, Carlos Cabo, Xinlian Liang, Martin Mokroš, Xi Peng, et al. 2024. “LiDAR Data Fusion to Improve Forest Attribute Estimates: A Review.” Current Forestry Reports 10 (4): 281–97. https://doi.org/10.1007/s40725-024-00223-7.\n\n\nBello, Saifullahi Aminu, Shangshu Yu, Cheng Wang, Jibril Muhmmad Adam, and Jonathan Li. 2020. “Review: Deep Learning on 3D Point Clouds.” Remote Sensing 12 (11): 1729. https://doi.org/10.3390/rs12111729.\n\n\nBouvier, Marc, Sylvie Durrieu, Richard A. Fournier, and Jean-Pierre Renaud. 2015. “Generalizing Predictive Models of Forest Inventory Attributes Using an Area-Based Approach with Airborne LiDAR Data.” Remote Sensing of Environment 156 (January): 322–34. https://doi.org/10.1016/j.rse.2014.10.004.\n\n\nBurt, Andrew, Mathias Disney, and Kim Calders. 2018. “Extracting Individual Trees from Lidar Point Clouds Using Treeseg.” Edited by Sarah Goslee. Methods in Ecology and Evolution, December, 2041–210X.13121. https://doi.org/10.1111/2041-210X.13121.\n\n\nCarson, Ward W, Hans-Erik Andersen, Stephen E Reutebuch, and Robert J McGaughey. 2004. “LiDAR Applications in Forestry—An Overview.” In ASPRS Annual Conference Proceedings, 4.\n\n\nDouss, Rim, and Imed Riadh Farah. 2022. “Extraction of Individual Trees Based on Canopy Height Model to Monitor the State of the Forest.” Trees, Forests and People 8 (June): 100257. https://doi.org/10.1016/j.tfp.2022.100257.\n\n\nEysn, Lothar, Markus Hollaus, Eva Lindberg, Frédéric Berger, Jean-Matthieu Monnet, Michele Dalponte, Milan Kobal, et al. 2015. “A Benchmark of Lidar-Based Single Tree Detection Methods Using Heterogeneous Forest Data from the Alpine Space.” Forests 6 (5): 1721–47. https://doi.org/10.3390/f6051721.\n\n\nFerrari, Felipe, Matheus Pinheiro Ferreira, Cláudio Aparecido Almeida, and Raul Queiroz Feitosa. 2023. “Fusing Sentinel-1 and Sentinel-2 Images for Deforestation Detection in the Brazilian Amazon Under Diverse Cloud Conditions.” IEEE Geoscience and Remote Sensing Letters 20: 1–5. https://doi.org/10.1109/LGRS.2023.3242430.\n\n\nFerreira, Matheus Pinheiro, Daniel Rodrigues Dos Santos, Felipe Ferrari, Luiz Carlos Teixeira Coelho Filho, Gabriela Barbosa Martins, and Raul Queiroz Feitosa. 2024. “Improving Urban Tree Species Classification by Deep-Learning Based Fusion of Digital Aerial Images and LiDAR.” Urban Forestry & Urban Greening 94 (April): 128240. https://doi.org/10.1016/j.ufug.2024.128240.\n\n\nFu, Yuwen, Yifang Niu, Li Wang, and Wang Li. 2024. “Individual-Tree Segmentation from UAV–LiDAR Data Using a Region-Growing Segmentation and Supervoxel-Weighted Fuzzy Clustering Approach.” Remote Sensing 16 (4): 608. https://doi.org/10.3390/rs16040608.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. Adaptive Computation and Machine Learning. Cambridge, Massachusetts: The MIT Press.\n\n\nGoyal, Ankit, Hei Law, Bowei Liu, Alejandro Newell, and Jia Deng. 2021. “Revisiting Point Cloud Shape Classification with a Simple and Effective Baseline.” In Proceedings of the 38th International Conference on Machine Learning, 3809–20. PMLR. https://proceedings.mlr.press/v139/goyal21a.html.\n\n\nGuo, Li, Nesrine Chehata, Clément Mallet, and Samia Boukir. 2011. “Relevance of Airborne Lidar and Multispectral Image Data for Urban Scene Classification Using Random Forests.” ISPRS Journal of Photogrammetry and Remote Sensing 66 (1): 56–66. https://doi.org/10.1016/j.isprsjprs.2010.08.007.\n\n\nGuo, Yulan, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. 2021. “Deep Learning for 3D Point Clouds: A Survey.” IEEE Transactions on Pattern Analysis and Machine Intelligence 43 (12): 4338–64. https://doi.org/10.1109/TPAMI.2020.3005434.\n\n\nHansen, Johannes N., Edward T. A. Mitchard, and Stuart King. 2020. “Assessing Forest/Non-Forest Separability Using Sentinel-1 C-Band Synthetic Aperture Radar.” Remote Sensing 12 (11): 1899. https://doi.org/10.3390/rs12111899.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. “Deep Residual Learning for Image Recognition.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 770–78. https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html.\n\n\nJakubowski, Marek K., Qinghua Guo, and Maggi Kelly. 2013. “Tradeoffs Between Lidar Pulse Density and Forest Measurement Accuracy.” Remote Sensing of Environment 130 (March): 245–53. https://doi.org/10.1016/j.rse.2012.11.024.\n\n\nKc, Yam Bahadur, Qijing Liu, Pradip Saud, Damodar Gaire, and Hari Adhikari. 2024. “Estimation of Above-Ground Forest Biomass in Nepal by the Use of Airborne LiDAR, and Forest Inventory Data.” Land 13 (2): 213. https://doi.org/10.3390/land13020213.\n\n\nKirillov, Alexander, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, et al. 2023. “Segment Anything.” In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 4015–26.\n\n\nLa, Hien Phu, Yang Dam Eo, Anjin Chang, and Changjae Kim. 2015. “Extraction of Individual Tree Crown Using Hyperspectral Image and LiDAR Data.” KSCE Journal of Civil Engineering 19 (4): 1078–87. https://doi.org/10.1007/s12205-013-1178-z.\n\n\nLa Rosa, Laura Elena Cué, Camile Sothe, Raul Queiroz Feitosa, Cláudia Maria De Almeida, Marcos Benedito Schimalski, and Dário Augusto Borges Oliveira. 2021. “Multi-Task Fully Convolutional Network for Tree Species Mapping in Dense Forests Using Small Training Hyperspectral Data.” ISPRS Journal of Photogrammetry and Remote Sensing 179 (September): 35–49. https://doi.org/10.1016/j.isprsjprs.2021.07.001.\n\n\nLassalle, Guillaume, Matheus Pinheiro Ferreira, Laura Elena Cué La Rosa, and Carlos Roberto de Souza Filho. 2022. “Deep Learning-Based Individual Tree Crown Delineation in Mangrove Forests Using Very-High-Resolution Satellite Imagery.” ISPRS Journal of Photogrammetry and Remote Sensing 189 (July): 220–35. https://doi.org/10.1016/j.isprsjprs.2022.05.002.\n\n\nLi, Qian, Baoxin Hu, Jiali Shang, and Hui Li. 2023. “Fusion Approaches to Individual Tree Species Classification Using Multisource Remote Sensing Data.” Forests 14 (7): 1392. https://doi.org/10.3390/f14071392.\n\n\nLi, Wenkai, Qinghua Guo, Marek K. Jakubowski, and Maggi Kelly. 2012. “A New Method for Segmenting Individual Trees from the Lidar Point Cloud.” Photogrammetric Engineering & Remote Sensing 78 (1): 75–84. https://doi.org/10.14358/PERS.78.1.75.\n\n\nLian, Xugang, Hailang Zhang, Wu Xiao, Yunping Lei, Linlin Ge, Kai Qin, Yuanwen He, et al. 2022. “Biomass Calculations of Individual Trees Based on Unmanned Aerial Vehicle Multispectral Imagery and Laser Scanning Combined with Terrestrial Laser Scanning in Complex Stands.” Remote Sensing 14 (19): 4715. https://doi.org/10.3390/rs14194715.\n\n\nLisiewicz, Maciej, Agnieszka Kamińska, Bartłomiej Kraszewski, and Krzysztof Stereńczak. 2022. “Correcting the Results of CHM-Based Individual Tree Detection Algorithms to Improve Their Accuracy and Reliability.” Remote Sensing 14 (8): 1822. https://doi.org/10.3390/rs14081822.\n\n\nLong, Jonathan, Evan Shelhamer, and Trevor Darrell. 2015. “Fully Convolutional Networks for Semantic Segmentation.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 3431–40. https://openaccess.thecvf.com/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html.\n\n\nLópez Serrano, F. R., E. Rubio, F. A. García Morote, M. Andrés Abellán, M. I. Picazo Córdoba, F. García Saucedo, E. Martínez García, et al. 2022. “Artificial Intelligence-Based Software (AID-FOREST) for Tree Detection: A New Framework for Fast and Accurate Forest Inventorying Using LiDAR Point Clouds.” International Journal of Applied Earth Observation and Geoinformation 113 (September): 103014. https://doi.org/10.1016/j.jag.2022.103014.\n\n\nLucas, Chris, Willem Bouten, Zsófia Koma, W. Daniel Kissling, and Arie C. Seijmonsbergen. 2019. “Identification of Linear Vegetation Elements in a Rural Landscape Using LiDAR Point Clouds.” Remote Sensing 11 (3): 292. https://doi.org/10.3390/rs11030292.\n\n\nMallet, Clément, Frédéric Bretar, Michel Roux, Uwe Soergel, and Christian Heipke. 2011. “Relevance Assessment of Full-Waveform Lidar Data for Urban Area Classification.” ISPRS Journal of Photogrammetry and Remote Sensing 66 (6): S71–84. https://doi.org/10.1016/j.isprsjprs.2011.09.008.\n\n\nMartins, Gabriela Barbosa, Laura Elena Cué La Rosa, Patrick Nigri Happ, Luiz Carlos Teixeira Coelho Filho, Celso Junius F. Santos, Raul Queiroz Feitosa, and Matheus Pinheiro Ferreira. 2021. “Deep Learning-Based Tree Species Mapping in a Highly Diverse Tropical Urban Setting.” Urban Forestry & Urban Greening 64 (September): 127241. https://doi.org/10.1016/j.ufug.2021.127241.\n\n\nNæsset, Erik. 1997a. “Determination of Mean Tree Height of Forest Stands Using Airborne Laser Scanner Data.” ISPRS Journal of Photogrammetry and Remote Sensing 52 (2): 49–56. https://doi.org/10.1016/S0924-2716(97)83000-6.\n\n\n———. 1997b. “Estimating Timber Volume of Forest Stands Using Airborne Laser Scanner Data.” Remote Sensing of Environment 61 (2): 246–53. https://doi.org/10.1016/S0034-4257(97)00041-2.\n\n\nNelson, Ross, William Krabill, and Gordon MacLean. 1984. “Determining Forest Canopy Characteristics Using Airborne Laser Data.” Remote Sensing of Environment 15 (3): 201–12. https://doi.org/10.1016/0034-4257(84)90031-2.\n\n\nNilsson, Mats. 1996. “Estimation of Tree Heights and Stand Volume Using an Airborne Lidar System.” Remote Sensing of Environment 56 (1): 1–7. https://doi.org/10.1016/0034-4257(95)00224-3.\n\n\nNurunnabi, Abdul, Felicia Teferle, Debra F. Laefer, Meida Chen, and Mir Masoom Ali. 2024. “Development of a Precise Tree Structure from LiDAR Point Clouds.” The International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences XLVIII-2-2024 (June): 301–8. https://doi.org/10.5194/isprs-archives-XLVIII-2-2024-301-2024.\n\n\nOsco, Lucas Prado, Mauro Dos Santos De Arruda, José Marcato Junior, Neemias Buceli Da Silva, Ana Paula Marques Ramos, Érika Akemi Saito Moryia, Nilton Nobuhiro Imai, et al. 2020. “A Convolutional Neural Network Approach for Counting and Geolocating Citrus-Trees in UAV Multispectral Imagery.” ISPRS Journal of Photogrammetry and Remote Sensing 160 (February): 97–106. https://doi.org/10.1016/j.isprsjprs.2019.12.010.\n\n\nÖzdemi̇r, Emre. 2021. “A Deep Learning Framework for Geospatial Point Cloud Classification.” PhD thesis, Moscow: Skolkovo Institute of Science; Technology. https://www.skoltech.ru/app/data/uploads/2021/12/thesis-2.pdf.\n\n\nPauly, M., M. Gross, and L. P. Kobbelt. 2002. “Efficient Simplification of Point-Sampled Surfaces.” In IEEE Visualization, 2002. VIS 2002., 163–70. Boston, MA, USA: IEEE. https://doi.org/10.1109/VISUAL.2002.1183771.\n\n\nQi, Charles R., Hao Su, Kaichun Mo, and Leonidas J. Guibas. 2017. “PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation.” In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 652–60.\n\n\nQi, Charles R., Li Yi, Hao Su, and Leonidas J. Guibas. 2017. “PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space.” In Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc.\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” In Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015, edited by Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi, 234–41. Lecture Notes in Computer Science. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-24574-4_28.\n\n\nRoussel, Jean-Romain, David Auty, Nicholas C. Coops, Piotr Tompalski, Tristan R. H. Goodbody, Andrew Sánchez Meador, Jean-François Bourdon, Florian de Boissieu, and Alexis Achim. 2020. “lidR: An R Package for Analysis of Airborne Laser Scanning (ALS) Data.” Remote Sensing of Environment 251: 112061. https://doi.org/10.1016/j.rse.2020.112061.\n\n\nSimonyan, Karen, and Andrew Zisserman. 2014. “Very Deep Convolutional Networks for Large-Scale Image Recognition.” arXiv. https://doi.org/10.48550/ARXIV.1409.1556.\n\n\nSinica-Sinavskis, Juris, and Gunta Grube. 2022. “Forest Stand Volume Estimation by Species from Sentinel-2 and LiDAR Data Using Regression Models.” In 2022 18th Biennial Baltic Electronics Conference (BEC), 1–5. https://doi.org/10.1109/BEC56180.2022.9935590.\n\n\nTreitz, Paul, Kevin Lim, Murray Woods, Doug Pitt, Dave Nesbitt, and Dave Etheridge. 2012. “LiDAR Sampling Density for Forest Resource Inventories in Ontario, Canada.” Remote Sensing 4 (4): 830–48. https://doi.org/10.3390/rs4040830.\n\n\nVentura, Jonathan, Camille Pawlak, Milo Honsberger, Cameron Gonsalves, Julian Rice, Natalie L. R. Love, Skyler Han, et al. 2024. “Individual Tree Detection in Large-Scale Urban Environments Using High-Resolution Multispectral Imagery.” International Journal of Applied Earth Observation and Geoinformation 130 (June): 103848. https://doi.org/10.1016/j.jag.2024.103848.\n\n\nVermeer, Martijn, Jacob Alexander Hay, David Völgyes, Zsófia Koma, Johannes Breidenbach, and Daniele Stefano Maria Fantin. 2023. “Lidar-Based Norwegian Tree Species Detection Using Deep Learning.” arXiv. https://doi.org/10.48550/arXiv.2311.06066.\n\n\nViana, Aguida Beatriz Travaglia, Carlos Moreira Miquelino Eleto Torres, Cibele Humel do Amaral, Elpídio Inácio Fernandes Filho, Carlos Pedro Boechat Soares, Felipe Carvalho Santana, Lucas Brandão Timo, and Samuel José Silva Soares da Rocha. 2022. “Timber Volume Estimation By Using Terrestrial Laser Scanning: Method In Hyperdiverse Secondary Forests.” Revista Árvore 46 (August). https://doi.org/10.1590/1806-908820220000021.\n\n\nWang, Xizhao, Yanxia Zhao, and Farhad Pourpanah. 2020. “Recent Advances in Deep Learning.” International Journal of Machine Learning and Cybernetics 11 (4): 747–50. https://doi.org/10.1007/s13042-020-01096-5.\n\n\nWang, Zhen, Pu Li, Yuancheng Cui, Shuowen Lei, and Zhizhong Kang. 2023. “Automatic Detection of Individual Trees in Forests Based on Airborne LiDAR Data with a Tree Region-Based Convolutional Neural Network (RCNN).” Remote Sensing 15 (4): 1024. https://doi.org/10.3390/rs15041024.\n\n\nWeinmann, M., B. Jutzi, and C. Mallet. 2013. “Feature Relevance Assessment for the Semantic Interpretation of 3D Point Cloud Data.” ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences II-5-W2 (October): 313–18. https://doi.org/10.5194/isprsannals-II-5-W2-313-2013.\n\n\nWeinstein, Ben G., Sergio Marconi, Mélaine Aubry-Kientz, Gregoire Vincent, Henry Senyondo, and Ethan P. White. 2020. “DeepForest: A Python Package for RGB Deep Learning Tree Crown Delineation.” Edited by Sydne Record. Methods in Ecology and Evolution 11 (12): 1743–51. https://doi.org/10.1111/2041-210X.13472.\n\n\nWeinstein, Ben G., Sergio Marconi, Stephanie Bohlman, Alina Zare, and Ethan White. 2019. “Individual Tree-Crown Detection in RGB Imagery Using Semi-Supervised Deep Learning Neural Networks.” Remote Sensing 11 (11): 1309. https://doi.org/10.3390/rs11111309.\n\n\nWest, Karen F., Brian N. Webb, James R. Lersch, Steven Pothier, Joseph M. Triscari, and A. E. Iverson. 2004. “Context-Driven Automated Target Detection in 3D Data.” In Defense and Security, edited by Firooz A. Sadjadi, 133–43. Orlando, FL. https://doi.org/10.1117/12.542536.\n\n\nWhite, Joanne C., Michael A. Wulder, Andrés Varhola, Mikko Vastaranta, Nicholas C. Coops, Bruce D. Cook, Doug Pitt, and Murray Woods. 2013. A Best Practices Guide for Generating Forest Inventory Attributes from Airborne Laser Scanning Data Using the Area-Based Approach. Victoria, British Columbia: Canadian Forest Service.\n\n\nWilkes, Phil, Mathias Disney, John Armston, Harm Bartholomeus, Lisa Bentley, Benjamin Brede, Andrew Burt, et al. 2023. “TLS2trees : A Scalable Tree Segmentation Pipeline for TLS Data.” Methods in Ecology and Evolution 14 (12): 3083–99. https://doi.org/10.1111/2041-210X.14233.\n\n\nWu, Jingru, Qixia Man, Xinming Yang, Pinliang Dong, Xiaotong Ma, Chunhui Liu, and Changyin Han. 2024. “Fine Classification of Urban Tree Species Based on UAV-Based RGB Imagery and LiDAR Data.” Forests 15 (2): 390. https://doi.org/10.3390/f15020390.\n\n\nZhang, Zhengnan, Tiejun Wang, Andrew K. Skidmore, Fuliang Cao, Guanghui She, and Lin Cao. 2023. “An Improved Area-Based Approach for Estimating Plot-Level Tree DBH from Airborne LiDAR Data.” Forest Ecosystems 10 (January): 100089. https://doi.org/10.1016/j.fecs.2023.100089.\n\n\nZhen, Zhen, Lindi Quackenbush, and Lianjun Zhang. 2014. “Impact of Tree-Oriented Growth Order in Marker-Controlled Region Growing for Individual Tree Crown Delineation Using Airborne Laser Scanner (ALS) Data.” Remote Sensing 6 (1): 555–79. https://doi.org/10.3390/rs6010555.\n\n\nZhu, Liang, Zhijian Zhao, Chao Lu, Yining Lin, Yao Peng, and Tangren Yao. 2019. “Dual Path Multi-Scale Fusion Networks with Attention for Crowd Counting.” arXiv. https://doi.org/10.48550/ARXIV.1902.01115.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Literature review</span>"
    ]
  },
  {
    "objectID": "chapters/03_materials_and_methods.html",
    "href": "chapters/03_materials_and_methods.html",
    "title": "3  Materials and methods",
    "section": "",
    "text": "3.1 Lysva field inventory dataset\nThe main original dataset used in the study is the Lysva field inventory dataset, named by the closest town to its location. The dataset is released into open access with an accompanying paper that describes the data in detail and provides a basic baseline for individual tree detection (Dubrovin and Fortin 2024). The study area is located in Perm Krai, Russia, 86 kilometers to the east of Perm. The dataset consists of a field inventory of 3600 trees across 10 rectangular ground plots 100 meters in lengths and 50 meters in width fully covered by a UAV LiDAR and RGB orthophoto surveys. Figure 3.1 shows the locations of the ground plots over the full size RGB orthophoto and a visualization of the field inventory for a single plot on top of the LiDAR point cloud. Colored points represent trees, with different colors mapping to different species. The point cloud is visualized as a 2D scatter plot with points colored by height (darker points are lower, brighter points are higher, and points are unsorted – some lower points end up over higher ones). The baseline is a simple one-pass local maxima filter applied directly on the point cloud with a fixed window size.\nThe field inventory is a tabular dataset where every row represents a single tree. Table 3.1 shows a random sample of entries from the field inventory table. Every tree is represented by a point in UTM 40N coordinate reference system (EPSG:32640). Every tree has a species label and diameter at breast height (dbh), measured with calipers at 1.3 m from the ground at two perpendicular directions and averaged. Figure 3.2 shows the distribution of species in the data: the dominant species is spruce, but overall the trees are evenly split between deciduous and coniferous, 1793 and 1807 respectively, with seven species in total: spruce, birch, fir, aspen, tilia, alder, and willow. Approximately 20% of the trees have height data measured during the inventory, and 10% have ages measured on core samples, shown in the table in meters and years respectively.\nThe LiDAR sensor used for the survey is AGM-MS3 produced by AGM Systems. It has 640 kHz acquisition rate, 300-meter range, and spatial accuracy of 3–5 centimeters. The raw point clouds were processed with the combination of the AGM ScanWorks software from the sensor vendor and the TerraScan software. The point clouds were preprocessed by removing duplicate points and high and low noise points. The duplicate removal was run with a threshold distance between points of 1 mm. Noise was removed by visually inspecting the point cloud and manually selecting height thresholds to cut off points that are lower than the ground or higher than the canopies. Ground point classification was performed and ground points were used to normalize height by subtracting the ground level from the Z coordinate of every point. Height normalization allows treating the Z coordinate as height above ground rather than the absolute elevation, which simplifies many subsequent steps. The camera used for the orthophoto survey is Sony A6000. The resolution of the orthophoto is 7 centimeters per pixel. Figure 3.3 is a 3D visualization of the point cloud over plot number 10. It shows the unmodified point cloud on the left, with points colored by height above ground, and a point cloud enriched with color information by sampling the orthophoto at the planar coordinates of the points. Figure 3.4 shows the orthophoto for the same plot. The carrier UAV was configured to follow the terrain at 150 meter height using the SRTM elevation map as a reference (Farr and Kobrick 2000).\nTable 3.2 shows some descriptive statistics for each plot in the field inventory: the number of trees in the plot, average LiDAR point density in points per square meter, and dominant species type. The overall average point density is 37 points per square meter. Exactly half of the plots are predominantly coniferous and half are predominantly deciduous. Figure 3.5 shows two point clouds clipped by plot bounds in 3D, highlighting the differences in canopy structure complexity between predominantly deciduous and coniferous plots. The figure highlights that the forest is indeed dense and mixed, with non-uniform, complex canopy structure.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Materials and methods</span>"
    ]
  },
  {
    "objectID": "chapters/03_materials_and_methods.html#sec-lysva-dataset",
    "href": "chapters/03_materials_and_methods.html#sec-lysva-dataset",
    "title": "3  Materials and methods",
    "section": "",
    "text": "Figure 3.1: The study region for the Lysva field inventory dataset. Left: The locations of field survey plot boundaries on a full-size RGB orthophoto. Each plot is a 50 by 100 meter rectangle, with every tree within measured and recorded. Buffered cutouts of the orthophoto come with the dataset, along with LiDAR point clouds. Right: A close up of plot number 4. Each colored point represents a single tree of a different species, on top of a point cloud colored by the value of the height of each point (lower points are dark, higher points are bright) and the same orthophoto. Note that the points of the point cloud are unsorted, and some lower points overlap higher points. Figure reused from Dubrovin, Fortin, and Kedrov (2024).\n\n\n\n\n\n\n\n\n\nTable 3.1: Example of data in the field inventory table. Each row is a recorded tree.\n\n\n\n\n\n\nplot\ntree_no\nspecies\nd1\nd2\ndbh\nage\nheight\nangle\ncomment\ngeometry\n\n\n\n\n7.0\n111.0\nBirch\n23.0\n23.0\n23.00\nNaN\nNaN\n0.0\nNone\nPOINT (545784.419 6449359.132)\n\n\n5.0\n136.0\nFir\n23.5\n23.0\n23.25\n90.0\n17.5\n0.0\nRotten\nPOINT (545721.242 6449901.051)\n\n\n3.0\n119.0\nAspen\n36.1\n42.1\n39.10\n89.0\n25.5\n0.0\nNone\nPOINT (546630.326 6450158.241)\n\n\n9.0\n345.0\nSpruce\n19.7\n22.0\n20.85\nNaN\n15.9\n0.0\nNone\nPOINT (546201.645 6449118.199)\n\n\n6.0\n267.0\nSpruce\n12.9\n12.9\n12.90\nNaN\nNaN\n0.0\nNone\nPOINT (545568.842 6449407.13)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Distribution of species in the Lysva field inventory dataset. The dominant species is spruce, but overall the split between coniferous and decidious species is even: there are 1807 coniferous and 1793 deciduous trees. Figure reused from Dubrovin, Fortin, and Kedrov (2024).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Points colored by height.\n\n\n\n\n\n\n\n\n\n\n\n(b) Points assigned color by sampling the orthophoto.\n\n\n\n\n\n\n\nFigure 3.3: A 3D visualisation of the UAV LiDAR point cloud of plot 10.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.4: A visualisation of the orthophoto of plot 10.\n\n\n\n\n\n\n\n\n\n\n\n\nTable 3.2: Statistics for the plots in the Lysva dataset.\n\n\n\n\n\n\nPlot\nTree count\nPoint density\nDominant type\n\n\n\n\n1.0\n420\n31.7\nDeciduous\n\n\n2.0\n365\n47.9\nDeciduous\n\n\n3.0\n332\n40.3\nDeciduous\n\n\n4.0\n261\n33.5\nConiferous\n\n\n5.0\n208\n14.2\nConiferous\n\n\n6.0\n290\n39.1\nConiferous\n\n\n7.0\n408\n41.9\nDeciduous\n\n\n8.0\n341\n35.5\nConiferous\n\n\n9.0\n459\n42.1\nConiferous\n\n\n10.0\n518\n42.9\nDeciduous\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.5: 3D visualizations of point clouds from plot 1 (predominantly deciduous) and plot 6 (predominantly coniferous). Note the difference in the canopy structure: it is relatively easy to tell confifiers apart visually, while deciduous species do not have pronounced shapes and are hard to discriminate. Figure reused from Dubrovin, Fortin, and Kedrov (2024).\n\n\n\n\n\n\n\n3.1.1 On using intensity-based features\nEven though some sources report intensity-based features as some of the most important ones (Shi et al. 2018), the features seem to me unreliable in the context of forestry because of the physics of light reflection. There are simply too many factors that affect the amplitude of the reflected signal, which might be useful when imaging stable targets such as urban environments but become completely unpredictable on highly unstable targets such as trees: they move in the wind in the time span of a single survey, they grow in the time span between repeated surveys, the leaves and branches are angled in every possible way. Moreover, the quality of the recorded intensities highly depends on the used sensor. As an example, Figure 3.6 shows the distribution of intensity values for every point in the Lysva dataset, and shows plot number 10 in 3D with points colored by their recorded intensity. The distribution of intensities seems like an artifact of faulty quantization (the fact that the maximum overall value is 63 makes me suspect it is stored by the hardware using a 6-bit unsigned integer, probably an ad hoc optimization by the sensor vendor). With this distribution in mind, it is not surprising that coloring points by their intensity values results in images that look like noise, and there is no signal to be exploited for predictive modeling.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Distribution of intensity over all points in the Lysva dataset.\n\n\n\n\n\n\n\n\n\n\n\n(b) A point cloud of plot 10 with points colored by intensity.\n\n\n\n\n\n\n\nFigure 3.6: An example of the unreliability of the intensity attribute.\n\n\n\n\nSome sensors do provide more consistent values. However, relying on intensity-based features limits the applicability of developed models and methods, as any such model would surely fail on data like this.\n\n\n3.1.2 Comparison to other datasets\nOur dataset is in many ways similar to the NewFor benchmark (Eysn et al. 2015). It serves the same purpose and also offers overlapping field survey ground plots and UAV LiDAR point clouds. There are, however, many notable differences. The NewFor benchmark covers much more diverse regions, including ground plots from France, Italy, Switzerland, Austria, and Slovenia, while all our data comes from the same area. The tree species covered by the datasets are also different: both contain spruce and fir, but the NewFor data also has beech, Scots pine, larch, sycamore, and poplar, while ours also has birch, aspen, tilia, alder, and willow. Our dataset has more than twice as many individual trees as the Alpine benchmark, and the forest is denser and more complex, making it more complicated to detect trees in. Our data contains very mild terrain variations, while the slopes of the terrain in Alpine data are very steep, which plays a role during height normalization, since subtraction of steep terrain introduces artificial slope to the points in the canopy, changing the overall shape of the tree. Our dataset has an additional information source – an RGB orthophoto that allows development of algorithms that fuse multimodal data, which, we believe, is a key to success in such complex environments. Our dataset has species labels for every surveyed tree, but only partial coverage of tree heights and no timber volume information at all.\nAnother similar dataset is the NeonTreeEvaluation Benchmark (Weinstein, Marconi, and White 2022), which offers bounding box annotation for tree detection across a wide range of different forest types. It offers coregistered RGB, LiDAR, and hyperspectral images over 31,000 individual trees. The main difference in the reference data between the NeonTreeEvaluation Benchmark and our dataset is the source: our data comes from a field inventory and thus has additional tree information that can be used in downstream tasks, such as species classification or timber volume prediction, while the NeonTreeEvaluation Benchmark annotations are created from the RGB photo and thus only offer the positions and sized of trees.\nAnother dataset is the IDTReeS 2020 Competition Data (Graves and Marconi 2020) aimed to develop algorithms for delineation and species classification of individual tree crowns in RGB, LiDAR, and hyperspectral data. It offers bounding box annotations for 1200 individual trees covered by RGB, LiDAR, and hyperspectral images in 3 national forests in the USA. Similarly, the source of the data is annotation of images, not a field inventory.\nThere are also datasets available that don’t have LiDAR point cloud coverage, or use terrestrial LiDAR instead of UAV LiDAR, or use photogrammetric point clouds instead of LiDAR point clouds. We do not mention them here because we specifically focus on UAV LiDAR.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Materials and methods</span>"
    ]
  },
  {
    "objectID": "chapters/03_materials_and_methods.html#sec-individual-trees-dataset",
    "href": "chapters/03_materials_and_methods.html#sec-individual-trees-dataset",
    "title": "3  Materials and methods",
    "section": "3.2 Individual tree point clouds dataset",
    "text": "3.2 Individual tree point clouds dataset\nThe main dataset used for training the models is a collection of point clouds of individual trees, sometimes referred to in this text as tree clouds, extracted manually from larger UAV LiDAR point clouds. The dataset is released into open access, and was originally presented in (Dubrovin and Fortin 2024), although it has been expanded since and now contains twice as many individual trees. It consists of 394 trees, 192 of which are extracted from the previously described Lysva survey, and 202 from other surveys in Perm Krai. The distinction between the parts is important because the Lysva dataset has RGB orthophoto coverage, making it possible to infuse the tree clouds with orthophoto-based features. Thus, for training the tree segmentation networks that rely on these features, the effective size of the dataset is 192 tree clouds, as only the former part is used. However, the whole dataset is used for training regression and classification models that process segmented trees.\nFigure 3.7 shows the distribution of species in the individual tree point clouds dataset. The split between coniferous and deciduous trees is almost even: there are 202 coniferous and 193 deciduous trees. There are seven species in total: spruce, birch, aspen, pine, fir, alder, and tilia, with most focus on 4 most important species listed first. Note the presence of pine trees, which are not present in the Lysva field inventory data, and the absence of willow trees. All the pine trees are from other field surveys. Willows are skipped intentionally, since they are of little interest in terms of timber harvesting: they are considered low quality, and their ripening cycle is mismatched with main timber species – when the overall plot is ready to be harvested, the willows are already rotten.\n\n\n\n\n\n\n\n\n\nFigure 3.7: Distribution of tree species in the individual trees dataset. It contains 394 point clouds of individual trees: 201 coniferous and 193 deciduous. Note the presence of pine trees, as there are no pine trees in the Lysva field inventory – all of the pines come from other field surveys.\n\n\n\n\n\n\nFigure 3.8 is a visualization of the data. It shows a random tree of every species as a 2D scatter plot and a single spruce as a 3D scatter plot with points colored by height. Because the observations are made from above, many trees have the highest concentrations of points at the top of their canopy and a very limited number of points along the trunk. Additionally, slight slopes of the terrain manifest as artificial tilt in some of the trees because of the height normalization of the original point cloud.\n\n\n\n\n\n\n\n\n\n\n\n\n\n(a) Cross-sections of random trees of every species (ignoring the Y dimension).\n\n\n\n\n\n\n\n\n\n\n\n(b) A spruce in 3D.\n\n\n\n\n\n\n\nFigure 3.8: Visualizations of the individual tree point clouds in the dataset. Most of the tree clouds are top-heavy because of the observation from above, and some are artificially tilted because of slight terrain slopes and height normalization. The ground points are present.\n\n\n\n\n\nAn important note is that the extracted trees were not chosen for extraction randomly but were selected by humans based on whether it was possible and relatively easy to separate from the surrounding trees. So there is a selection bias in there favoring the trees that are easily separable, standing outside of large dense clusters. Because of that the trees don’t exactly represent what a tree closely surrounded by other trees is like in a point cloud. It is especially apparent in very pronounced trunks of all the visualized trees. In a dense forest, such as the one visualized in Figure 3.5, there is hardly ever enough penetration for such detailed trunk coverage. In fact, as mentioned in the literature review, good coverage of trunks is an immensely useful feature of terrestrial LiDAR surveys, which consistently show very good results using algorithm that segment trees from the trunk up.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Materials and methods</span>"
    ]
  },
  {
    "objectID": "chapters/03_materials_and_methods.html#sec-synthetic-forest-dataset",
    "href": "chapters/03_materials_and_methods.html#sec-synthetic-forest-dataset",
    "title": "3  Materials and methods",
    "section": "3.3 Synthetic forests generated from individual trees",
    "text": "3.3 Synthetic forests generated from individual trees\nTo train tree segmentation models, training data where every point is assigned to an individual tree is required. Data like that are very labor-intensive to label. An alternative way to create such data is by generating it from a set of tree clouds, as the per point labels arise naturally in this case. I used the dataset of individual tree point clouds described in the previous section to generate a synthetic forest to train a tree segmentation PointNet++.\nSynthetic forest is generated in patches of set height and width by placing trees sampled from the full set from left to right, until the set width is extended, then from bottom to top, until the set height is extended. A height threshold is applied to each tree before placing it to remove ground points and non-tree reflections. This results in patches that are slightly bigger than the set size, but that can be cropped into the exact size. This has an added benefit of mimicking cutoff trees at the edges of the patch that are inevitable when the model is applied in a sliding window. The trees can be sampled with or without replacement, but since a set of augmentations described in Section 3.4.2 is applied to each tree individually, exact same tree never occurs in the patch fed to the model even when sampling with replacement. When trees are placed, their planar bounding boxes are tracked to avoid overlap. However, since some overlap might, in fact, be desired to better represent actual forests, a parameter that controls the amount of overlap is added to the dataset generation process. Each tree is also assigned a label, which simply tracks its ordinal number in the patch. An example of a 20 by 20 meter synthetic forest patch with 0.75 meter overlap and 2 meter height threshold is shown in Figure 3.9 from two different perspectives and using two color schemes: the label, i.e. the ordinal ID of the tree within a patch, and RGB color sampled from the orthophoto.\n\n\n\n\n\n\n\n\n\nFigure 3.9: A visualisation of a synthetic forest patch used for training the tree segmentation network. Patch width and height are set to 20 meters, overlap is set to 0.75. Top: Top-down view of the patch. Bottom: 3D view of the same patch. Left: Points colored by label: unique tree ID within the patch. Right: Points colored by RGB color sampled from the orthophoto.\n\n\n\n\n\n\nFor comparison, a patch of the same size from a real point cloud over one of the plots of the Lysva survey is shown in Figure 3.10.\n\n\n\n\n\n\n\n\n\nFigure 3.10: A piece of the Lysva LiDAR point cloud over plot 10 in within a 20 by 20 meter window. Top: Top-down view. Bottom: The same patch in 3D view. Left: Points colored by height. Right: Points colored by RGB color sampled from the orthophoto.\n\n\n\n\n\n\nAn alternative approach to creating patches of synthetic forest can be to use a fixed number of trees instead of fixed patch size. However, it poorly correlates with how the final model will be applied: in windows of fixed size, not windows with varying size but fixed number of trees. And size of the patch the model sees is important because the scale is normalized, and using a different patch sizes changes the scaled coordinates and confuses the model that learned to rely on them.\nEnriching point clouds using only color information from RGB orthophotos provides limited utility because it adds only local information to the points. As was mentioned in the introduction, one of the aspects of the complementary nature of the two data sources is that images provide continuous representations and capture textures. To enrich the image features with context, they can be preprocessed with feature extractors that encode context into pixel values and thus provide more information for the segmentation network. The feature extractors can be simple algorithms or specialized convolutional network feature extractors. For the same reason a simple PointNet++ is used as the architecture for the tree segmentation network, a simple collection of multi-scale features are used as orthophoto features, including intensity, edge, and texture features, extracted from the orthophotos using the scikit-image Python package. The features are calculated on different scales by applying Gaussian smoothing with varying parameters before calculation. Figure 3.11 shows example features from every mentioned group on a very fine scale. The top left image is the original orthophoto of plot 10 from the Lysva survey, the top right image is the intensity calculated from it, and bottom images are examples of an edge feature and a texture feature. The same features but on a coarser scale are shown in Figure 3.12.\n\n\n\n\n\n\n\n\n\nFigure 3.11: Basic orthophoto-based features used (on a single scale with sigma=1, actual features are across multiple increasing scales). Top left: The original orthophoto for plot 10. Top right: Intensity feature. Bottom left: Edges feature. Bottom right: Texture feature.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.12: Basic orthophoto-based features used (on a single scale with sigma=16, actual features are across multiple increasing scales). Top left: The original orthophoto for plot 10. Top right: Intensity feature. Bottom left: Edges feature. Bottom right: Texture feature.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Materials and methods</span>"
    ]
  },
  {
    "objectID": "chapters/03_materials_and_methods.html#training-tree-segmentation-neural-networks",
    "href": "chapters/03_materials_and_methods.html#training-tree-segmentation-neural-networks",
    "title": "3  Materials and methods",
    "section": "3.4 Training tree segmentation neural networks",
    "text": "3.4 Training tree segmentation neural networks\nThe architecture chosen to serve as the tree segmentation network is the PointNet++ (Qi et al. 2017), described in detail in Section 2.2. It is a relatively simple architecture, and further potential quality improvements might be achieved by using more modern and advanced architectures instead. However, the main goal of this thesis was to develop and verify an overall framework, and thus the choice was set on a model that is simple to implement and work with to allow easy experimentation with other parts of the proposed system.\nThe architecture of the used PointNet++ is similar to the segmentation architecture shown in Figure 2.3. The main differences from the shown architecture are the use of one more stacked set abstraction layer to make the network deeper, and the use of a regression head that predicts a continuous value for each point instead of a classification head that predicts per-point class scores, since the model needs to assign a unique ID to every tree in the patch. The three stacked set abstraction layers have the proportions of points sampled set to 0.75, 0.5, and 0.5, and neighborhood radii for feature aggregation set to 0.1, 0.2, and 0.4. Note that the scale of the input is normalized (see next section for details), so the radii are not in meters. The final model has 30 million trainable parameters.\n\n3.4.1 Coordinate and feature normalization\nIt’s a well established practice to scale the inputs to neural networks that is known to improve the speed and accuracy of gradient descent convergence (Bishop 2006). Before going through the network, a set of augmentations and transformations is applied to each synthetic forest patch. The augmentations are described in detail with visualized examples in Section 3.4.2. The transformations include scale and feature normalization – the coordinates are centered and normalized to the interval \\((-1, 1)\\) and the features are normalized to the interval \\((-1, 1)\\) without centering.\n\n\n3.4.2 Data augmentation\nThe primary objective of data augmentation is to enhance the quantity, quality, and variety of data used for training (Mumuni and Mumuni 2022). This is especially important when the sizes of the available datasets are limited, like in the case of the individual trees data that is the base of the synthetic forest datasets. Carefully selected augmentations allow increasing the effective dataset size. However, it is important to pay attention that the chosen augmentations keep the transformed examples semantically equivalent to the original. A readily understandable example of a bad augmentation in the task of digit classification, which is often used as the “hello world” of deep learning with the MNIST dataset of handwritten digits (Deng 2012), is a vertical flip, as most digits lose meaning when upside down. A flip is also a bad augmentation example for a synthetic forest patch described in Section 3.3, as it changes the order of the trees within a patch, thus breaking the labels that are assumed to increase in a specific pattern.\nIn the scenario when the data for is created by combining several smaller inputs, there is a possibility of applying augmentations on two different scales. The most simple approach is to treat a synthetic forest patch as a whole and apply augmentations directly to it. However, it is also possible to apply per-tree augmentations, effectively increasing the size of the underlying tree set from which synthetic forest patches constructed. Only the latter kind are used for training the tree segmentation network.\nThe first transformation that changes the shape but doesn’t affect any semantics for an individual tree is a random rotation around the vertical axis. A tree remains completely the same when rotated, but the coordinates of all points change. Figure 3.13 shows the effect of applying random rotation transformation of different magnitudes to a single aspen tree. For visualization purposes, the rotation is forced to apply with full magnitude for every parameter. During training, the angle is uniformly sampled from the specified range. For the final tree segmentation model, the range is set to \\([-180, 180]\\) degrees, as no amount of rotation breaks the semantics.\n\n\n\n\n\n\n\n\n\nFigure 3.13: Visualization of the random rotation around Z axis augmentation on a single aspen tree. The effect is forced to happen with full amplitude for visualization purposes, during training a rotation angle is uniformly sampled from a set range.\n\n\n\n\n\n\nAnother transformation that keeps the tree the same but changes the coordinates of the points is random scaling. It simply multiplies the coordinates by a scaling factor, making the tree larger or smaller in all directions. Unlike random rotation around the vertical axis, however, the range needs to be chosen much more carefully, as there is a possibility of making unrealistically large or small trees that would confuse the model during training. Figure 3.14 shows the effect of applying random scale transformation to a single aspen tree. Again, for purposes of visualization, the scale is forced to apply with full magnitude. During training, the scale is uniformly sampled from the specified range. For the final tree segmentation model, the range is set to \\([0.8, 1.2]\\).\n\n\n\n\n\n\n\n\n\nFigure 3.14: Visualization of the random scale augmentation on a single aspen tree. The effect is forced to happen with full amplitude for visualization purposes, during training a scale factor is uniformly sampled from a set range.\n\n\n\n\n\n\nAnother way to change the positions is to slightly translate each point for a random distance in a random direction. That transformation is commonly referred to as random jitter. Since LiDAR sensors have limited spatial accuracy, introducing random translations within the accuracy range should not have any effect on the result of tree segmentation. Going further, small translations even outside the accuracy range make sense, since they have very limited effect on the overall shape of the tree. Figure 3.15 shows this effect on a single aspen tree. The amount of translation is uniformly sampled independently for each point from a set range. Note that the random jitter augmentation is applied before the scale normalization. It matters because the augmentation’s only parameter is the translation range, which depends on the scale. The parameters on the figure are thus in the original coordinate units – meters. Note how the shape of the tree almost doesn’t change when the maximum range is set to 20 centimeters, and starts to become fuzzy and loose shape at 1 meter and higher. For the final tree segmentation model, the maximum translation is set to 30 centimeters.\n\n\n\n\n\n\n\n\n\nFigure 3.15: Visualization of the random jitter augmentation on a single aspen tree. The translation magnitude is uniformly sampled from a set range for every point.\n\n\n\n\n\n\nAnother useful effect augmentations can provide is to make synthetic data look more like real data. As was mentioned in Section 3.2, there is a selection bias in the dataset of individual trees: the trees that are easiest to manually separate are exponentially more likely to end up in the data. As such, the tree clouds in the individual tree dataset are significantly different from trees of the same species with similar sizes and shapes, but standing in dense clusters. One of the most important differences is that almost all the tree clouds have trunks, while in actual forest, dense canopy cover blocks most pulses and the trunk representation is very poor. One way to mitigate that issue is to apply a height-dependent dropout function to each tree cloud. For that purpose a probability threshold function that is around zero for the highest points of the tree and quickly ramps up to almost one for the lowest points is needed. An example function that satisfies these criteria is a modified sigmoid:\n\\[\n\\text{threshold}(z) = \\big[1 + e^{z \\times \\text{scale} + \\text{shift}}\\big]^{-1},\n\\]\nwhere \\(z\\) is the height normalized to \\([0, 1]\\) and reversed by subtraction from 1, \\(\\text{scale}\\) and \\(\\text{shift}\\) are hyperparameters that control the shape of the curve. Changing \\(\\text{scale}\\) controls how steep is the climb from 0 to 1: the larger, the steeper. Changing \\(\\text{shift}\\) controls the position of the climb: the larger, the lower. Figure 3.16 shows an example of applying such dropout function to a single aspen tree, and Figure 3.17 show the same tree with more aggressive parameters, resulting in much more points being dropped from the tree cloud.\n\n\n\n\n\n\n\n\n\nFigure 3.16: Effect of the height-dependent modified sigmoid dropout on a single aspen tree. Scale is set to 8, shift is set to 3. a) A single aspen tree with points colored by height. b) Height-dependent probability of dropout (a modified sigmoid). c) The same aspen with points colored by probability of dropout. d) The same aspen with points that will be dropped marked red. e) The same aspen after the dropout is appleid with point colored by height.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.17: Same as Figure 3.16, but with a more aggressive dropout threshold function. Scale is set to 18, shift is set to 2.5.\n\n\n\n\n\n\nAll the augmentation are applied on the fly right before the examples are loaded into GPU memory and passed through the network.\n\n\n3.4.3 Other training parameters\nMean absolute error is used as a loss function. Some experiments have shown improvements when using a custom loss function that modifies the mean absolute error loss by weighting it reversely proportional to the distance of the tree centroid. The idea of the modification is to make the network focus more on points closer to the center of each tree, as these points are much less likely to be overlapping with the crowns of adjacent trees.\nThe batch size is limited by the available GPU device memory, and in the described setup competes for memory space with the synthetic forest patch dimensions used in the training dataset. The preference is given to the size of the patch, so the batch size is set to 1, but is compensated for by using gradient accumulation steps – updates to the model parameters are made after the gradient is accumulated for a set number of iterations. This slows down the training, but enables usage of larger batches when there is not enough memory to fit them, making the training procedure overall more stable.\nEarly stopping (Prechelt 1998) is set up to terminate training early if there is no improvement in average validation loss in a set number of epochs. This makes sure that valuable GPU time is not wasted on continuing training models that are likely to have started overfitting.\nModel checkpointing is set up as well. After each training epoch, when the average validation loss and accuracy are calculated, the model state is saved to disk if it’s current accuracy is better than the last saved one, where accuracy is the proportion of points for which the rounded integer label is correct. This makes sure that the best performing model can always be recovered, even if the training process was run for too long and the latest model is not the best one.\nThe learning rate schedule is set to follow a fast linear warm up and slow linear decay. Learning rate is set to ramp up from 0.001 to 0.01 in a span of 2 epochs, and then decay back to 0.001 in a span of 30 epochs. Adam optimizer is used (Kingma and Ba 2014).\nTraining was performed on an NVIDIA A100 GPU with 80G of memory. Inference, depending on the point density of the input, might work even on a much smaller GPU like Tesla T4 with 16G of memory. GPUs of this size are available with limits on usage for free on services like Google Colab and Kaggle.\n\n\n3.4.4 On implementation of PointNet++\nAs mentioned in the introduction, the deep learning code, including the code for PointNet++, is implemented using the PyTorch Geometric library designed for writing and training graph neural networks. This makes the implementation not exactly the same as described in the PointNet papers. The main ideas, namely local feature learning in a k-nearest neighbors neighborhood and max pulling for permutation-invariant aggregation, are there. Input transform and feature transform networks are not defined explicitly, but networks that process features learn to perform a similar function.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Materials and methods</span>"
    ]
  },
  {
    "objectID": "chapters/03_materials_and_methods.html#sec-training-tree-processors",
    "href": "chapters/03_materials_and_methods.html#sec-training-tree-processors",
    "title": "3  Materials and methods",
    "section": "3.5 Training segmented trees processing models",
    "text": "3.5 Training segmented trees processing models\nTo predict per-tree forest attributes from a segmented point cloud, a set of specialized regression and classification models is trained on the tree clouds. These specialized models are classic machine learning models that operate on most common metrics described in the literature overview chapter. As mentioned in the literature overview, common features used for machine learning on point clouds are based on the eigenvalues of the covariance matrix of the point coordinates, calculated either for an entire cloud, or per-point in a neighborhood around it. These features include linearity, planarity, scatter, that aim to indicate the presence of linear, planar, or volumetric structures, and also omnivariance, anisotropy, eigentropy, the sum of eigenvalues, and curvature. The features are defined as follows, with eigenvalues sorted in descending order such that \\(\\lambda_1 \\ge \\lambda_2 \\ge \\lambda_3\\):\n\\[\n\\begin{aligned}\n\\text{linearity} &= \\frac{\\lambda_1 - \\lambda_2}{\\lambda_1} \\\\\n\\text{planarity} &= \\frac{\\lambda_2 - \\lambda_3}{\\lambda_1} \\\\\n\\text{scatter} &= \\frac{\\lambda_3}{\\lambda_1}  \\\\\n\\text{omnivariance} &= \\sqrt[3]{\\lambda_1\\lambda_2\\lambda_3} \\\\\n\\text{anisotropy} &= \\frac{\\lambda_1 - \\lambda_3}{\\lambda_1} \\\\\n\\text{eigentropy} &= -\\sum_{i=1}^{3} \\lambda_i \\ln(\\lambda_i) \\\\\n\\text{sum of eigenvalues} &= \\lambda_1 + \\lambda_2 + \\lambda_3 \\\\\n\\text{curvature} &= \\frac{\\lambda_3}{\\lambda_1 + \\lambda_2 + \\lambda_3} \\\\\n\\end{aligned}\n\\]\nAnother common set of features, especially popular in forestry applications, are various statistics that describe the height distribution of points within the neighborhood or the cloud. They include maximum and average height, standard deviation, kurtosis, skew, and entropy of the height distribution, percentage of points above the mean and each of the deciles of height and the deciles of height themselves. The features are calculated for the entire tree cloud, effectively reducing each tree to a collection of metrics, resulting in a tabular dataset. In this form, the dataset is used to train the models.\nIn Dubrovin and Fortin (2024), we propose a way to help build intuition into the meaning of some of the less obvious features by visualizing individual trees on a different end of the range of the feature’s values. Figure 3.18 shows is an example of such visualization, showing the effect of the shape of spruce on omnivariance and the effect of the shape of aspen on percent of points above mean height. Note the presence of ground points, which are filtered out before calculating features for the models.\n\n\n\n\n\n\n\nFigure 3.18: Visualizations of how values of commonly used features calculated for the whole tree cloud map to the shapes of individual trees. Top: Effect of the shape of spruce on omnivariance. Bottom: Effect of on the shape of aspen on percent of points higher than mean height. Figure reused from Dubrovin and Fortin (2024). Note the presence of ground points, which are filtered out before calculating features for the models.\n\n\n\nThe feature set is thinned by using sequential feature selection: a greedy approach that selects the best feature according to 5-fold cross-validation accuracy on every iteration until a set number of features is selected. The models are trained on 20 best features selected this way.\nTo make the trees more closely resemble the trees standing in dense clusters, the same height-dependent dropout function that is used for augmenting synthetic forests is used. This dropout, together with a small amount of random jitter, is also used to create additional samples for training the models, as the original dataset size is very small. Great care is taken to only create additional samples within the training set, to avoid any data leakage. Every training set sample is repeated 3 times with random jitter with maximum amplitude of 20 cm and a dropout applied.\nTwo models are used as proof of concept for the proposed framework: a classifier that predicts the tree species, and a regressor that estimates a tree’s diameter at breast height. Random Forest models are used in both cases. The number of trees is set to 200. For the classification model, class weights are assigned to every example that are inversely proportional to the class frequency in the data during model fitting.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Materials and methods</span>"
    ]
  },
  {
    "objectID": "chapters/03_materials_and_methods.html#sec-matching-algorithm",
    "href": "chapters/03_materials_and_methods.html#sec-matching-algorithm",
    "title": "3  Materials and methods",
    "section": "3.6 Matching detections to field inventory ground truth",
    "text": "3.6 Matching detections to field inventory ground truth\nTo evaluate the results of any tree detection system against field inventory data, an automated and deterministic method for matching detected tree candidates to the ground truth trees is needed. It should determine which ground truth trees, if any, the detected candidates correspond to. It should also classify both the trees and the candidates as either a true positive, meaning that the ground truth tree was successfully detected, a false negative, meaning that a ground truth tree was not detected, or a false positive, meaning a tree was detected when there is none.\nI use the same matching procedure that is described and implemented in Dubrovin, Fortin, and Kedrov (2024). It considers the locations and heights of the trees, and falls back to using only locations when the height is not available for the ground truth tree. It is parametrized by the maximum allowed 2D distance and the maximum height difference between a detected and a ground truth tree to consider them a match. First, it constructs a distance matrix between the ground truth tree locations and the detected candidates and filters out the pairs for which the distance is larger than the allowed maximum. It then iterates over the remaining pairs in the order of increasing distance, and marks the pairs with suitable heights in which both trees are not yet assigned a class as a true positive. The pairs in which one of the trees is already assigned a class are marked as either a false positive or a false negative, and the pairs in which both trees are assigned a class are skipped. Finally, it marks all unmatched ground truth trees as false negatives, and all unmatched candidate trees as false positives.\nBased on the results of the matching algorithm, a set of metrics that are often used to evaluate the results of tree detection can be calculated. The same metrics are usually used for evaluation in classification problems and thus are well-known. The first commonly used metric is recall, which is the proportion of the ground truth trees that were detected. The second metric is precision, which is the proportion of candidates that are correct. Both are important, and describe a detection system from different perspectives. To combine them into a single metric, \\(F_1\\)-score is often used, which is the harmonic mean of precision and recall that provide a balanced measure of the system performance. The formulas for the metrics are as follows:\n\\[\n\\text{recall} =\n\\frac{N_{\\text{true positives}}}{N_\\text{trees}} =\n\\frac{N_{\\text{true positives}}}{N_{\\text{true positives}} + N_{\\text{false negatives}}}\n\\]\n\\[\n\\text{precision} =\n\\frac{N_{\\text{true positives}}}{N_\\text{candidates}} =\n\\frac{N_{\\text{true positives}}}{N_{\\text{true positives}} + N_{\\text{false positive}}}\n\\]\n\\[\nF_1\\text{-score} = 2 \\times \\frac{\\text{precision} \\times \\text{recall}}{\\text{precision} + \\text{recall}}\n\\]\nAnother commonly used metric to evaluate the results of tree detection approaches is the average distance between a detected and a ground truth tree. This metric, however, has limitations that are important to keep in mind when analyzing the results. The first important limitation is that its value is limited in how low it can get by the difference in nature of the compared detections and ground truth field inventory. During the inventory, the trees’ coordinates are recorded at their trunks, approximately at breast height, which is almost at the ground level compared to the height of the trees. The detections, on the other hand, happen with the perspective from above, and correspond to tree tops. Planar distance between the bottom of the trunk and the top of the canopy can be significant, especially if trees are even slightly tilted, or for broadleaf species with wide and irregular canopies, or both. So the average distance between predicted trees and their ground truth counterparts will rarely be zero, even for a perfect detection system. The other limitation it that that distance is also limited from above by the parameters used for the matching algorithm. As mentioned, one of the parameters is the maximum distance allowed between a candidate and an actual tree to consider them a match. Thus, the average distance metric is limited from both directions, but in ways that are not immediately obvious. I do calculate and report it in the results section, but the reader is advised to keep the limitations in mind.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Materials and methods</span>"
    ]
  },
  {
    "objectID": "chapters/03_materials_and_methods.html#application-of-the-framework",
    "href": "chapters/03_materials_and_methods.html#application-of-the-framework",
    "title": "3  Materials and methods",
    "section": "3.7 Application of the framework",
    "text": "3.7 Application of the framework\nThe framework is applied in two steps. First, the tree segmentation network is applied in a sliding window of the same size that was used for patch generation during its training, with overlap to be able to combine the results into a single prediction. The continuous predictions of the regression model are rounded to get the integer labels for every point. The results are merged to create a single point cloud, and a postprocessing routine aimed to transform per-window tree IDs to global tree IDs is run. It traces the overlapping points to find the labels that need to be updated in the later window, and adds a large number to predictions with remaining labels in the later window. Second, the segments identified by predicted integer labels are extracted, selected feature sets are calculated for each one, and each one is passed through the collection of attribute prediction models to get per-tree predictions.\n\n\n\n\nBishop, Christopher. 2006. Pattern Recognition and Machine Learning. Springer. https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/.\n\n\nDeng, Li. 2012. “The Mnist Database of Handwritten Digit Images for Machine Learning Research.” IEEE Signal Processing Magazine 29 (6): 141–42.\n\n\nDubrovin, Ivan, and Clement Fortin. 2024. “An Exploration of Properties of Point Clouds of Individual Trees Extracted From a Larger UAV Lidar Survey.” In IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium, 4503–6. Athens, Greece: IEEE. https://doi.org/10.1109/IGARSS53475.2024.10641061.\n\n\nDubrovin, Ivan, Clement Fortin, and Alexander Kedrov. 2024. “An Open Dataset for Individual Tree Detection in UAV LiDAR Point Clouds and RGB Orthophotos in Dense Mixed Forests.” Scientific Reports 14 (1): 21938. https://doi.org/10.1038/s41598-024-72669-5.\n\n\nEysn, Lothar, Markus Hollaus, Eva Lindberg, Frédéric Berger, Jean-Matthieu Monnet, Michele Dalponte, Milan Kobal, et al. 2015. “A Benchmark of Lidar-Based Single Tree Detection Methods Using Heterogeneous Forest Data from the Alpine Space.” Forests 6 (5): 1721–47. https://doi.org/10.3390/f6051721.\n\n\nFarr, Tom G., and Mike Kobrick. 2000. “Shuttle Radar Topography Mission Produces a Wealth of Data.” Eos, Transactions American Geophysical Union 81 (48): 583–85. https://doi.org/10.1029/EO081i048p00583.\n\n\nGraves, Sarah, and Sergio Marconi. 2020. “IDTReeS 2020 Competition Data.” Zenodo. https://doi.org/10.5281/ZENODO.3934932.\n\n\nKingma, Diederik P., and Jimmy Ba. 2014. “Adam: A Method for Stochastic Optimization.” arXiv. https://doi.org/10.48550/ARXIV.1412.6980.\n\n\nMumuni, Alhassan, and Fuseini Mumuni. 2022. “Data Augmentation: A Comprehensive Survey of Modern Approaches.” Array 16 (December): 100258. https://doi.org/10.1016/j.array.2022.100258.\n\n\nPrechelt, Lutz. 1998. “Automatic Early Stopping Using Cross Validation: Quantifying the Criteria.” Neural Networks 11 (4): 761–67. https://doi.org/10.1016/S0893-6080(98)00010-0.\n\n\nQi, Charles R., Li Yi, Hao Su, and Leonidas J. Guibas. 2017. “PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space.” In Advances in Neural Information Processing Systems. Vol. 30. Curran Associates, Inc.\n\n\nShi, Yifang, Tiejun Wang, Andrew K. Skidmore, and Marco Heurich. 2018. “Important LiDAR Metrics for Discriminating Forest Tree Species in Central Europe.” ISPRS Journal of Photogrammetry and Remote Sensing 137 (March): 163–74. https://doi.org/10.1016/j.isprsjprs.2018.02.002.\n\n\nWeinstein, Ben, Sergio Marconi, and Ethan White. 2022. “Data for the NeonTreeEvaluation Benchmark.” Zenodo. https://doi.org/10.5281/ZENODO.5914554.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Materials and methods</span>"
    ]
  },
  {
    "objectID": "chapters/04_results.html",
    "href": "chapters/04_results.html",
    "title": "4  Results",
    "section": "",
    "text": "4.1 Tree segmentation network training results\nThe results of training a tree segmentation PointNet++ are challenging to evaluate on their own because there are not inherently good metrics that would put the performance into easily understandable numbers. As was mentioned in Section 3.4.3, during training model checkpointing is set up to track average validation accuracy – proportion of points for which the final predicted label is correct. However, the predictions are quite noisy, resulting in low accuracies even for results that are not bad. Figure 4.1 shows the prediction on a sample from the validation dataset used to monitor the training process, and Figure 4.2 shows the same prediction in 3D. The accuracy of the model on the figures is 33.6%, and it is around the best accuracy of all the experiments I ran, excluding ones with simpler data.\nOverall, the final model was selected less based on raw numbers such as maximum average validation accuracy or minimum average validation loss, but by expecting the visualizations of the predictions on a subset of samples from the validation dataset.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/04_results.html#tree-segmentation-network-training-results",
    "href": "chapters/04_results.html#tree-segmentation-network-training-results",
    "title": "4  Results",
    "section": "",
    "text": "Figure 4.1: A top-down view of an example prediction of the tree segmentation PointNet++ on a sample from the validation dataset. Left: Points colored by label – tree ID within the patch. Middle: Points colored by the raw prediction of the network (the regression head outputs a continuous prediction for every point). Right: Points colored by the rounded prediction – converted from the continuous to the final integer tree ID.\n\n\n\n\n\n\n\n\n\nFigure 4.2: A 3D view of an example prediction of the tree segmentation PointNet++ on a sample from the validation dataset. Left: Points colored by label – tree ID within the patch. Middle: Points colored by the raw prediction of the network (the regression head outputs a continuous prediction for every point). Right: Points colored by the rounded prediction – converted from the continuous to the final integer tree ID.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/04_results.html#attribute-prediction-model-training-results",
    "href": "chapters/04_results.html#attribute-prediction-model-training-results",
    "title": "4  Results",
    "section": "4.2 Attribute prediction model training results",
    "text": "4.2 Attribute prediction model training results\nTo evaluate the performance of the attribute prediction models, 10-fold stratified cross validation was used, as well as a separate 40% hold-out set. For the tree species classification model, the stratification was performed using the labels directly. For the diameter at breast height regression model, the stratification was performed on the label split into 5 equal-width buckets across the range.\n\n4.2.1 Tree species classification\nThe tree species classification model was evaluated using accuracy and macro \\(F_1\\)-score (an average of \\(F_1\\)-scores across all the classes). Figure 4.3 shows out-of-fold metrics across all ten folds of cross-validation. The average accuracy is 0.71, with a standard deviation of 0.06. The average macro \\(F_1\\)-score is 0.70, with a standard deviation of 0.07. The model is overall relatively consistent across all ten folds, showing that the cross-validation metrics estimates are reliable.\n\n\n\n\n\n\n\n\n\nFigure 4.3: 10-fold cross-validation results of the species classification Random Forest model. The average accuracy is 0.71 with a standard deviation of 0.06. The average macro F-score is 0.70 with a standard deviation of 0.07. The model is overall stable across all of the folds.\n\n\n\n\n\n\nThe holdout set was used to take a more in-depth look at the classification results. Figure 4.4 shows the confusion matrix for the holdout set, with values normalized by row. The set is relatively small – 158 examples, but enough to see some patterns emerge in the predictions. The best overall species is spruce, and all coniferous species are better than all the deciduous species. The model often confuses deciduous species, classifying alder as birch or tilia, aspen as birch, tilia as alder or birch. This behavior is to be expected, as the shapes of deciduous species are very similar to each other and in complex in general. The overall accuracy in the holdout set is 61%.\n\n\n\n\n\n\n\n\n\nFigure 4.4: Confusion matrix for the tree species classification Random Forest. Calulated on the 40% holdout set of 158 examples. Values normalized by row. The model confuses deciduous species a lot more than coniferous ones.\n\n\n\n\n\n\n\n\n4.2.2 Diameter at breast height regression\nDiameter at breast height regression model was evaluated using root mean squared error (RMSE), mean absolute error (MAE), and coefficient of determination \\(R^2\\). Formulas for the metrics are provided below, with \\(y_i\\) representing the true dbh value, \\(\\hat y_i\\) – the corresponding prediction, and \\(\\overline y\\) – the average dbh. The dataset here is smaller than the one for tree species classification because diameter at breast height is only available for trees from the Lysva field inventory, while species are available for all trees in the individual tree point cloud dataset, some of which are from other surveys in the region.\n\\[\n\\begin{aligned}\n\\text{RMSE} &= \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat y_i)^2} \\\\\n\\text{MAE} &= \\frac{1}{N} \\sum_{i=1}^N | y_i - \\hat y_i | \\\\\nR^2 &= 1 - \\frac{\\sum_{i=1}^N (y_i - \\hat y_i)^2}{\\sum_{i=1}^N (y_i - \\overline y)^2}\n\\end{aligned}\n\\]\nFigure 4.5 shows out-of-fold metrics across all ten folds of cross-validation. Both the mean squared error and the mean absolute error are stable across the folds, with average RMSE 4.55 centimeters with a standard deviation of 0.19 and average MAE 3.49 centimeters with a standard deviation of 0.38. Coefficient of determination \\(R^2\\) is less stable. Even though most folds have it around 0.6 to 0.8, there are two folds where it drops to below 0.2 and even close to 0, meaning that the model there is no better than predicting the mean dbh for every input. This drops the average \\(R^2\\) to 0.53 with a standard deviation of 0.24.\n\n\n\n\n\n\n\n\n\nFigure 4.5: 10-fold cross-validation results of the dbh regression Random Forest model. The average RMSE is 4.55 centimeters with a standard deviation of 0.19. The average MAE is 3.49 centimeters with a standard deviation of 0.38. The average coefficient of determination \\(R^2\\) is 0.53 with a standard deviation of 0.24. The model is relatively stable in terms of the errors, but the coefficient of determination is all over the place.\n\n\n\n\n\n\nSimilar to the classification model, the holdout set was used to take a more in-depth look at the regression results. Here the holdout set is even smaller than for regression – 68 samples. Figure 4.6 offers a collection of diagnostic visualizations aimed to help better understand the regression performance. On the top left is the residual plot, where the differences between the actual dbh and the predicted dbh are plotted against the actual dbh values. Ideally, the points should be around a horizontal line at zero, shown in black, without any patterns. This would mean that the size of the error does not depend on the value of dbh. The resulting dbh regression model shows a trend, which means it tends to overestimate low dbh values and underestimate high dbh values. A related plot is on the top right, showing the distribution of the residuals across the holdout set. It should be centered around zero and as narrow as possible. On the bottom left is a scatter plot of predicted dbh against the actual dbh. Ideally, this should be a 45-degree line, shown in black. Because of the underestimation on the high end and overestimation in the low end, the points seem to lie on a line with a smaller slope. Although most points are in the middle, where the line is followed a lot more closely. And, finally, a related plot on the bottom right, comparing distributions of the actual dbh and predicted dbh in the holdout set. The same lack of extreme can be observed. The final holdout set RMSE is 5.76 cm, MAE is 4.11, \\(R^2\\) is 0.52.\n\n\n\n\n\n\n\n\n\nFigure 4.6: Quality assessment plots for the diameter at breast height regression Random Forest. Calculated on a 40% holdout set of 68 examples. Top Left: Residual scatter plot: difference between the prediction and the true value vs. the true value. Ideally the spread should be equal across the valu range. Bottom Left: Predicted vs. actual scatter plot. Ideally should be as close to a line as possible. Top right: Distribution of the residuals. Bottom right: Distribution of the predicted and actual values of dbh.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/04_results.html#validation-on-the-lysva-field-inventory-data",
    "href": "chapters/04_results.html#validation-on-the-lysva-field-inventory-data",
    "title": "4  Results",
    "section": "4.3 Validation on the Lysva field inventory data",
    "text": "4.3 Validation on the Lysva field inventory data\nTo validate the framework, the results were evaluated on the Lysva field inventory dataset, described in Section 3.1. The mean X and Y coordinates of each segmented tree were calculated to convert the cloud into a set of points, where each point represents a candidate detected tree. The maximum height of the points in the cluster was used as the height of the detected tree. Then the algorithm described in Section 3.6 was applied to match the detected trees their corresponding the ground truth trees, if any. The matching algorithm was run with maximum distance between a detected tree candidate and a ground truth tree of 5 meters and maximum height difference of 3 meters (ignored for ground truth trees that don’t have measured height). Then, precision, recall, and \\(F_1\\)-score were calculated for the detection results, and for the true positive matches the average distance between the detected tree and the actual tree, species classification accuracy and macro \\(F_1\\)-score, and diameter at breast height errors are calculated.\nTable 4.1 shows the tree detection metrics. When interpreting the metrics, the reader is advised to keep in mind the limitations of the distance metric mentioned in Section 3.6, and instead focus on the \\(F_1\\)-score as the most informative metric. The overall average \\(F_1\\)-score for tree detection by the described system is 0.63.\n\n\n\nTable 4.1: Results of the tree detection across all plots in the Lysva field survey.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot\nTree count\nDominant type\nPoint density\nRecall\nPrecision\nF1-score\nDistance\n\n\n\n\n1\n420\nDeciduous\n31.7\n0.83\n0.63\n0.72\n0.77\n\n\n2\n365\nDeciduous\n47.9\n0.61\n0.72\n0.66\n0.81\n\n\n3\n332\nDeciduous\n40.3\n0.63\n0.49\n0.56\n0.85\n\n\n4\n261\nConiferous\n33.5\n0.85\n0.50\n0.63\n1.06\n\n\n5\n208\nConiferous\n14.2\n0.68\n0.55\n0.61\n0.97\n\n\n6\n290\nConiferous\n39.1\n0.80\n0.52\n0.63\n0.83\n\n\n7\n408\nDeciduous\n41.9\n0.65\n0.62\n0.63\n0.85\n\n\n8\n341\nConiferous\n35.5\n0.87\n0.57\n0.69\n0.93\n\n\n9\n459\nConiferous\n42.1\n0.56\n0.65\n0.60\n0.92\n\n\n10\n518\nDeciduous\n42.9\n0.42\n0.92\n0.58\n0.99\n\n\n\n\n\nAverage:\n0.69\n0.62\n0.63\n0.90\n\n\n\n\n\n\nFigure 4.7 shows the confusion matrix for species prediction calculated for true positive matches. Note that willow trees are not included, as the model has not seen any in the training data, and all pine predictions are counted as spruce instead. The pattern of the confusion matrix is similar to the evaluation on the holdout set. The overall accuracy is 66.1%. The overall macro \\(F_1\\)-score is 55.4%.\n\n\n\n\n\n\nFigure 4.7: Confusion matrix for the tree species classification on true positive detected trees in the Lysva field inventory plots.\n\n\n\nFigure 4.8 show the quality assessment plots for the results of diameter at breast height regression. It’s obvious from the plots that the individual tree dataset is not representative of the entire Lysva dataset in terms of dbh, as the range of dbh in there is not wide enough. The distributions show that the individual trees dataset has higher mean dbh values than the overall distribution, which makes the model eager to overestimate a lot. This effect is no doubt connected to the sampling bias mentioned in the description of the individual trees dataset: trees were selected by humans based on how easy they are to extract from a large point cloud. As the result, the predictions outside the range seen by the Random Forest are constant. Within the covered range, however, the behavior is similar to what was observed on the holdout set: a trend in the residual plot indicating over- and underestimation of low and high values, and errors close to zero near the middle of the range. The overall root mean squared error is 5.37 centimeters. The overall mean absolute error is 4.21 centimeters. The overall coefficient of determination \\(R^2\\) is 0.65.\n\n\n\n\n\n\nFigure 4.8: Regression quality assessment plots plots for the diameter at breast height regression on true positive detected trees in the Lysva field inventory plots.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "chapters/05_conclusion.html",
    "href": "chapters/05_conclusion.html",
    "title": "5  Conclusion",
    "section": "",
    "text": "5.1 Potential further improvements\nI see numerous ways to potentially improve the results of the proposed system. Some of the most important are as follows:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/05_conclusion.html#potential-further-improvements",
    "href": "chapters/05_conclusion.html#potential-further-improvements",
    "title": "5  Conclusion",
    "section": "",
    "text": "Use of more advanced tree segmentation network architectures. As mentioned, the PointNet++ is a pioneering model in the field of deep learning on 3D point clouds that was chosen because it is relatively easy to implement and work with. It is still competitive in many tasks, but the field of deep learning is evolving rapidly, and many more powerful architectures were suggested and proven since its release. The framework will definitely benefit from a better, more efficient tree segmentation network.\nUse of more advanced feature extraction for RGB orthophoto-based features. For the same reason the PointNet++ was chosen as a baseline for tree segmentation, a very basic approach was used to extract features from RGB orthophotos. Despite being better than just RGB colors, these features still offer very limited representation power. Instead, convolutional neural networks specialized in creating good representations for downstream tasks can be used.\nUse of open-access datasets for pretraining. At its current state, the tree segmentation network learns to perform a very hard task from scratch on a dataset of a very limited size. I believe it can benefit from first training on a similar, but larger dataset, for example, the NeonTreeEvaluation Benchmark (Weinstein, Marconi, and White 2022). It is briefly described in the comparison subsection of the Lysva field inventory section. The bounding boxes from the orthophoto can be used to label the point cloud for tree segmentation. The labels will be noisy, but pretraining on a large amount of noisy data often shows good results.\nUse of more sophisticated techniques for creating synthetic forest patches. Both the sampling of the trees and the placement onto the patch can be improved. For example, sampling can take into consideration the species of the trees, to make the patches more realistic: some fully coniferous, some fully deciduous, some mixed. Placing of the trees can use packing algorithms, or try to mimic the spatial distribution of trees in the inventory.\nUse of more sophisticated augmentations to make the synthetic forest patches look as close as possible to real forest patches. For example, the height-depended dropout can be improved to take into account canopy density and overlap. A shorter tree almost completely covered from above by a larger tree should have fewer points overall, not just at low heights relative to its highest point.\nUse of a more careful approach to training attribute prediction models. The results of the second step can be further improved by adopting a more careful approach to modeling, including a more careful approach to feature engineering and model selection.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/05_conclusion.html#concluding-thoughts",
    "href": "chapters/05_conclusion.html#concluding-thoughts",
    "title": "5  Conclusion",
    "section": "5.2 Concluding thoughts",
    "text": "5.2 Concluding thoughts\nThe quote at the beginning of a chapter is relevant for a few reasons, most important of which is that there are so many things I still would like to try and improve. But I don’t have any more time. I wish I spent the time I had more wisely, but it is a well established fact that hindsight is 20/20, and what I see as wise now probably wasn’t so obviously wise in the process.\nThe thesis evolved with both my understanding of current problems and the needs and point of view of the industrial partners, with whose collaboration the work was conducted. I wanted the work to be practical, which explains many choices made – most notably, the data sources.\nThis was a journey. Thank you for reading. Hope you are safe.\nI would also love to include a rant about the awful state of so many things in the world including science and scientific publishing and high-tech industry and overwhelming widespread hypocrisy and how capitalism must die for us as humanity to have a chance of survival. I won’t though because it’s not the place.\n\n\n\n\nDubrovin, Ivan, and Clement Fortin. 2024. “An Exploration of Properties of Point Clouds of Individual Trees Extracted From a Larger UAV Lidar Survey.” In IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium, 4503–6. Athens, Greece: IEEE. https://doi.org/10.1109/IGARSS53475.2024.10641061.\n\n\nDubrovin, Ivan, Clement Fortin, and Alexander Kedrov. 2024. “An Open Dataset for Individual Tree Detection in UAV LiDAR Point Clouds and RGB Orthophotos in Dense Mixed Forests.” Scientific Reports 14 (1): 21938. https://doi.org/10.1038/s41598-024-72669-5.\n\n\nWeinstein, Ben, Sergio Marconi, and Ethan White. 2022. “Data for the NeonTreeEvaluation Benchmark.” Zenodo. https://doi.org/10.5281/ZENODO.5914554.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/05_conclusion.html#footnotes",
    "href": "chapters/05_conclusion.html#footnotes",
    "title": "5  Conclusion",
    "section": "",
    "text": "I am purposefully avoiding the term “artificial intelligence” as it seems to have been abused so much as to become practically meaningless, and thus not useful in technical context.↩︎",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Conclusion</span>"
    ]
  },
  {
    "objectID": "chapters/99_references.html",
    "href": "chapters/99_references.html",
    "title": "References",
    "section": "",
    "text": "Aerts, Raf, and Olivier Honnay. 2011. “Forest Restoration,\nBiodiversity and Ecosystem Functioning.” BMC Ecology 11\n(1): 29. https://doi.org/10.1186/1472-6785-11-29.\n\n\nAllaire, J. J., Charles Teague, Carlos Scheidegger, Yihui Xie, and\nChristophe Dervieux. 2024. “Quarto.” https://doi.org/10.5281/zenodo.5960048.\n\n\nAllen, Matthew J., Stuart W. D. Grieve, Harry J. F. Owen, and Emily R.\nLines. 2022. “Tree Species Classification from Complex Laser\nScanning Data in Mediterranean Forests Using Deep\nLearning.” Methods in Ecology and Evolution, September.\nhttps://doi.org/10.1111/2041-210X.13981.\n\n\nAnsel, Jason, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain,\nMichael Voznesensky, Bin Bao, et al. 2024. “PyTorch 2: Faster Machine Learning Through Dynamic Python\nBytecode Transformation and Graph Compilation.” In\n29th ACM International Conference on Architectural Support for\nProgramming Languages and Operating Systems, Volume 2 (ASPLOS ’24).\nACM. https://doi.org/10.1145/3620665.3640366.\n\n\nArenas-Corraliza, Isabel, Ana Nieto, and Gerardo Moreno. 2020.\n“Automatic Mapping of Tree Crowns in Scattered-Tree Woodlands\nUsing Low-Density LiDAR Data and Infrared Imagery.”\nAgroforestry Systems 94 (5): 1989–2002. https://doi.org/10.1007/s10457-020-00517-2.\n\n\nAubry-Kientz, Mélaine, Anthony Laybros, Ben Weinstein, James G. C. Ball,\nToby Jackson, David Coomes, and Grégoire Vincent. 2021.\n“Multisensor Data Fusion for Improved\nSegmentation of Individual Tree Crowns in\nDense Tropical Forests.” IEEE Journal of\nSelected Topics in Applied Earth Observations and Remote Sensing\n14: 3927–36. https://doi.org/10.1109/JSTARS.2021.3069159.\n\n\nBalestra, Mattia, Suzanne Marselis, Temuulen Tsagaan Sankey, Carlos\nCabo, Xinlian Liang, Martin Mokroš, Xi Peng, et al. 2024.\n“LiDAR Data Fusion to Improve Forest Attribute\nEstimates: A Review.” Current Forestry\nReports 10 (4): 281–97. https://doi.org/10.1007/s40725-024-00223-7.\n\n\nBello, Saifullahi Aminu, Shangshu Yu, Cheng Wang, Jibril Muhmmad Adam,\nand Jonathan Li. 2020. “Review: Deep Learning on\n3D Point Clouds.” Remote Sensing 12 (11):\n1729. https://doi.org/10.3390/rs12111729.\n\n\nBishop, Christopher. 2006. Pattern Recognition and\nMachine Learning. Springer. https://www.microsoft.com/en-us/research/publication/pattern-recognition-machine-learning/.\n\n\nBouvier, Marc, Sylvie Durrieu, Richard A. Fournier, and Jean-Pierre\nRenaud. 2015. “Generalizing Predictive Models of Forest Inventory\nAttributes Using an Area-Based Approach with Airborne LiDAR\nData.” Remote Sensing of Environment 156 (January):\n322–34. https://doi.org/10.1016/j.rse.2014.10.004.\n\n\nBurley, Jeffery, John Youngquist, and Julian Evans, eds. 2004.\nEncyclopedia of Forest Sciences. 1st ed. Oxford: Elsevier.\n\n\nBurt, Andrew, Mathias Disney, and Kim Calders. 2018. “Extracting\nIndividual Trees from Lidar Point Clouds Using\nTreeseg.” Edited by Sarah Goslee.\nMethods in Ecology and Evolution, December, 2041–210X.13121. https://doi.org/10.1111/2041-210X.13121.\n\n\nCarson, Ward W, Hans-Erik Andersen, Stephen E Reutebuch, and Robert J\nMcGaughey. 2004. “LiDAR Applications in\nForestry—An Overview.” In ASPRS Annual\nConference Proceedings, 4.\n\n\nDeng, Li. 2012. “The Mnist Database of Handwritten Digit Images\nfor Machine Learning Research.” IEEE Signal Processing\nMagazine 29 (6): 141–42.\n\n\nDouss, Rim, and Imed Riadh Farah. 2022. “Extraction of Individual\nTrees Based on Canopy Height Model to Monitor the State of\nthe Forest.” Trees, Forests and People 8 (June): 100257.\nhttps://doi.org/10.1016/j.tfp.2022.100257.\n\n\nDubrovin, Ivan, and Clement Fortin. 2024. “An\nExploration of Properties of Point\nClouds of Individual Trees Extracted From a\nLarger UAV Lidar Survey.” In IGARSS\n2024 - 2024 IEEE International Geoscience and Remote\nSensing Symposium, 4503–6. Athens, Greece: IEEE. https://doi.org/10.1109/IGARSS53475.2024.10641061.\n\n\nDubrovin, Ivan, Clement Fortin, and Alexander Kedrov. 2024. “An\nOpen Dataset for Individual Tree Detection in UAV LiDAR\nPoint Clouds and RGB Orthophotos in Dense Mixed\nForests.” Scientific Reports 14 (1): 21938. https://doi.org/10.1038/s41598-024-72669-5.\n\n\nEysn, Lothar, Markus Hollaus, Eva Lindberg, Frédéric Berger,\nJean-Matthieu Monnet, Michele Dalponte, Milan Kobal, et al. 2015.\n“A Benchmark of Lidar-Based Single Tree\nDetection Methods Using Heterogeneous Forest Data from the\nAlpine Space.” Forests 6 (5): 1721–47. https://doi.org/10.3390/f6051721.\n\n\nFahey, Timothy J, Peter B Woodbury, John J Battles, Christine L Goodale,\nSteven P Hamburg, Scott V Ollinger, and Christopher W Woodall. 2010.\n“Forest Carbon Storage: Ecology, Management, and Policy.”\nFrontiers in Ecology and the Environment 8 (5): 245–52. https://doi.org/10.1890/080169.\n\n\nFalcon, William, and The PyTorch Lightning team. 2019.\n“PyTorch Lightning.” https://doi.org/10.5281/zenodo.3828935.\n\n\nFarr, Tom G., and Mike Kobrick. 2000. “Shuttle Radar Topography\nMission Produces a Wealth of Data.” Eos, Transactions\nAmerican Geophysical Union 81 (48): 583–85. https://doi.org/10.1029/EO081i048p00583.\n\n\nFerrari, Felipe, Matheus Pinheiro Ferreira, Cláudio Aparecido Almeida,\nand Raul Queiroz Feitosa. 2023. “Fusing Sentinel-1\nand Sentinel-2 Images for Deforestation\nDetection in the Brazilian Amazon Under Diverse Cloud\nConditions.” IEEE Geoscience and Remote Sensing\nLetters 20: 1–5. https://doi.org/10.1109/LGRS.2023.3242430.\n\n\nFerreira, Matheus Pinheiro, Daniel Rodrigues Dos Santos, Felipe Ferrari,\nLuiz Carlos Teixeira Coelho Filho, Gabriela Barbosa Martins, and Raul\nQueiroz Feitosa. 2024. “Improving Urban Tree Species\nClassification by Deep-Learning Based Fusion of Digital Aerial Images\nand LiDAR.” Urban Forestry & Urban\nGreening 94 (April): 128240. https://doi.org/10.1016/j.ufug.2024.128240.\n\n\nFey, Matthias, and Jan Eric Lenssen. 2019. “Fast Graph Representation Learning with PyTorch\nGeometric.” https://github.com/pyg-team/pytorch_geometric.\n\n\nForster, Piers M., Chris Smith, Tristram Walsh, William F. Lamb, Robin\nLamboll, Bradley Hall, Mathias Hauser, et al. 2024. “Indicators of\nGlobal Climate Change 2023: Annual Update of Key Indicators\nof the State of the Climate System and Human Influence.”\nEarth System Science Data 16 (6): 2625–58. https://doi.org/10.5194/essd-16-2625-2024.\n\n\nFu, Yuwen, Yifang Niu, Li Wang, and Wang Li. 2024.\n“Individual-Tree Segmentation from\nUAV–LiDAR Data Using a Region-Growing\nSegmentation and Supervoxel-Weighted Fuzzy Clustering\nApproach.” Remote Sensing 16 (4): 608. https://doi.org/10.3390/rs16040608.\n\n\nGillies, Sean et al. 2013/. “Rasterio: Geospatial Raster\nI/O for Python\nProgrammers.” Mapbox. https://github.com/rasterio/rasterio.\n\n\nGlobal Forest Resources Assessment. 2020. FAO. https://doi.org/10.4060/ca9825en.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep\nLearning. Adaptive Computation and Machine Learning. Cambridge,\nMassachusetts: The MIT Press.\n\n\nGoyal, Ankit, Hei Law, Bowei Liu, Alejandro Newell, and Jia Deng. 2021.\n“Revisiting Point Cloud Shape Classification with a\nSimple and Effective Baseline.” In\nProceedings of the 38th International Conference on\nMachine Learning, 3809–20. PMLR. https://proceedings.mlr.press/v139/goyal21a.html.\n\n\nGraves, Sarah, and Sergio Marconi. 2020. “IDTReeS\n2020 Competition Data.” Zenodo. https://doi.org/10.5281/ZENODO.3934932.\n\n\nGuo, Li, Nesrine Chehata, Clément Mallet, and Samia Boukir. 2011.\n“Relevance of Airborne Lidar and Multispectral Image Data for\nUrban Scene Classification Using Random Forests.”\nISPRS Journal of Photogrammetry and Remote Sensing 66 (1):\n56–66. https://doi.org/10.1016/j.isprsjprs.2010.08.007.\n\n\nGuo, Yulan, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed\nBennamoun. 2021. “Deep Learning for 3D Point\nClouds: A Survey.” IEEE Transactions on\nPattern Analysis and Machine Intelligence 43 (12): 4338–64. https://doi.org/10.1109/TPAMI.2020.3005434.\n\n\nHansen, Johannes N., Edward T. A. Mitchard, and Stuart King. 2020.\n“Assessing Forest/Non-Forest Separability Using\nSentinel-1 C-Band Synthetic Aperture Radar.” Remote\nSensing 12 (11): 1899. https://doi.org/10.3390/rs12111899.\n\n\nHansen, M. C., P. V. Potapov, R. Moore, M. Hancher, S. A. Turubanova, A.\nTyukavina, D. Thau, et al. 2013. “High-Resolution Global\nMaps of 21st-Century Forest Cover Change.”\nScience 342 (6160): 850–53. https://doi.org/10.1126/science.1244693.\n\n\nHarris, Charles R., K. Jarrod Millman, Stéfan J van der Walt, Ralf\nGommers, Pauli Virtanen, David Cournapeau, Eric Wieser, et al. 2020.\n“Array Programming with NumPy.”\nNature 585: 357–62. https://doi.org/10.1038/s41586-020-2649-2.\n\n\nHe, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.\n“Deep Residual Learning for Image\nRecognition.” In Proceedings of the IEEE\nConference on Computer Vision and Pattern\nRecognition, 770–78. https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html.\n\n\nHunter, John D. 2007. “Matplotlib: A 2D Graphics\nEnvironment.” Computing in Science & Engineering 9\n(3): 90–95. https://doi.org/10.1109/MCSE.2007.55.\n\n\nJakubowski, Marek K., Qinghua Guo, and Maggi Kelly. 2013.\n“Tradeoffs Between Lidar Pulse Density and Forest Measurement\nAccuracy.” Remote Sensing of Environment 130 (March):\n245–53. https://doi.org/10.1016/j.rse.2012.11.024.\n\n\nKc, Yam Bahadur, Qijing Liu, Pradip Saud, Damodar Gaire, and Hari\nAdhikari. 2024. “Estimation of Above-Ground Forest\nBiomass in Nepal by the Use of\nAirborne LiDAR, and Forest Inventory\nData.” Land 13 (2): 213. https://doi.org/10.3390/land13020213.\n\n\nKellogg, Kent, Pamela Hoffman, Shaun Standley, Scott Shaffer, Paul\nRosen, Wendy Edelstein, Charles Dunn, et al. 2020.\n“NASA-ISRO Synthetic Aperture Radar\n(NISAR) Mission.” In 2020\nIEEE Aerospace Conference, 1–21. Big Sky, MT, USA:\nIEEE. https://doi.org/10.1109/AERO47225.2020.9172638.\n\n\nKingma, Diederik P., and Jimmy Ba. 2014. “Adam: A\nMethod for Stochastic Optimization.” arXiv.\nhttps://doi.org/10.48550/ARXIV.1412.6980.\n\n\nKirillov, Alexander, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe\nRolland, Laura Gustafson, Tete Xiao, et al. 2023. “Segment\nAnything.” In Proceedings of the\nIEEE/CVF International Conference on\nComputer Vision (ICCV), 4015–26.\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.\n\n\nLa, Hien Phu, Yang Dam Eo, Anjin Chang, and Changjae Kim. 2015.\n“Extraction of Individual Tree Crown Using Hyperspectral Image and\nLiDAR Data.” KSCE Journal of Civil\nEngineering 19 (4): 1078–87. https://doi.org/10.1007/s12205-013-1178-z.\n\n\nLa Rosa, Laura Elena Cué, Camile Sothe, Raul Queiroz Feitosa, Cláudia\nMaria De Almeida, Marcos Benedito Schimalski, and Dário Augusto Borges\nOliveira. 2021. “Multi-Task Fully Convolutional Network for Tree\nSpecies Mapping in Dense Forests Using Small Training Hyperspectral\nData.” ISPRS Journal of Photogrammetry and Remote\nSensing 179 (September): 35–49. https://doi.org/10.1016/j.isprsjprs.2021.07.001.\n\n\nLassalle, Guillaume, Matheus Pinheiro Ferreira, Laura Elena Cué La Rosa,\nand Carlos Roberto de Souza Filho. 2022. “Deep Learning-Based\nIndividual Tree Crown Delineation in Mangrove Forests Using\nVery-High-Resolution Satellite Imagery.” ISPRS Journal of\nPhotogrammetry and Remote Sensing 189 (July): 220–35. https://doi.org/10.1016/j.isprsjprs.2022.05.002.\n\n\nLi, Qian, Baoxin Hu, Jiali Shang, and Hui Li. 2023. “Fusion\nApproaches to Individual Tree Species Classification\nUsing Multisource Remote Sensing Data.” Forests\n14 (7): 1392. https://doi.org/10.3390/f14071392.\n\n\nLi, Wenkai, Qinghua Guo, Marek K. Jakubowski, and Maggi Kelly. 2012.\n“A New Method for Segmenting Individual\nTrees from the Lidar Point Cloud.”\nPhotogrammetric Engineering & Remote Sensing 78 (1): 75–84.\nhttps://doi.org/10.14358/PERS.78.1.75.\n\n\nLian, Xugang, Hailang Zhang, Wu Xiao, Yunping Lei, Linlin Ge, Kai Qin,\nYuanwen He, et al. 2022. “Biomass Calculations of\nIndividual Trees Based on Unmanned Aerial Vehicle\nMultispectral Imagery and Laser Scanning Combined\nwith Terrestrial Laser Scanning in Complex\nStands.” Remote Sensing 14 (19): 4715. https://doi.org/10.3390/rs14194715.\n\n\nLisiewicz, Maciej, Agnieszka Kamińska, Bartłomiej Kraszewski, and\nKrzysztof Stereńczak. 2022. “Correcting the Results\nof CHM-Based Individual Tree Detection Algorithms to\nImprove Their Accuracy and\nReliability.” Remote Sensing 14 (8): 1822.\nhttps://doi.org/10.3390/rs14081822.\n\n\nLong, Jonathan, Evan Shelhamer, and Trevor Darrell. 2015. “Fully\nConvolutional Networks for Semantic\nSegmentation.” In Proceedings of the IEEE\nConference on Computer Vision and Pattern\nRecognition, 3431–40. https://openaccess.thecvf.com/content_cvpr_2015/html/Long_Fully_Convolutional_Networks_2015_CVPR_paper.html.\n\n\nLópez Serrano, F. R., E. Rubio, F. A. García Morote, M. Andrés Abellán,\nM. I. Picazo Córdoba, F. García Saucedo, E. Martínez García, et al.\n2022. “Artificial Intelligence-Based Software\n(AID-FOREST) for Tree Detection: A New\nFramework for Fast and Accurate Forest Inventorying Using\nLiDAR Point Clouds.” International Journal of\nApplied Earth Observation and Geoinformation 113 (September):\n103014. https://doi.org/10.1016/j.jag.2022.103014.\n\n\nLucas, Chris, Willem Bouten, Zsófia Koma, W. Daniel Kissling, and Arie\nC. Seijmonsbergen. 2019. “Identification of Linear\nVegetation Elements in a Rural Landscape Using LiDAR Point\nClouds.” Remote Sensing 11 (3): 292. https://doi.org/10.3390/rs11030292.\n\n\nMallet, Clément, Frédéric Bretar, Michel Roux, Uwe Soergel, and\nChristian Heipke. 2011. “Relevance Assessment of Full-Waveform\nLidar Data for Urban Area Classification.” ISPRS Journal of\nPhotogrammetry and Remote Sensing 66 (6): S71–84. https://doi.org/10.1016/j.isprsjprs.2011.09.008.\n\n\nMartins, Gabriela Barbosa, Laura Elena Cué La Rosa, Patrick Nigri Happ,\nLuiz Carlos Teixeira Coelho Filho, Celso Junius F. Santos, Raul Queiroz\nFeitosa, and Matheus Pinheiro Ferreira. 2021. “Deep Learning-Based\nTree Species Mapping in a Highly Diverse Tropical Urban Setting.”\nUrban Forestry & Urban Greening 64 (September): 127241. https://doi.org/10.1016/j.ufug.2021.127241.\n\n\nMumuni, Alhassan, and Fuseini Mumuni. 2022. “Data Augmentation:\nA Comprehensive Survey of Modern Approaches.”\nArray 16 (December): 100258. https://doi.org/10.1016/j.array.2022.100258.\n\n\nNæsset, Erik. 1997a. “Determination of Mean Tree Height of Forest\nStands Using Airborne Laser Scanner Data.” ISPRS Journal of\nPhotogrammetry and Remote Sensing 52 (2): 49–56. https://doi.org/10.1016/S0924-2716(97)83000-6.\n\n\n———. 1997b. “Estimating Timber Volume of Forest Stands Using\nAirborne Laser Scanner Data.” Remote Sensing of\nEnvironment 61 (2): 246–53. https://doi.org/10.1016/S0034-4257(97)00041-2.\n\n\nNelson, Ross, William Krabill, and Gordon MacLean. 1984.\n“Determining Forest Canopy Characteristics Using Airborne Laser\nData.” Remote Sensing of Environment 15 (3): 201–12. https://doi.org/10.1016/0034-4257(84)90031-2.\n\n\nNilsson, Mats. 1996. “Estimation of Tree Heights and Stand Volume\nUsing an Airborne Lidar System.” Remote Sensing of\nEnvironment 56 (1): 1–7. https://doi.org/10.1016/0034-4257(95)00224-3.\n\n\nNurunnabi, Abdul, Felicia Teferle, Debra F. Laefer, Meida Chen, and Mir\nMasoom Ali. 2024. “Development of a Precise Tree\nStructure from LiDAR Point Clouds.” The\nInternational Archives of the Photogrammetry, Remote Sensing and Spatial\nInformation Sciences XLVIII-2-2024 (June): 301–8. https://doi.org/10.5194/isprs-archives-XLVIII-2-2024-301-2024.\n\n\nOsco, Lucas Prado, Mauro Dos Santos De Arruda, José Marcato Junior,\nNeemias Buceli Da Silva, Ana Paula Marques Ramos, Érika Akemi Saito\nMoryia, Nilton Nobuhiro Imai, et al. 2020. “A Convolutional Neural\nNetwork Approach for Counting and Geolocating Citrus-Trees in\nUAV Multispectral Imagery.” ISPRS Journal of\nPhotogrammetry and Remote Sensing 160 (February): 97–106. https://doi.org/10.1016/j.isprsjprs.2019.12.010.\n\n\nÖzdemi̇r, Emre. 2021. “A Deep Learning Framework for\nGeospatial Point Cloud Classification.” PhD thesis,\nMoscow: Skolkovo Institute of Science; Technology. https://www.skoltech.ru/app/data/uploads/2021/12/thesis-2.pdf.\n\n\nPauly, M., M. Gross, and L. P. Kobbelt. 2002. “Efficient\nSimplification of Point-Sampled Surfaces.” In IEEE\nVisualization, 2002. VIS 2002., 163–70. Boston,\nMA, USA: IEEE. https://doi.org/10.1109/VISUAL.2002.1183771.\n\n\nPedregosa, F., G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O.\nGrisel, M. Blondel, et al. 2011. “Scikit-Learn: Machine Learning\nin Python.” Journal of Machine Learning\nResearch 12: 2825–30.\n\n\nPrechelt, Lutz. 1998. “Automatic Early Stopping Using Cross\nValidation: Quantifying the Criteria.” Neural Networks\n11 (4): 761–67. https://doi.org/10.1016/S0893-6080(98)00010-0.\n\n\nQi, Charles R., Hao Su, Kaichun Mo, and Leonidas J. Guibas. 2017.\n“PointNet: Deep Learning on Point\nSets for 3D Classification and\nSegmentation.” In Proceedings of the IEEE\nConference on Computer Vision and Pattern\nRecognition, 652–60.\n\n\nQi, Charles R., Li Yi, Hao Su, and Leonidas J. Guibas. 2017.\n“PointNet++: Deep Hierarchical Feature\nLearning on Point Sets in a Metric\nSpace.” In Advances in Neural Information\nProcessing Systems. Vol. 30. Curran Associates, Inc.\n\n\nQuegan, Shaun, Thuy Le Toan, Jerome Chave, Jorgen Dall, Jean-François\nExbrayat, Dinh Ho Tong Minh, Mark Lomas, et al. 2019. “The\nEuropean Space Agency BIOMASS Mission:\nMeasuring Forest Above-Ground Biomass from Space.”\nRemote Sensing of Environment 227 (June): 44–60. https://doi.org/10.1016/j.rse.2019.03.032.\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015.\n“U-Net: Convolutional Networks for\nBiomedical Image Segmentation.” In Medical\nImage Computing and Computer-Assisted\nIntervention – MICCAI 2015, edited by Nassir\nNavab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi,\n234–41. Lecture Notes in Computer Science.\nCham: Springer International Publishing. https://doi.org/10.1007/978-3-319-24574-4_28.\n\n\nRoussel, Jean-Romain, David Auty, Nicholas C. Coops, Piotr Tompalski,\nTristan R. H. Goodbody, Andrew Sánchez Meador, Jean-François Bourdon,\nFlorian de Boissieu, and Alexis Achim. 2020. “lidR: An R Package for Analysis of\nAirborne Laser Scanning (ALS) Data.”\nRemote Sensing of Environment 251: 112061. https://doi.org/10.1016/j.rse.2020.112061.\n\n\nShi, Yifang, Tiejun Wang, Andrew K. Skidmore, and Marco Heurich. 2018.\n“Important LiDAR Metrics for Discriminating Forest\nTree Species in Central Europe.” ISPRS Journal\nof Photogrammetry and Remote Sensing 137 (March): 163–74. https://doi.org/10.1016/j.isprsjprs.2018.02.002.\n\n\nSimonyan, Karen, and Andrew Zisserman. 2014. “Very Deep\nConvolutional Networks for Large-Scale Image\nRecognition.” arXiv. https://doi.org/10.48550/ARXIV.1409.1556.\n\n\nSinica-Sinavskis, Juris, and Gunta Grube. 2022. “Forest\nStand Volume Estimation by Species from\nSentinel-2 and LiDAR Data Using Regression\nModels.” In 2022 18th Biennial Baltic\nElectronics Conference (BEC), 1–5. https://doi.org/10.1109/BEC56180.2022.9935590.\n\n\nThe pandas development team. n.d. “Pandas-Dev/Pandas:\nPandas.” https://doi.org/10.5281/zenodo.3509134.\n\n\nThe State of the World’s\nForests. 2020. FAO and UNEP. https://doi.org/10.4060/ca8642en.\n\n\nTreitz, Paul, Kevin Lim, Murray Woods, Doug Pitt, Dave Nesbitt, and Dave\nEtheridge. 2012. “LiDAR Sampling Density for\nForest Resource Inventories in Ontario,\nCanada.” Remote Sensing 4 (4): 830–48. https://doi.org/10.3390/rs4040830.\n\n\nvan der Walt, Stéfan J., Johannes L. Schönberger, Juan Nunez-Iglesias,\nFrançois Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle Gouillart,\nTony Yu, and the scikit-image contributors. 2014. “Scikit-Image:\nImage Processing in Python.” PeerJ 2\n(June): e453. https://doi.org/10.7717/peerj.453.\n\n\nVentura, Jonathan, Camille Pawlak, Milo Honsberger, Cameron Gonsalves,\nJulian Rice, Natalie L. R. Love, Skyler Han, et al. 2024.\n“Individual Tree Detection in Large-Scale Urban Environments Using\nHigh-Resolution Multispectral Imagery.” International Journal\nof Applied Earth Observation and Geoinformation 130 (June): 103848.\nhttps://doi.org/10.1016/j.jag.2024.103848.\n\n\nVermeer, Martijn, Jacob Alexander Hay, David Völgyes, Zsófia Koma,\nJohannes Breidenbach, and Daniele Stefano Maria Fantin. 2023.\n“Lidar-Based Norwegian Tree Species Detection Using\nDeep Learning.” arXiv. https://doi.org/10.48550/arXiv.2311.06066.\n\n\nViana, Aguida Beatriz Travaglia, Carlos Moreira Miquelino Eleto Torres,\nCibele Humel do Amaral, Elpídio Inácio Fernandes Filho, Carlos Pedro\nBoechat Soares, Felipe Carvalho Santana, Lucas Brandão Timo, and Samuel\nJosé Silva Soares da Rocha. 2022. “Timber Volume Estimation\nBy Using Terrestrial Laser Scanning: Method In Hyperdiverse\nSecondary Forests.” Revista Árvore\n46 (August). https://doi.org/10.1590/1806-908820220000021.\n\n\nVirtanen, Pauli, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler\nReddy, David Cournapeau, Evgeni Burovski, et al. 2020.\n“SciPy 1.0: Fundamental Algorithms for\nScientific Computing in Python.”\nNature Methods 17: 261–72. https://doi.org/10.1038/s41592-019-0686-2.\n\n\nWang, Xizhao, Yanxia Zhao, and Farhad Pourpanah. 2020. “Recent\nAdvances in Deep Learning.” International Journal of Machine\nLearning and Cybernetics 11 (4): 747–50. https://doi.org/10.1007/s13042-020-01096-5.\n\n\nWang, Zhen, Pu Li, Yuancheng Cui, Shuowen Lei, and Zhizhong Kang. 2023.\n“Automatic Detection of Individual Trees\nin Forests Based on Airborne LiDAR Data with a\nTree Region-Based Convolutional Neural Network\n(RCNN).” Remote Sensing 15 (4): 1024. https://doi.org/10.3390/rs15041024.\n\n\nWaskom, Michael. 2021. “Seaborn: Statistical Data\nVisualization.” Journal of Open Source Software 6 (60):\n3021. https://doi.org/10.21105/joss.03021.\n\n\nWeinmann, M., B. Jutzi, and C. Mallet. 2013. “Feature Relevance\nAssessment for the Semantic Interpretation of 3D Point\nCloud Data.” ISPRS Annals of the Photogrammetry, Remote\nSensing and Spatial Information Sciences II-5-W2 (October): 313–18.\nhttps://doi.org/10.5194/isprsannals-II-5-W2-313-2013.\n\n\nWeinstein, Ben G., Sergio Marconi, Mélaine Aubry-Kientz, Gregoire\nVincent, Henry Senyondo, and Ethan P. White. 2020.\n“DeepForest: A Python Package for RGB Deep\nLearning Tree Crown Delineation.” Edited by Sydne Record.\nMethods in Ecology and Evolution 11 (12): 1743–51. https://doi.org/10.1111/2041-210X.13472.\n\n\nWeinstein, Ben G., Sergio Marconi, Stephanie Bohlman, Alina Zare, and\nEthan White. 2019. “Individual Tree-Crown Detection\nin RGB Imagery Using Semi-Supervised Deep Learning Neural\nNetworks.” Remote Sensing 11 (11): 1309. https://doi.org/10.3390/rs11111309.\n\n\nWeinstein, Ben, Sergio Marconi, and Ethan White. 2022. “Data for\nthe NeonTreeEvaluation Benchmark.” Zenodo. https://doi.org/10.5281/ZENODO.5914554.\n\n\nWest, Karen F., Brian N. Webb, James R. Lersch, Steven Pothier, Joseph\nM. Triscari, and A. E. Iverson. 2004. “Context-Driven Automated\nTarget Detection in 3D Data.” In Defense and\nSecurity, edited by Firooz A. Sadjadi, 133–43.\nOrlando, FL. https://doi.org/10.1117/12.542536.\n\n\nWhite, Joanne C., Michael A. Wulder, Andrés Varhola, Mikko Vastaranta,\nNicholas C. Coops, Bruce D. Cook, Doug Pitt, and Murray Woods. 2013.\nA Best Practices Guide for Generating Forest Inventory Attributes\nfrom Airborne Laser Scanning Data Using the Area-Based Approach.\nVictoria, British Columbia: Canadian Forest Service.\n\n\nWilkes, Phil, Mathias Disney, John Armston, Harm Bartholomeus, Lisa\nBentley, Benjamin Brede, Andrew Burt, et al. 2023.\n“TLS2trees : A Scalable Tree\nSegmentation Pipeline for TLS Data.” Methods in Ecology\nand Evolution 14 (12): 3083–99. https://doi.org/10.1111/2041-210X.14233.\n\n\nWu, Jingru, Qixia Man, Xinming Yang, Pinliang Dong, Xiaotong Ma, Chunhui\nLiu, and Changyin Han. 2024. “Fine Classification of\nUrban Tree Species Based on UAV-Based RGB\nImagery and LiDAR Data.” Forests 15\n(2): 390. https://doi.org/10.3390/f15020390.\n\n\nZhang, Zhengnan, Tiejun Wang, Andrew K. Skidmore, Fuliang Cao, Guanghui\nShe, and Lin Cao. 2023. “An Improved Area-Based Approach for\nEstimating Plot-Level Tree DBH from Airborne\nLiDAR Data.” Forest Ecosystems 10\n(January): 100089. https://doi.org/10.1016/j.fecs.2023.100089.\n\n\nZhen, Zhen, Lindi Quackenbush, and Lianjun Zhang. 2014. “Impact of\nTree-Oriented Growth Order in Marker-Controlled\nRegion Growing for Individual Tree Crown Delineation Using\nAirborne Laser Scanner (ALS)\nData.” Remote Sensing 6 (1): 555–79. https://doi.org/10.3390/rs6010555.\n\n\nZhu, Liang, Zhijian Zhao, Chao Lu, Yining Lin, Yao Peng, and Tangren\nYao. 2019. “Dual Path Multi-Scale Fusion Networks\nwith Attention for Crowd Counting.”\narXiv. https://doi.org/10.48550/ARXIV.1902.01115.",
    "crumbs": [
      "References"
    ]
  }
]